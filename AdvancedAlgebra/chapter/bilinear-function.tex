
\section{Bilinear Functions}

\subsection{Concept of Bilinear Functions}

\begin{definition}{Bilinear Functions}{}
  Let $V$ be a linear space over $\mathbb{P}$.
  A function $f: V\times V \rightarrow \mathbb{P}$ that satisfies
  \begin{equation}
    f(k_1\alpha_1 + k_2\alpha_2, \beta) = k_1 f(\alpha_1, \beta) + k_2f(\alpha_2, \beta), \quad
    f(\alpha, k_1\beta_1 + k_2\beta_2) = k_1 f(\alpha, \beta_1) + k_2f(\alpha, \beta_2)
  \end{equation}
  is called a \emph{bilinear function on $V$}.
\end{definition}

\begin{definition}{Metric Matrix}{}
  Let $V$ be a $n$-dimensional linear space over $\mathbb{P}$,
  and $\epsilon_1,\cdots,\epsilon_n$ be a basis of $V$.
  For a bilinear function $f$ on $V$, 
  it \emph{metric matrix $G$} is defined as follows:
  \begin{equation}
    G=\left[\begin{matrix}
    f(\epsilon_1,\epsilon_1) & f(\epsilon_1,\epsilon_2) & \cdots & f(\epsilon_1,\epsilon_n)\\
    f(\epsilon_2,\epsilon_1) & f(\epsilon_2,\epsilon_2) & \cdots & f(\epsilon_2,\epsilon_n)\\
    \vdots & \vdots & \ddots & \vdots\\
    f(\epsilon_n,\epsilon_1) & f(\epsilon_n,\epsilon_2) & \cdots & f(\epsilon_n,\epsilon_n)
    \end{matrix}\right]
  \end{equation}
\end{definition}

\begin{proposition}{Representation of Bilinear Functions}{}
  Let $\alpha, \beta \in V$ be two arbitrary vectors,
  with their coordinates under $\epsilon_1,\cdots,\epsilon_n$ be $X, Y$ respectively.
  Then we have
  \begin{equation}
    f(\alpha, \beta) = X^T GY.
  \end{equation}
\end{proposition}

\begin{proof}
  Let $f(\alpha, \beta) = f(x_1\epsilon_1 + \cdots + x_n\epsilon_n,
  y_1\epsilon_1 + \cdots + y_n\epsilon_n)$, use the definition of bilinear
  functions and write the equation into matrix form:
  \begin{equation}
    f(\alpha, \beta) =
    \left[
      \begin{array}{cccc}
        x_1&x_2&\cdots&x_n
      \end{array}
    \right] \left[
      \begin{array}{c}
        f(\epsilon_1, y_1 \epsilon_1 + \cdots y_n\epsilon_n)\\
        f(\epsilon_2, y_1 \epsilon_1 + \cdots y_n\epsilon_n)\\
        \vdots\\
        f(\epsilon_n, y_1 \epsilon_1 + \cdots y_n\epsilon_n)
      \end{array}
    \right] = X^T Z,
  \end{equation}
  where $Z$ can be also decomposed into
  \begin{equation}
    Z = \left[
      \begin{array}{c}
        y_1f(\epsilon_1,\epsilon_1) + \cdots + y_n(\epsilon_1, \epsilon_n)\\
        y_1f(\epsilon_2,\epsilon_1) + \cdots + y_n(\epsilon_2, \epsilon_n)\\
        \vdots\\
        y_1f(\epsilon_n,\epsilon_1) + \cdots + y_n(\epsilon_n, \epsilon_n)
      \end{array}
    \right] = \left[
      \begin{array}{cccc}
        f(\epsilon_1,\epsilon_1)&f(\epsilon_1,\epsilon_2)&\cdots&f(\epsilon_1, \epsilon_n)\\
        f(\epsilon_2,\epsilon_1)&f(\epsilon_2,\epsilon_2)&\cdots&f(\epsilon_2, \epsilon_n)\\
        \vdots\\
        f(\epsilon_n,\epsilon_1)&f(\epsilon_n,\epsilon_2)&\cdots&f(\epsilon_n, \epsilon_n)
      \end{array}
    \right] \left[
      \begin{array}{c}
        y_1\\
        y_2\\
        \vdots\\
        y_n
      \end{array}
    \right] = GY.
  \end{equation}
  Combining the above two equations yields $f(\alpha, \beta) = X^T GY$.
\end{proof}

\subsection{Congruence: Matrices of Bilinear Functions under Different Bases}

\begin{definition}{Congruence}{}
  Let $A$ and $B$ be $n$-order square matrices over $\mathbb{P}$.
  If there exists an invertible $n$-order matrix $P$ over $\mathbb{P}$ such that
  \begin{equation}
    B = P^T A P,
  \end{equation}
  then $A$ and $B$ are said to be \emph{congruent}.
\end{definition}

\begin{proposition}{Congruence and Bilinear Functions}{}
  Matrices $A$ and $B$ are congruent if and only if they are the matrices of
  a bilinear function under two different bases.
\end{proposition}

\begin{proof}
  Let $\epsilon_1,\cdots,\epsilon_n$ and $\eta_1,\cdots,\eta_n$ be two bases satisfying
  \begin{equation}
    (\epsilon_1,\cdots,\epsilon_n) = (\eta_1, \cdots, \eta_n) P.
  \end{equation}
  Then we explore the relation between $f(\epsilon_i, \epsilon_j)$ and
  $f(\eta_i, \eta_j)$.
  By the definition of bilinear functions
  \begin{equation}
    f(\epsilon_i, \epsilon_j) = f(\sum_k p_{ki}\eta_k, \sum_k p_{kj}\eta_k)
    = P_i^T G_{\eta} P_j,
  \end{equation}
  where $G_{\eta}$ is the matrix of $f$ under the basis $\eta_1,\cdots,\eta_n$,
  and $P_i, P_j$ are the $i$-th and $j$-th column of $P$, respectively.
  Then
  \begin{equation}
    G_{\epsilon} = \left[
      \begin{array}{cccc}
        P_1^T G_{\eta}P_1&P_1^T G_{\eta}P_2&\cdots&P_1^T G_{\eta}P_n\\
        P_2^T G_{\eta}P_1&P_2^T G_{\eta}P_2&\cdots&P_3^T G_{\eta}P_n\\
        \vdots\\
        P_n^T G_{\eta}P_1&P_n^T G_{\eta}P_2&\cdots&P_n^T G_{\eta}P_n
      \end{array}
    \right] = 
    \left[
      \begin{array}{c}
        P_1^T\\
        P_2^T\\
        \vdots\\
        P_n^T
      \end{array}
    \right] \left[
      \begin{array}{cccc}
        G_{\eta}P_1&G_{\eta}P_2&\cdots&G_{\eta}P_n
      \end{array}
    \right] = P^TG_{\eta}P.
  \end{equation}
  Then we get the conclusion that $G_{\epsilon} = P^TG_{\eta}P$.
\end{proof}

\begin{proposition}{Properties of Congruence}{}
  Let $A$ and $B$ be two congruent matrices,
  then (1) $r(A) = r(B)$; (2) The congruent matrix $P$ is not necessarily unique.
\end{proposition}

\section{Quadratic Forms}

\subsection{Concept of Quadratic Forms and Their Matrices}

\begin{definition}{Quadratic Forms}{}
  A multivariable quadratic polynomial of the form
  \begin{equation}
    f(x_1,x_2,\cdots,x_n) = \sum\limits_{i = 1}^n a_{ii}x_i^2 + \sum\limits_{1 \leq i < j \leq n} 2a_{ij}x_ix_j
  \end{equation}
  is called a \emph{quadratic form}.
\end{definition}

\begin{definition}{Matrix of a Quadratic Form}{}
  For any quadratic form $f(x_1,\cdots,x_n)$,
  it can be represented in the form
  \begin{equation}
    f(x_1,\cdots,x_n) = x^TAx,
  \end{equation}
  where $x = [x_1,\cdots,x_n]^T$,
  and $A$ is a symmetric matrix is called \emph{the matrix of the quadratic
    form}.
\end{definition}

\begin{example}{Find the Matrix of Quadratic Forms}{}
  Find the matrices of the following quadratic forms:
  \begin{equation}
    (1) f(x_1, x_2, x_3) = 2x_1x_2 + 2x_1x_3 - 6x_2x_3, \quad
    (2) f(x_1,\cdots,x_n) = \sum\limits_{i = 1}^m (a_{i1}x_1 + \cdots + a_{in}x_n)^2.
  \end{equation}
\end{example}

\begin{solution}
  (1) Direct from the definition, we know $a_{12} = 1$, $a_{13} = 1$, $a_{23} = -3$.
  That is, the matrix is
  \begin{equation}
    A = \left[
      \begin{array}{ccc}
        0&1&1\\
        1&0&-3\\
        1&-3&0
      \end{array}
    \right].
  \end{equation}
\end{solution}

\begin{definition}{Equivalence of Quadratic Forms}{}
  Given quadratic forms $f(x) = x^TAx$ and $g(x) = x^TBx$,
  if there exists an invertible matrix $P$ such that
  \begin{equation}
    A = P^T B P,
  \end{equation}
  then we say $f(x)$ and $g(x)$ are \emph{equivalent}.
\end{definition}

\subsection{Canonical Form of Quadratic Forms}

\begin{definition}{Canonical Form of Quadratic Forms}{}
  For a quadratic form $f(x) = x^TAx$,
  if $A$ is congruent to a diagonal matrix $D$,
  then the quadratic form
  \begin{equation}
    g(x) = x^T Dx
  \end{equation}
  is called the \emph{canonical form of $f$}.
\end{definition}

\begin{theorem}{Existence of the Canonical Form of Quadratic Forms}{}
  For any symmetric matrix $A$ over the number field $\mathbb{P}$,
  it is congruent to a diagonal matrix,
  and the diagonal matrix is not unique.
\end{theorem}

\begin{proof}
  We prove it by mathematical induction. The conclusion holds when $n = 1$.
  Assume that conclusion holds for $n - 1$, we consider
  \begin{equation}
    A_n =
    \begin{bmatrix}
      \alpha_{11} & \alpha^T\\
      \alpha & A_{n-1}
    \end{bmatrix},
    \quad
    P_n =
    \begin{bmatrix}
      1 & - \frac{1}{a_{11}}\alpha^T\\
      0 & I_{n-1}
    \end{bmatrix},
  \end{equation}
  we get $P_n^TAP_n = \operatorname{diag} (a_{11}, B_{n-1})$,
  where $B_{n-1} = A_{n-1} - \frac{1}{a_{11}}\alpha \alpha^T$.
  By assumption, there exists $Q_{n-1}$ such that $Q_{n-1}^TB_{n-1}Q_{n-1}$ is
  diagonal.
  Let $Q_n = \operatorname{diag}(1, Q_{n-1})$, then
  \begin{equation}
    Q_n^T P_n^T A P_n Q_n = \operatorname{diag}(a_{11}, Q_{n-1}^T B_{n-1}Q_{n-1}),
  \end{equation}
  which completes the proof.
\end{proof}

\begin{proposition}{Completing-the-Square Method for Canonical Form of Quadratic
  Forms}{}
  The completing-the-square method for connonical form of quadratic forms
  proceeds as follows:
  \begin{enumerate}
  \item If there is no square term of $x_1$, but there is cross-term $x_1x_2$,
    then use the substitution $x_1 = y_1 + y_2$, $x_2 = y_1 - y_2$;
  \item Group all terms containing $x_1$ together and complete the square;
  \item Take the compeleted-square term as a new variable.
  \end{enumerate}
\end{proposition}

\begin{note}
  It is not very convenient to calculate the transition matrix using the
  completing-the-square method.
\end{note}

\begin{example}{Use the Completing-the-Square Method}{}
  Use the completing-the-square method to find the canonical form of
  \begin{equation}
    f(x_1, x_2, x_3) = 2x_1x_2 + 2x_1x_3 - 6x_2x_3.
  \end{equation}
\end{example}

\begin{solution}
  
\end{solution}

\begin{proposition}{Elementary Transformation Method for Canonical Form of
    Quadratic Forms}{}
  Let $A$ be a symmetric matrix, we can find the transition matrix and its
  canonical form by performing elementary row transformation and
  corresponding column transformation on the matrix
  \begin{equation}
    \begin{pmatrix}A\\I\end{pmatrix}\to\begin{pmatrix}P^TAP\\P\end{pmatrix}.
  \end{equation}
  After the process, $P^TAP$ is the canonical form of $A$, and $P$ is the
  transition matrix.
\end{proposition}

\begin{note}
  If the leading diagonal of $A$ is zero, then find a non-zero diagonal element
  and swap their position.
  If all diagonal elements are zero, then find a non-zero element in the same
  column, and add it to the current row.
\end{note}

\begin{example}{Using the Elementary Transformation Method}{}
  Use the elementary transformation method to find the canonical form
  and corresponding transition matrix of
  \begin{equation}
    f = x_1^2 + 2x_1x_2 + 2x_2^2 + 4x_2x_3 + 4x_3^2, \quad
    g = 2x_1x_2 + 2x_1x_3 - 6x_2x_3.
  \end{equation}
\end{example}

\begin{solution}
  
\end{solution}

\subsection{Orthogonal Transformation Method for Quadratic Forms}

\begin{proposition}{Feasibility of Orthogonal Diagonalization}{}
  Any real symmetric matrix $A$ is orthogonally similar to a diagonal matrix.
\end{proposition}

\begin{proof}
  We prove the proposition by mathematical induction.
  When $n = 1$, it is obvious.
  Assume that it holds for $n - 1$, take an eigenvalue $\lambda$ arbitrarily,
  and the corresponding unit-eigenvector $\alpha_1$.
  Expand $\alpha_1$ to an orthogonal basis $\alpha_1,\cdots,\alpha_n$ of
  $\mathbb{R}^n$.
  Let $P = (\alpha_1,\cdots,\alpha_n)$, then
  \begin{equation}
    AP = P \begin{pmatrix}
        \lambda&\beta^T\\
        0&A_{n-1}
      \end{pmatrix}
    \Rightarrow
    P^TAP=
    \begin{pmatrix}
      \lambda&\beta^T\\
      0&A_{n-1}
    \end{pmatrix}.
  \end{equation}
  Since $P^TAP$ is symmetric, we get $\beta = 0$.
  Then according to the induction hypothesis, there exists an orthogonal matrix $Q_1$
  such that $Q_1^T A_{n-1}Q_1$ is a diagonal matrix.
  Now let $Q = \operatorname{diag}(1, Q_1)$, then
  \begin{equation}
    Q^TP^TAPQ =
    \begin{pmatrix}
      \lambda & 0\\
      0 & Q_1^TA_{n-1}Q_1
    \end{pmatrix}
  \end{equation}
  And the matrix $PQ$ is an orthogonal matrix, which completes the proof.
\end{proof}

\begin{proposition}{Calculation of Canonical Form: Orthogonal Transformation}{}
  The process of calculating the canonical form by orthogonal transformation is
  as follows:
  \begin{enumerate}
  \item Calculate the eigenvalues $\lambda_1,\cdots,\lambda_n$ of A;
  \item Calculate the eigenvector $v_i$ corresponding to $\lambda_i$.
    For eigenvectors corresponding to the same eigenvalue need to be orthogonalized;
  \item Normalize the eigenvectors and form $P = [v_1,\cdots,v_n]$.
  \end{enumerate}
\end{proposition}

\begin{example}{Calculate Canonical Form via Orthogonal Transformation}{}
  Find the canonical form of the following quadratic forms:
  \begin{equation}
    f = x_1^2 + 2x_1x_2 + 2x_2^2 + 4x_2x_3 + 4x_3^2.
  \end{equation}
\end{example}

\begin{solution}
  
\end{solution}

\begin{example}{Application of Orthogonal Similarity}{}
  Prove the following propositions:
  \begin{enumerate}
  \item Let $A$ be a $n$-order real symmetric matrix with eigenvalues
    $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$, then
    \begin{equation}
      \max \limits_{x \in \mathbb{R}^n} \frac{x^T A x}{x^Tx} = \lambda_1,\quad
      \min \limits_{x \in \mathbb{R}^n} \frac{x^TAx}{x^Tx} = \lambda_n.
    \end{equation}
  \item Let $A, B$ be two $n$-order real symmetric matrices,
    all the eigenvalues of $A$ are greater than $a$,
    and all the eigenvalues of $B$ are greater than $b$.
    Then the eigenvalues of $A + B$ are greater than $a + b$.
  \end{enumerate}
\end{example}

\begin{solution}
  
\end{solution}

\subsection{Positive Definite Quadratic Forms over $\mathbb{R}$}

A positive definite quadratic form is a special equivalent class of
quadratic forms in $\mathbb{R}^n$, and its representative is $x^TIx$.

\begin{definition}{Positive Definite Quadratic Form}{}
  Given a quadratic form $f(x_1,\cdots,x_n)$ over the real number field,
  if for any non-zero $x_1,\cdots,x_n$, we have
  \begin{equation}
    f(x_1,\cdots,x_n) > 0,
  \end{equation}
  then $f(x_1,\cdots,x_n)$ is called a \emph{positive definite quadratic form},
  and its matrix is called a \emph{positive definite matrix}.
\end{definition}

\begin{example}{Basic Properties of Positive Definite Quadratic Forms}{}
  Let $A = (a_{ij})$ be an $n$-order positive definite matrix, prove that
  \begin{enumerate}
  \item $a_{ii} > 0$, for $i = 1,\cdots,n$;
  \item $2|a_{ij}| < a_{ii} + a_{ij}$, for $i \neq j$;
  \item Among all the elements of $A$, the element with the largest absolute value
    must be on the main diagonal, and it must be a positive number.
  \end{enumerate}
\end{example}

\begin{proposition}{Sufficient and Necessary Conditions for Positive Definiteness}{}
  The quadratic form $f(x) = x^TAx$ is a positive definite quadratic form
  if and only if $A$ is congruent to the identity matrix $I$.
\end{proposition}

\begin{proof}
  Since quadratic forms over $\mathbb{R}$ is equivalent to 
  \begin{equation}
    g = u_1^2 + \cdots + u_p^2 - u_{p+1}^2 - \cdots - u_r^2.
  \end{equation}
  The given condition $f(x) > 0$ is satisfied if and only if
  all the coefficients of $g$ are $1$.
\end{proof}

\begin{corollary}{From a Matrix Decomposition Perspective}{}
  The quadratic form $f = x^TAx$ is a positive definite quadratic form if and only if there exists
  an invertible matrix $P$ such that
  \begin{equation}
    A = P^TP.
  \end{equation}
\end{corollary}

\begin{proof}
  Since $A$ is congruent to the identity matrix $I$,
  so there exists an invertible matrix $P$ such that $PAP^T = I$, which means
  \begin{equation}
    A = P^TP.
  \end{equation}
  Thus the conclusion is proved.
\end{proof}

\begin{proposition}{From a Eigenvalue Perspective}{}
  The quadratic form $f = x^TAx$ is a positive definite quadratic form if and
  only if all eigenvalues of $A$ are greater than $0$.
\end{proposition}

\begin{proof}
  Since $A$ is a symmetric real matrix, then there exists an orthogonal matrix
  $P$ such that
  \begin{equation}
    P^TAP = \Lambda.
  \end{equation}
  Then $x^T\Lambda x$ is also a positive definite quadratic form, which means
  all the diagonal elements of $\Lambda$ are positive.
\end{proof}

\begin{corollary}{From a Leading Principal Minors Perspective}{}
  The quadratic form $f = x^TAx$ is a positive definite quadratic form if and
  only if all the leading principal minors (or all the principal minors) of $A$
  are greater than $0$.
\end{corollary}

\begin{proof}
  The eigenvalue perspective directly yields the conclusion.
\end{proof}

\begin{example}{Determine Positive Definite Quadratic Forms}{}
  When $t$ takes what value, is the quadratic form $f$ positive definite, where
  \begin{equation}
    f = x_1^2 + x_2^2 + 5x_3^2 + 2tx_1x_2 - 2x_1x_3 + 4x_2x_3.
  \end{equation}
\end{example}

\begin{proposition}{}{}
  Let $A, B$ be two positive definite matrices,
  then
  \begin{equation}
    A^{-1}, A^k, A+B, A^{\ast}
  \end{equation}
  are all positive definite.
  If $AB$ is symmetric, then it is also positive definite.
\end{proposition}

\subsection{Matrix Decomposition via Positive Definite Quadratics}

\begin{proposition}{Square Decomposition}{}
  Let $A$ be an $n$-order square positive definite matrix,
  then there exists a real symmetric matrix $C$ such that
  \begin{equation}
    A = C^2.
  \end{equation}
\end{proposition}

\begin{proof}
  Since $A$ is a positive definite matrix, then there exists an
  orthogonal matrix $P$ such that
  $P^TAP = \Lambda$ where $\Lambda =
  \operatorname{diag}(\lambda_1,\cdots,\lambda_n)$.
  Let $C = P^T \sqrt{\Lambda} P$, then $A = C^2$.
\end{proof}

\begin{proposition}{Upper-Triangular Decomposition}{}
  Let $A$ be an $n$-order square positive definite matrix,
  then there exists an invertible upper-triangular matrix $R$ such that
  \begin{equation}
    A = R^TR.
  \end{equation}
\end{proposition}

\begin{proof}
  Since $A$ is positive definite, then there exists an invertible matrix $C$
  such that $A = C^TC$.
  Apply QR decomposition to $C = QR$, then we get
  \begin{equation}
    A = R^TR.
  \end{equation}
\end{proof}

\begin{lemma}{}{}
  Let $A$ be an $m \times n$ real matrix, then
  \begin{equation}
    r(A^TA) = r(A).
  \end{equation}
\end{lemma}

\begin{theorem}{}{}
  Let $A$ be an $m \times n$ matrix over $\mathbb{R}$,
  then $A^TA$ and $AA^T$ are semi-positive definite matrices.
  And $A^TA$ is positive definite when it is column-full-rank,
  $AA^T$ is positive definite when it is row-full-rank.
\end{theorem}

\subsection{Simultaneous Diagonalization of Positive Definite Matrices}

\begin{proposition}{Simultaneous Diagonalization}{}
  Let $A, B$ be two semi-positive definite matrices,
  then there exists an invertible matrix $P$, such that
  \begin{equation}
    P^TAP, P^TBP
  \end{equation}
  are diagonal matrices.
\end{proposition}





