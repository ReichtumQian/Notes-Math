
\section{Linear Spaces}

\subsection{Concepts of Linear Spaces}

\begin{definition}{Linear Space}{}
  Let $\mathbb{P}$ be a number field, $V$ be a non-empty set.
  If two binary operations(addition and scalar multiplication),
  are defined on $V$, and these two operations satisfy the following axioms
  \begin{enumerate}
  \item Binary Operations: (1) Commutativity
    (2) Associativity (3) Existence of zero element (4) Existence of negative element
  \item Scalar Multiplication: (1) Distributivity over vector addition
    (2) Distributivity over scalar addition
    (3) Associativity of scalar multiplication
    (4) Identity scalar multiplication
  \end{enumerate}
  Then $V$ is a \emph{linear space over $\mathbb{P}$}.
\end{definition}

\begin{definition}{Linear Dependence and Linear Independence}{}
  Let $\alpha_1, \cdots, \alpha_n$ be a sequence of vectors,
  if there exist $n$ numbers $k_1, \cdots, k_n$ in $\mathbb{P}$, not all zero, such that
  \begin{equation}
    k_1 \alpha_1 + \cdots + k_n \alpha_n = 0.
  \end{equation}
  Then they are said to be \emph{linearly dependent}.
  If this equation holds if and only if $k_1 = k_2 = \cdots = k_n = 0$,
  then they are \emph{linearly independent}.
\end{definition}

\begin{example}{Linear Independence of Nilpotent Basis}{}
  Let $\mathcal{A}$ be a linear transformation on $V$,
  and let $\xi$ be a vector in $V$.
  Prove that if $\mathcal{A}^{k - 1}\xi \neq 0, \mathcal{A}^k \xi = 0$ ($k > 1$),
  then
  \begin{equation}
    \xi, \mathcal{A} \xi, \cdots, \mathcal{A}^{k-1}\xi
  \end{equation}
  are linearly independent.
\end{example}

\begin{proof}
  Assume that $k_0 \xi + k_1 \mathcal{A} \xi + \cdots +
  k_{k-1}\mathcal{A}^{k-1}\xi = 0$.
  Apply $\mathcal{A}^{k-1}$ to both sides to get $k_0 \mathcal{A}^{k-1}\xi = 0$.
  Since $\mathcal{A}^{k-1}\xi \neq 0$,
  we get $k_0 = 0$.
  And by repeating this process, we can deduce that $k_0, k_1, \cdots, k_{k-1}$
  are all zeroes.
\end{proof}

\begin{proposition}{Finding the Maximal Linearly Independent Subsets}{}
  Given a set of vectors $\alpha_1, \cdots, \alpha_n$,
  form a matrix with these vectors as column vectors,
  and perform elementary row operations on the matrix to transform it into a
  row-echelon form.
  The vectors corresponding to the columns where the leading $1$s (pivots or
  leading entries) located form a maximal linearly independent subset.
\end{proposition}

\begin{example}{Finding the Maximal Linearly Independent Sub-sets}{}
  Calculate the maximal linearly independent subsets of the following vector set:
  \begin{equation}
    \alpha_1 = (1, -1, 2, 4), \alpha_2 = (0, 3, 1, 2), \alpha_3 = (3, 0, 7, 14), \alpha_4 = (1, -1, 2, 0), \alpha_5 = (2, 1, 5, 6).
  \end{equation}
\end{example}

\begin{solution}
  Forming a new matrix and performing elementary row operations yield
  \begin{equation}
    \begin{bmatrix}1&0&3&1&2\\-1&3&0&-1&1\\2&1&7&2&5\\4&2&14&0&6\end{bmatrix}\to\begin{bmatrix}1&0&3&1&-2\\0&1&1&0&1\\0&0&0&1&1\\0&0&0&0&0\end{bmatrix},
  \end{equation}
  so the maximal linearly independent subset is $\alpha_1, \alpha_2, \alpha_4$.
\end{solution}

\begin{definition}{Rank of Vector Set}{}
  Let $\alpha_1, \cdots, \alpha_n$ be a vector set,
  the number of vectors in its maximal linearly independent subset is called
  their \emph{rank}.
\end{definition}

\subsection{Basis and Coordinates}

\begin{definition}{Basis}{}
  Let $V$ be a finite-dimensional linear space,
  and $\epsilon_1, \cdots, \epsilon_n$ be linearly independent in $V$.
  If any $\alpha \in V$ can be linearly represented by this set of vectors,
  then this set of vectors are called the \emph{basis} of $V$.
\end{definition}

\begin{example}{Proving a Basis}{}
  Given $\alpha_1 = (1, 1, \cdots, 1)$,
  $\alpha_2 = (1, 1, \cdots, 1, 0)$,
  $\cdots$,
  $\alpha_n = (1, 0, \cdots, 0)$.
  Prove that $\alpha_1, \cdots, \alpha_n$ is a basis of $\mathbb{R}^n$.
\end{example}

\begin{proof}
  Hint: Just arrange them into a matrix and check its full-rank property.
\end{proof}

\begin{definition}{Coordinates}{}
  Let $V$ be a finite-dimensional linear space,
  and $\epsilon_1, \cdots, \epsilon_n$ be a basis of $V$.
  If a vector $\alpha \in V$ can be represented as
  \begin{equation}
    \alpha = (\epsilon_1, \cdots, \epsilon_n)X,
  \end{equation}
  where $X \in \mathbb{P}^n$ is called the \emph{coordinate} of $\alpha$
  under the basis $\epsilon_1,\cdots, \epsilon_n$.
\end{definition}

\begin{example}{Calculating Coordinates}{}
  Given that $\epsilon_{1}, \epsilon_2, \epsilon_3$ is a basis of
  $\mathbb{R}^3$,
  calculate the coordinate of $\alpha = (1, 2, 1)^T$ under this basis, where
  \begin{equation}
    \epsilon_1=(1,1,1)^T,\epsilon_2=(1,1,-1)^T,\epsilon_3=(1,-1,-1)^T.
    \end{equation}
\end{example}

\begin{definition}{Transition Matrix}{}
  Given two bases $(\epsilon_1, \cdots, \epsilon_n)$
  and $(\eta_1, \cdots, \eta_n)$, if there exists a matrix $T$ such that
  \begin{equation}
    (\epsilon_1, \cdots, \epsilon_n) = (\eta_1, \cdots, \eta_n)T,
  \end{equation}
  then $T$ is called the \emph{transition matrix} from $\eta_1, \cdots, \eta_n$
  to $\epsilon_1, \cdots, \epsilon_n$.
\end{definition}

\begin{proposition}{Coordinate Transformation}{}
  Let $V$ be an $n$-dimensional linear space, $\eta_1,\cdots,\eta_n$ and
  $\epsilon_1,\cdots, \epsilon_n$ be two sets of bases in the space.
  Suppose $T$ is the transition matrix from $\eta_i$ to $\epsilon_i$,
  then for any $\alpha \in V$, if its coordinates under the bases $\epsilon_i$
  and $\eta_i$ are $X$ and $Y$ respectively, then
  \begin{equation}
    X = TY.
  \end{equation}
\end{proposition}

\begin{note}{}
  Calculation of Transition Matrix:
  By elementary row operations find $(\eta_1, \cdots, \eta_n)^{-1} (\epsilon_1,
  \cdots, \epsilon_n)$.
\end{note}

\begin{example}{Calculation of Transition Matrix}{}
  Given the following two bases, (1) find the transition matrix between them
  (2) find the coordinate of $\xi = (1, 0, 0, -1)^T$ under $\alpha_1, \alpha_2,
  \alpha_3, \alpha_4$.
  \begin{equation}
    \alpha_1=(1,1,1,1),\alpha_2=(1,1,-1,-1),\alpha_3=(1,-1,1,-1),\alpha_4=(1,-1,-1,1)
  \end{equation}
  \begin{equation}
    \beta_1=(1,1,0,1),\beta_2=(2,1,3,1),\beta_3=(1,1,0,0),\beta_4=(0,1,-1,-1)
  \end{equation}
\end{example}

\begin{solution}
  (1) Hint: Construct matrix $A, B$ and calculate the transition matrix by
  $(A|B) \rightarrow (I|A^{-1}B)$. The answer is
  \begin{equation}
    A^{-1}B=\frac{1}{4}\left[\begin{array}{cccc}3&7&2&-1\\1&-1&2&3\\-1&3&0&-1\\1&-1&0&-1\end{array}\right]
  \end{equation}

  (2) Hint: Solve a system of linear equations, and the final answer is $[0,
  0.5, 0.5, 0]^T$.
\end{solution}

\begin{definition}{Isomorphism}{}
  If there exists a bijection between two algebraic systems,
  and this bijection preserves the operations of the algebraic systems,
  then the two algebraic systems are \emph{isomorphic}.
\end{definition}

\begin{theorem}{Isomorphism Mapping of Finite-Dimensional Linears Spaces}{}
  Let $V$ be a finite-dimensional linear space over $\mathbb{P}$,
  then it is isomorphic with $\mathbb{P}^n$.
\end{theorem}

\begin{proof}
  Suppose that $\alpha_1, \cdots, \alpha_n$ is a basis of the space $V$,
  we can construct a linear mapping $f$ that maps $\alpha \in V$ to
  its coordinate over $\alpha_1, \cdots, \alpha_n$.
  Then $f$ is an isomorphism mapping from $V$ to $\mathbb{P}^n$,
  thus $V$ is isomorphic with $\mathbb{P}^n$.
\end{proof}

\begin{example}{Applications of Isomorphism}{}
  \begin{itemize}
  \item Let $\beta_1 = \alpha_1 + \alpha_2, \beta_2 = \alpha_1 + \alpha_2,
    \beta_3 = \alpha_2 + \alpha_3$. Prove that $\alpha_1, \alpha_2, \alpha_3$
    and $\beta_1, \beta_2, \beta_3$ are equivalent.
  \item Let $\beta_1 = \alpha_2 + \cdots + \alpha_n,
    \beta_2 = \alpha_1 + \alpha_3 + \cdots + \alpha_n,
    \beta_n = \alpha_1 + \cdots + \alpha_{n-1}$.
    Prove that $\alpha_1, \cdots, \alpha_n$ and $\beta_1, \cdots, \beta_n$ have
    the same rank.
  \end{itemize}
\end{example}

\begin{proof}
  Hint: Represent the matrix $[\beta_1, \beta_2, \beta_3]$ by $\alpha_1, \alpha_2, \alpha_3$.
\end{proof}

\subsection{Sum and Dimension Theorem}

\begin{definition}{Sum of Linear spaces}{}
  Let $V_1$ and $V_2$ be two linear spaces, then their \emph{sum} is
  \begin{equation}
    V_1 + V_2 := \{\alpha = \alpha_1 + \alpha_2: \alpha_1 \in V_1, \alpha_2 \in V_2\}.
  \end{equation}
\end{definition}

\begin{example}{Basic Calculations of Intersection and Sum}{}
  Given $V_1 = \operatorname{span}(\alpha_1, \alpha_2, \alpha_3)$,
  and $V_2 = \operatorname{span}(\beta_1, \beta_2)$,
  where
  \begin{equation}
    \alpha_1=\begin{pmatrix}1\\2\\1\\0\end{pmatrix},\alpha_2=\begin{pmatrix}-1\\1\\1\\1\end{pmatrix},\alpha_3=\begin{pmatrix}0\\3\\2\\1\end{pmatrix},\beta_1=\begin{pmatrix}2\\-1\\0\\1\end{pmatrix},\beta_2=\begin{pmatrix}1\\-1\\3\\7\end{pmatrix}.
  \end{equation}
  Find the basis and dimension of $V_1 + V_2$, $V_1 \cap V_2$.
\end{example}

\begin{solution}
  (1) Hint: $V_1 + V_2 = \operatorname(\alpha_1, \alpha_2, \alpha_3, \beta_1,
  \beta_2)$,
  arrange them in a matrix and perform elementary row-operations.

  (2) Hint: for any $\alpha \in V_1 \cap V_2$,
  assume
  \begin{equation}
    \alpha = x_1 \alpha_1 + x_2 \alpha_2 + x_3 \alpha_3 = y_1 \beta_1 + y_2 \beta_2,
  \end{equation}
  then the coefficients need to satisfy the system of equations:
  \begin{equation}
    x_1 \alpha_1 + x_2 \alpha_2 + x_3 \alpha_3 - y_1 \beta_1 - y_2 \beta_2 = 0.
  \end{equation}
  Let $x_4 = -y_1, x_5 = y_2$, that is, solve $AX = 0$.
  And the fundamental solution system is
  \begin{equation}
    \left.\eta_1=\left(\begin{array}{c}-1\\-1\\1\\0\\0\end{array}\right.\right),\eta_2=\left( \begin{array}{c}1\\-4\\0\\-3\\1\end{array} \right).
  \end{equation}
\end{solution}

\begin{example}{The Differences between Sum and Union}{}
  \begin{enumerate}
  \item Let $V_1$ and $V_2$ be two non-trivial subspaces of the linear space $V$ over
  the number field $\mathbb{P}$. Prove that there exists $\alpha \in V$ such that
  $\alpha \not\in V_1, \alpha \not \in V_2$.
  \item 
  \end{enumerate}
\end{example}

\begin{proof}
  (1)
\end{proof}

\begin{theorem}{Dimension Theorem}{}
  Let $V_1$ and $V_2$ be two subspaces of the finite-dimensional linear space
  $V$. Then
  \begin{equation}
    \dim V_1 + \dim V_2 = \dim (V_1 + V_2) + \dim (V_1 \cap V_2).
  \end{equation}
\end{theorem}

\begin{proof}
  We start from a small space and expand the basis.
  Let the basis of $V_1 \cap V_2$ be $\alpha_1,\cdots,\alpha_s$,
  and the bases of $V_1, V_2$ be respectively:
  \begin{equation}
    \alpha_1,\cdots,\alpha_s,\beta_{s+1},\cdots,\beta_m; \quad 
    \alpha_1,\cdots,\alpha_s,\gamma_{s+1},\cdots,\gamma_n,
  \end{equation}
  Then $V_1 + V_2 = \operatorname{span}(\alpha_1,\cdots,\alpha_s,\beta_{s+1},\cdots,\beta_m,\gamma_{s+1},\cdots,\gamma_n)$.
  Next, we prove that the above-mentioned vectors are linearly independent.
  Without loss of generality, assume
  \begin{equation}
    k_1\alpha_1+\cdots+k_s\alpha_s+k_{s+1}\beta_{s+1}+\cdots+k_n\beta_n+l_{s+1}\gamma_{s+1}+\cdots+l_n\gamma_n=0.
  \end{equation}
  Move the part with $l_i \gamma_i$ to the right-hand side.
  At this time, the left-hand side linear combination belongs to $V_1$,
  and the right-hand side belongs to $V_2$,
  so it belongs to $V_1 \cap V_2$.
  So it is not hard to find that $k_{s+1} = k_{s+2} = \cdots = k_n = 0$,
  and $l_{s+1} = \cdots = l_n = 0$.
  Since the vector set $\alpha_1, \cdots, \alpha_s$ is a basis,
  $\alpha_1,\cdots,\alpha_s$ are linearly independent. Thus all the coefficients are zero,
  which implies the linear independence.
\end{proof}


\subsection{Direct-Sum Theory}

\begin{definition}{Direct Sum}{}
  Let $V_1$ and $V_2$ be subspaces of the linear space $V$.
  If $V_1 \cap V_2 = \{0\}$,
  then we call the sum of $V_1, V_2$ the \emph{direct sum},
  denoted as
  \begin{equation}
    V_1 \oplus V_2 := V_1 + V_2.
  \end{equation}
\end{definition}

\begin{definition}{Complementary Space}{}
  If $V = V_1 \oplus V_2$,
  then $V_2$ is called the \emph{complementary space of $V_1$}.
\end{definition}

\begin{proposition}{Equivalent Conditions for Direct Sum}{}
  The equation $V = V_1 \oplus V_2$ holds if and only if:
  \begin{itemize}
  \item Any vector $\alpha$ in $V$ has a unique representation $\alpha =
    \alpha_1 + \alpha_2$, where $\alpha_1 \in V_1$ and $\alpha_2 \in V_2$.
  \item The zero element in $V$ is uniquely represented as the sum of
    the zero elements in $V_1$ and $V_2$.
  \item $\operatorname{dim} V = \operatorname{dim} V_1 + \operatorname{dim} V_2$.
  \end{itemize}
\end{proposition}

\begin{proposition}{Direct-Sum Combination Theorem}{}
  If we have $V = V_1 \oplus V_2$,
  where $V_1 = V_{11} \oplus V_{12} \oplus \cdots \oplus V_{1s}$,
  and $V_2 = V_{21} \oplus \cdots \oplus V_{2t}$,
  then
  \begin{equation}
    V = V_{11} \oplus \cdots \oplus V_{1s} \oplus V_{21} \oplus \cdots \oplus V_{2t}.
  \end{equation}
\end{proposition}

\begin{proof}
  We only prove the case when $V = V_1 \oplus V_2$ and $V_1 = V_{11} \oplus
  V_{12}$,
  that is, $V = V_{11} \oplus V_{12} \oplus V_2$.
  First, it is obvious that $V = V_{11} + V_{12} + V_2$.
  Let $0 = \alpha_{11} + \alpha_{12} + \alpha_2 = (\alpha_{11} + \alpha_{12}) + \alpha_2$.
  Since $V = V_1 \oplus V_2$,
  we get $\alpha_{11} + \alpha_{12} = \alpha_2 = 0$.
\end{proof}


\section{Linear Mappings}


\section{Linear Transformations}

\section{Eigenvalues and Similarity Diagonalization}

\section{Minimal Polynomials}



