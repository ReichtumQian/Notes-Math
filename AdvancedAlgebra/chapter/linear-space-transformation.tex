
\section{Linear Spaces}

\subsection{The Concept of Linear Spaces}

\begin{definition}{Linear Space}{}
  Let $\mathbb{P}$ be a number field, $V$ be a non-empty set.
  If two binary operations(addition and scalar multiplication),
  are defined on $V$, and these two operations satisfy the following axioms
  \begin{enumerate}
  \item Binary Operations:
    (1) Commutativity: $\alpha + \beta = \beta + \alpha$;
    (2) Associativity: $(\alpha + \beta) + \gamma = \alpha + (\beta + \gamma)$;
    (3) Existence of zero element: $\exists \theta$ such that for all $\alpha
    \in V$, $\alpha + \theta = \alpha$;
    (4) Existence of negative element: For all $\alpha$, there exists $\beta$ such that $\alpha + \beta
    = \theta$.
  \item Scalar Multiplication:
    (1) Distributivity over vector addition: $c(\alpha + \beta) = c\alpha + c\beta$;
    (2) Distributivity over scalar addition: $(c_1 + c_2)\alpha = c_1 \alpha +
    c_2 \alpha$;
    (3) Associativity of scalar multiplication: $(c_1c_2)\alpha = c_1(c_2\alpha)$;
    (4) Identity scalar multiplication: $1 \cdot \alpha = \alpha$.
  \end{enumerate}
  Then $V$ is said to be a \emph{linear space over $\mathbb{P}$}.
\end{definition}

\begin{example}{Prove a Space is Linear Space}{}
  Prove that the set of all real-coefficient polynomials of degree equal to $n
  (n \geq 1)$, with respect to the addition of polynomial and the scalar multiplication
  by real numbers is a linear space.
\end{example}

\begin{proof}
  Hint: Verify the eight axioms of linear space.
\end{proof}

\begin{definition}{Linear Dependence and Linear Independence}{}
  Let $\alpha_1, \cdots, \alpha_n$ be a sequence of vectors,
  if there exist $n$ numbers $k_1, \cdots, k_n$ in $\mathbb{P}$, not all zero, such that
  \begin{equation}
    k_1 \alpha_1 + \cdots + k_n \alpha_n = 0.
  \end{equation}
  Then they are said to be \emph{linearly dependent}.
  If this equation holds if and only if $k_1 = k_2 = \cdots = k_n = 0$,
  then they are \emph{linearly independent}.
\end{definition}

\begin{example}{Linear Independence of Nilpotent Basis}{}
  Let $\mathcal{A}$ be a linear transformation on $V$,
  and let $\xi$ be a vector in $V$.
  Prove that if $\mathcal{A}^{n - 1}\xi \neq 0, \mathcal{A}^n \xi = 0$ ($k > 1$),
  then
  \begin{equation}
    \xi, \mathcal{A} \xi, \cdots, \mathcal{A}^{n-1}\xi
  \end{equation}
  are linearly independent.
\end{example}

\begin{proof}
  Assume that $k_0 \xi + k_1 \mathcal{A} \xi + \cdots +
  k_{n-1}\mathcal{A}^{n-1}\xi = 0$.
  Apply $\mathcal{A}^{n-1}$ to both sides to get $k_0 \mathcal{A}^{n-1}\xi = 0$.
  Since $\mathcal{A}^{n-1}\xi \neq 0$,
  we get $k_0 = 0$.
  And by repeating this process, we can deduce that $k_0, k_1, \cdots, k_{n-1}$
  are all zeroes.
\end{proof}

\begin{proposition}{Finding the Maximal Linearly Independent Subsets}{}
  Given a set of vectors $\alpha_1, \cdots, \alpha_n$,
  form a matrix with these vectors as column vectors,
  and perform elementary row operations on the matrix to transform it into a
  row-echelon form.
  The vectors corresponding to the columns where the leading $1$s (pivots or
  leading entries) located form a maximal linearly independent subset.
\end{proposition}

\begin{example}{Finding the Maximal Linearly Independent Subsets}{}
  Calculate the maximal linearly independent subsets of the following vector set:
  \begin{equation}
    \alpha_1 = (1, -1, 2, 4), \alpha_2 = (0, 3, 1, 2), \alpha_3 = (3, 0, 7, 14), \alpha_4 = (1, -1, 2, 0), \alpha_5 = (2, 1, 5, 6).
  \end{equation}
\end{example}

\begin{solution}
  Forming a new matrix and performing elementary row operations yield
  \begin{equation}
    \begin{bmatrix}1&0&3&1&2\\-1&3&0&-1&1\\2&1&7&2&5\\4&2&14&0&6\end{bmatrix}\to\begin{bmatrix}1&0&3&1&-2\\0&1&1&0&1\\0&0&0&1&1\\0&0&0&0&0\end{bmatrix},
  \end{equation}
  so the maximal linearly independent subset is $\alpha_1, \alpha_2, \alpha_4$.
\end{solution}

\begin{definition}{Rank of Vector Set}{}
  Let $\alpha_1, \cdots, \alpha_n$ be a vector set,
  the number of vectors in its maximal linearly independent subset is called
  their \emph{rank}.
\end{definition}

\subsection{Bases and Coordinates}

\begin{definition}{Basis}{}
  Let $V$ be a finite-dimensional linear space,
  and $\epsilon_1, \cdots, \epsilon_n$ be linearly independent in $V$.
  If any $\alpha \in V$ can be linearly represented by this set of vectors,
  then this set of vectors are called the \emph{basis} of $V$.
\end{definition}

\begin{example}{Proving a Basis}{}
  Given $\alpha_1 = (1, 1, \cdots, 1)$,
  $\alpha_2 = (1, 1, \cdots, 1, 0)$,
  $\cdots$,
  $\alpha_n = (1, 0, \cdots, 0)$.
  Prove that $\alpha_1, \cdots, \alpha_n$ is a basis of $\mathbb{R}^n$.
\end{example}

\begin{proof}
  Hint: Just arrange them into a matrix and check its full-rank property.
\end{proof}

\begin{definition}{Coordinates}{}
  Let $V$ be a finite-dimensional linear space,
  and $\epsilon_1, \cdots, \epsilon_n$ be a basis of $V$.
  If a vector $\alpha \in V$ can be represented as
  \begin{equation}
    \alpha = (\epsilon_1, \cdots, \epsilon_n)X,
  \end{equation}
  where $X \in \mathbb{P}^n$ is called the \emph{coordinate of $\alpha$
  under the basis $\epsilon_1,\cdots, \epsilon_n$}.
\end{definition}

\begin{example}{Calculating Coordinates}{}
  Given that $\epsilon_{1}, \epsilon_2, \epsilon_3$ is a basis of
  $\mathbb{R}^3$,
  calculate the coordinate of $\alpha = (1, 2, 1)^T$ under this basis, where
  \begin{equation}
    \epsilon_1=(1,1,1)^T,\epsilon_2=(1,1,-1)^T,\epsilon_3=(1,-1,-1)^T.
    \end{equation}
\end{example}

\begin{solution}
  Let the coordinate of $\alpha$ under $\epsilon_1,\epsilon_2,\epsilon_3$ be
  $x$, then
  \begin{equation}
    \alpha = (\epsilon_1,\epsilon_2,\epsilon_3)x.
  \end{equation}
  That is, solve the system of linear equations
  \begin{equation}
    \begin{bmatrix}
      1 & 1 & 1\\
      1 & 1 & -1\\
      1 & -1 & -1
    \end{bmatrix}x =
    \begin{bmatrix}
      1\\
      2\\
      1
    \end{bmatrix}.
  \end{equation}
  And the solution of the above system of linear equation is $x = [1,
  \frac{1}{2}, - \frac{1}{2}]^T$.
\end{solution}

\begin{definition}{Transition Matrix}{}
  Given two bases $(\epsilon_1, \cdots, \epsilon_n)$
  and $(\eta_1, \cdots, \eta_n)$, if there exists a matrix $T$ such that
  \begin{equation}
    (\epsilon_1, \cdots, \epsilon_n) = (\eta_1, \cdots, \eta_n)T,
  \end{equation}
  then $T$ is called the \emph{transition matrix from $\eta_1, \cdots, \eta_n$
  to $\epsilon_1, \cdots, \epsilon_n$}.
\end{definition}

\begin{proposition}{Coordinate Transformation}{}
  Let $V$ be an $n$-dimensional linear space, $\eta_1,\cdots,\eta_n$ and
  $\epsilon_1,\cdots, \epsilon_n$ be two sets of bases in the space.
  Suppose $T$ is the transition matrix from $\eta_i$ to $\epsilon_i$,
  then for any $\alpha \in V$, if its coordinates under the bases $\epsilon_i$
  and $\eta_i$ are $X$ and $Y$ respectively, then
  \begin{equation}
    Y = TX.
  \end{equation}
\end{proposition}

\begin{proof}
  By the definition of the coordinate and transition matrix, we get
  \begin{equation}
    \alpha = (\epsilon_1,\cdots,\epsilon_n)X
    = (\eta_1,\cdots,\eta_n)TX.
  \end{equation}
  At the same time $\alpha = (\eta_1,\cdots,\eta_n)Y$,
  comparing the above two equations implies $Y = TX$.
\end{proof}

\begin{note}{}
  Calculation of Transition Matrix:
  By elementary row operations find $(\eta_1, \cdots, \eta_n)^{-1} (\epsilon_1,
  \cdots, \epsilon_n)$.
\end{note}

\begin{example}{Calculation of Transition Matrix}{}
  Given the following two bases, (1) find the transition matrix between them
  (2) find the coordinate of $\xi = (1, 0, 0, -1)^T$ under $\alpha_1, \alpha_2,
  \alpha_3, \alpha_4$.
  \begin{equation}
    \alpha_1=(1,1,1,1),\alpha_2=(1,1,-1,-1),\alpha_3=(1,-1,1,-1),\alpha_4=(1,-1,-1,1)
  \end{equation}
  \begin{equation}
    \beta_1=(1,1,0,1),\beta_2=(2,1,3,1),\beta_3=(1,1,0,0),\beta_4=(0,1,-1,-1)
  \end{equation}
\end{example}

\begin{solution}
  (1) Hint: Construct matrix $A, B$ and calculate the transition matrix by
  elementary row operations $(A|B) \rightarrow (I|A^{-1}B)$. The answer is
  \begin{equation}
    A^{-1}B=\frac{1}{4}\left[\begin{array}{cccc}3&7&2&-1\\1&-1&2&3\\-1&3&0&-1\\1&-1&0&-1\end{array}\right]
  \end{equation}

  (2) Hint: Solve a system of linear equations, and the final answer is $[0,
  0.5, 0.5, 0]^T$.
\end{solution}

\begin{definition}{Isomorphism}{}
  If there exists a bijection between two algebraic systems,
  and this bijection preserves the operations of the algebraic systems,
  then the two algebraic systems are \emph{isomorphic}.
\end{definition}

\begin{theorem}{Isomorphism Mapping of Finite-Dimensional Linears Spaces}{}
  Let $V$ be an $n$-dimensional linear space over $\mathbb{P}$,
  then it is isomorphic with $\mathbb{P}^n$.
\end{theorem}

\begin{proof}
  Suppose that $\alpha_1, \cdots, \alpha_n$ is a basis of the space $V$,
  we can construct a linear mapping $f$ that maps $\alpha \in V$ to
  its coordinate over $\alpha_1, \cdots, \alpha_n$.
  Then $f$ is an isomorphism mapping from $V$ to $\mathbb{P}^n$,
  thus $V$ is isomorphic with $\mathbb{P}^n$.
\end{proof}

\begin{example}{Applications of Isomorphism}{}
  \begin{itemize}
  \item Let $\beta_1 = \alpha_1 + \alpha_2, \beta_2 = \alpha_1 + \alpha_2,
    \beta_3 = \alpha_2 + \alpha_3$. Prove that $\alpha_1, \alpha_2, \alpha_3$
    and $\beta_1, \beta_2, \beta_3$ are equivalent.
  \item Let $\beta_1 = \alpha_2 + \cdots + \alpha_n,
    \beta_2 = \alpha_1 + \alpha_3 + \cdots + \alpha_n,
    \beta_n = \alpha_1 + \cdots + \alpha_{n-1}$.
    Prove that $\alpha_1, \cdots, \alpha_n$ and $\beta_1, \cdots, \beta_n$ have
    the same rank.
  \end{itemize}
\end{example}

\begin{proof}
  (1) It is equivalent to prove that there exists an invertible matrix $T$ such that
  $(\beta_1, \beta_2, \beta_3) = (\alpha_1, \alpha_2, \alpha_3)T$.
  So it is similar to find the transition matrix between them, i.e.,
  apply elementary row operations to find $T = A^{-1}B$.
\end{proof}

\subsection{Sum Operations and Dimension Theorem}

\begin{definition}{Subspace}{}
  Let $V$ be a linear space over $\mathbb{P}$,
  and $W$ be a non-empty subset of $V$.
  If $W$ also forms a linear space over $\mathbb{P}$ with respect to
  the addition operation and scalar multiplication operation of $V$,
  then $W$ is called a \emph{subspace of $V$}.
\end{definition}

\begin{proposition}{Sufficient and Necessary Condition for Subspaces}{}
  Let $V$ be a linear space over $\mathbb{P}$,
  and $W$ be a non-empty subset of $V$.
  Then $W$ is a subspace of $V$ if and only if
  it is closed under the addition and scalar multiplication of $V$.
\end{proposition}

\begin{proof}
  It is direct, so we omit the proof here.
\end{proof}

\begin{definition}{Sum of Linear Spaces}{}
  Let $V_1$ and $V_2$ be two linear spaces, then their \emph{sum} is
  \begin{equation}
    V_1 + V_2 := \{\alpha = \alpha_1 + \alpha_2: \alpha_1 \in V_1, \alpha_2 \in V_2\}.
  \end{equation}
\end{definition}

\begin{proposition}{Sum and Intersection of Linear Spaces Are Linear Spaces}{}
  Let $V_1$ and $V_2$ be two linear spaces,
  then $V_1 \cap V_2$ and $V_1 + V_2$ are linear spaces.
\end{proposition}

\begin{proof}
  We first prove $V_1 \cap V_2$ is a linear space.
  Choose $u, v \in V$ and $c \in \mathbb{P}$, then $u, v \in V_1$ and $u, v \in V_2$,
  \begin{enumerate}
  \item Closure under addition: Since $u + v \in V_1, V_2$, we have $u + v \in
    V$, $V$ is closed under addition.
  \item Closure under scalar multiplication: $c v \in V_1, c v \in V_2$,
    thus $c v \in V$.
  \end{enumerate}
  Therefore, we get $V_1 \cap V_2$ is a linear space.
  Next prove that $V_1 + V_2$ is a subspace.
  \begin{enumerate}
  \item Closure under addition: Let $u = u_1 + u_2$ and $v = v_1 + v_2$,
    where $u_1, v_1 \in V_1$ and $u_2, v_2 \in V_2$.
    Then $u + v = (u_1 + v_1) + (u_2 + v_2)$, and $u_1 + v_1 \in V_1$, $u_2 +
    v_2 \in V_2$.
  \item Closure under scalar multiplication: $cu = cu_1 + cu_2$,
    where $cu_1 \in V_1$ and $cu_2 \in V_2$.
  \end{enumerate}
  Thus $V_1 + V_2$ is a linear space.
\end{proof}

\begin{example}{Basic Calculations of Intersection and Sum}{}
  Given $V_1 = \operatorname{span}(\alpha_1, \alpha_2, \alpha_3)$,
  and $V_2 = \operatorname{span}(\beta_1, \beta_2)$,
  where
  \begin{equation}
    \alpha_1=\begin{pmatrix}1\\2\\1\\0\end{pmatrix},\alpha_2=\begin{pmatrix}-1\\1\\1\\1\end{pmatrix},\alpha_3=\begin{pmatrix}0\\3\\2\\1\end{pmatrix},\beta_1=\begin{pmatrix}2\\-1\\0\\1\end{pmatrix},\beta_2=\begin{pmatrix}1\\-1\\3\\7\end{pmatrix}.
  \end{equation}
  Find the basis and dimension of $V_1 + V_2$, $V_1 \cap V_2$.
\end{example}

\begin{solution}
  (1) Hint: $V_1 + V_2 = \operatorname(\alpha_1, \alpha_2, \alpha_3, \beta_1,
  \beta_2)$,
  arrange them in a matrix and perform elementary row-operations.

  (2) Hint: for any $\alpha \in V_1 \cap V_2$,
  assume
  \begin{equation}
    \alpha = x_1 \alpha_1 + x_2 \alpha_2 + x_3 \alpha_3 = y_1 \beta_1 + y_2 \beta_2,
  \end{equation}
  then the coefficients need to satisfy the system of equations:
  \begin{equation}
    x_1 \alpha_1 + x_2 \alpha_2 + x_3 \alpha_3 - y_1 \beta_1 - y_2 \beta_2 = 0.
  \end{equation}
  Let $x_4 = -y_1, x_5 = y_2$, that is, solve $AX = 0$.
  And the fundamental solution system is
  \begin{equation}
    \left.\eta_1=\left(\begin{array}{c}-1\\-1\\1\\0\\0\end{array}\right.\right),\eta_2=\left( \begin{array}{c}1\\-4\\0\\-3\\1\end{array} \right).
  \end{equation}
  This implies the dimension of $V_1 \cap V_2$ is $2$,
  with the basis being $\eta_1, \eta_2$.
\end{solution}

\begin{example}{The Differences between Sum and Union}{}
  \begin{enumerate}
  \item Let $V_1$ and $V_2$ be two non-trivial subspaces of the linear space $V$ over
    the number field $\mathbb{P}$. Prove that there exists $\alpha \in V$ such that
    $\alpha \not\in V_1, \alpha \not \in V_2$.
  \item Let $V_1$ and $V_2$ be two subspaces of $V$,
    prove that $V_1 + V_2 = V_1 \cup V_2$ if and only if $V_1 \subseteq V_2$ or
    $V_2 \subseteq V_1$.
  \end{enumerate}
\end{example}

\begin{proof}
  (1) Since $V_1$ is a non-trivial subspace, there exists $\alpha \in V$,
  such that $\alpha \not\in V_1$. If $\alpha \not \in V_2$, then the conclusion
  holds.
  Otherwise, if $\alpha \in V_2$, there exists $\beta \in V$ but
  $\beta \not\in V_2$.
  If $\beta \not \in V_1$, the conclusion holds. Otherwise, $\beta \in V_1$,
  then we have
  \begin{equation}
    \alpha \not\in V_1, \beta \in V_1, \alpha \in V_2, \beta \not \in V_2.
  \end{equation}
  Let $\gamma = \alpha + \beta$, it satisfies $\gamma \not \in V_1, \gamma \not \in V_2$.

  (2) The right-to-left direction is direct, so we only prove the left-to-right direction.
  Assume that $V_1 + V_2 = V_1 \cup V_2$, but $V_1 \not\subseteq V_2$,
  then there exists $\alpha_1 \in V_1, \alpha_1 \not \in V_2$.
  For all $\alpha_2 \in V_2$, denote
  \begin{equation}
    \alpha = \alpha_1 + \alpha_2,
  \end{equation}
  Since $V_1 + V_2 = V_1 \cup V_2$,
  $\alpha$ should belong to either $V_1$ or $V_2$.
  According to the closure of addition, we know $\alpha \not \in V_2$,
  which means that $\alpha \in V_1$.
  Then $\alpha_2 = \alpha - \alpha_1 \in V_1$, that is,
  $V_2 \subseteq V_1$.
\end{proof}

\begin{theorem}{Dimension Theorem}{}
  Let $V_1$ and $V_2$ be two subspaces of the finite-dimensional linear space
  $V$. Then
  \begin{equation}
    \dim V_1 + \dim V_2 = \dim (V_1 + V_2) + \dim (V_1 \cap V_2).
  \end{equation}
\end{theorem}

\begin{proof}
  We start from a small space and expand the basis.
  Let the basis of $V_1 \cap V_2$ be $\alpha_1,\cdots,\alpha_s$,
  and the bases of $V_1, V_2$ be respectively:
  \begin{equation}
    \alpha_1,\cdots,\alpha_s,\beta_{s+1},\cdots,\beta_m; \quad 
    \alpha_1,\cdots,\alpha_s,\gamma_{s+1},\cdots,\gamma_n,
  \end{equation}
  Then $V_1 + V_2 = \operatorname{span}(\alpha_1,\cdots,\alpha_s,\beta_{s+1},\cdots,\beta_m,\gamma_{s+1},\cdots,\gamma_n)$.
  Next, we prove that the above-mentioned vectors are linearly independent.
  Without loss of generality, assume
  \begin{equation}
    k_1\alpha_1+\cdots+k_s\alpha_s+k_{s+1}\beta_{s+1}+\cdots+k_n\beta_n+l_{s+1}\gamma_{s+1}+\cdots+l_n\gamma_n=0.
  \end{equation}
  Move the part with $l_i \gamma_i$ to the right-hand side.
  At this time, the left-hand side linear combination belongs to $V_1$,
  and the right-hand side belongs to $V_2$,
  so it belongs to $V_1 \cap V_2$.
  So it is not hard to find that $k_{s+1} = k_{s+2} = \cdots = k_n = 0$,
  and $l_{s+1} = \cdots = l_n = 0$.
  Since the vector set $\alpha_1, \cdots, \alpha_s$ is a basis,
  $\alpha_1,\cdots,\alpha_s$ are linearly independent. Thus all the coefficients are zero,
  which implies the linear independence.
\end{proof}

\begin{example}{Application of the Dimension Theorem}{}
  Let $V_1, V_2$ be two subspace of a finite-dimensional space $V$,
  satisfying
  \begin{equation}
    \operatorname{dim}(V_1 + V_2) = \operatorname{dim}(V_1 \cap V_2) + 1,
  \end{equation}
  prove that either $V_1 \subseteq V_2$ or $V_2 \subseteq V_1$.
\end{example}

\begin{proof}
  The dimension theorem yields
  \begin{equation}
    \operatorname{dim}(V_1) + \operatorname{dim}(V_2)
    = \operatorname{dim}(V_1 + V_2) + \operatorname{dim}(V_1 \cap V_2)
    = 2 \operatorname{dim}(V_1 \cap V_2) + 1,
  \end{equation}
  which means
  \begin{equation}
    [\operatorname{dim}(V_1) - \operatorname{dim}(V_1 \cap V_2)]
    + [\operatorname{dim}(V_2) - \operatorname{dim}(V_1 \cap V_2)] = 1.
  \end{equation}
  Since all the dimensions are integers,
  then one of the above terms is $0$, while the other one is $1$.
\end{proof}


\subsection{Direct-Sum Theory}

\begin{definition}{Direct Sum}{}
  Let $V_1$ and $V_2$ be subspaces of the linear space $V$.
  If $V_1 \cap V_2 = \{0\}$,
  then we call the sum of $V_1, V_2$ the \emph{direct sum},
  denoted as
  \begin{equation}
    V_1 \oplus V_2 := V_1 + V_2.
  \end{equation}
\end{definition}

\begin{definition}{Complementary Space}{}
  If $V = V_1 \oplus V_2$,
  then $V_2$ is called the \emph{complementary space of $V_1$}.
\end{definition}

\begin{proposition}{Essential Equivalent Conditions for Direct Sum}{}
  Let $V_1 = \operatorname{span}(\epsilon_1,\cdots,\epsilon_n),
  V_2 = \operatorname{span}(\eta_1,\cdots,\eta_m)$ be two subspaces of $V$.
  Then $V_1 + V_2 = V_1 \oplus V_2$ if and only if the vectors
  \begin{equation}
    \epsilon_1,\cdots,\epsilon_n,\eta_1,\cdots,\eta_m
  \end{equation}
  are linearly independent.
\end{proposition}

\begin{proof}
  Right-to-left: Let $\alpha \in V_1 \cap V_2$, then
  \begin{equation}
    \alpha = k_1\epsilon_1 + \cdots + k_n\epsilon_n = l_1\eta_1 + \cdots + l_m\eta_m,
  \end{equation}
  which implies $k_1 \epsilon_1 + \cdots + k_n \epsilon_n - l_1\eta_1 - \cdots -
  l_m \eta_m = 0$.
  Apply the given condition, we know all the coefficents are zero,
  thus $\alpha = 0$, and $V_1 \cap V_2 = \{0\}$.

  Left-to-right:
  Assume that $k_1\epsilon_1 + \cdots + k_n\epsilon_n + l_1\eta_1 + \cdots +
  l_m\eta_m = 0$, which yields
  \begin{equation}
    k_1\epsilon_1 + \cdots + k_n\epsilon_n = -l_1\eta_1 - \cdots - l_m\eta_m.
  \end{equation}
  Define the both the left-side and right-side term in above equation as
  $\alpha$, then $\alpha \in V_1 \cap V_2$.
  According to the definition of direct sum,
  $V_1 \cap V_2 = \{0\}$, which means $\alpha = 0$.
  And the linear independence of $\epsilon_1,\cdots,\epsilon_n$ and
  $\eta_1,\cdots,\eta_m$
  implies all the coefficents are zero.
\end{proof}

\begin{proposition}{Other Equivalent Conditions for Direct Sum}{}
  The equation $V = V_1 \oplus V_2$ holds if and only if:
  \begin{itemize}
  \item Any vector $\alpha$ in $V$ has a unique representation $\alpha =
    \alpha_1 + \alpha_2$, where $\alpha_1 \in V_1$ and $\alpha_2 \in V_2$.
  \item The zero element in $V$ is uniquely represented as the sum of
    the zero elements in $V_1$ and $V_2$.
  \item $\operatorname{dim} V = \operatorname{dim} V_1 + \operatorname{dim} V_2$.
  \end{itemize}
\end{proposition}

\begin{proof}
  Hint: Use the essential equivalent condition.
\end{proof}

\begin{proposition}{Direct-Sum Combination Theorem}{}
  If we have $V = V_1 \oplus V_2$,
  where $V_1 = V_{11} \oplus V_{12} \oplus \cdots \oplus V_{1s}$,
  and $V_2 = V_{21} \oplus \cdots \oplus V_{2t}$,
  then
  \begin{equation}
    V = V_{11} \oplus \cdots \oplus V_{1s} \oplus V_{21} \oplus \cdots \oplus V_{2t}.
  \end{equation}
\end{proposition}

\begin{proof}
  We only prove the case when $V = V_1 \oplus V_2$ and $V_1 = V_{11} \oplus
  V_{12}$,
  that is, $V = V_{11} \oplus V_{12} \oplus V_2$.
  First, it is obvious that $V = V_{11} + V_{12} + V_2$.
  Let $0 = \alpha_{11} + \alpha_{12} + \alpha_2 = (\alpha_{11} + \alpha_{12}) + \alpha_2$.
  Since $V = V_1 \oplus V_2$,
  we get $\alpha_{11} + \alpha_{12} = \alpha_2 = 0$.
\end{proof}


\section{Linear Mappings and Linear Transformations}

\subsection{The Concept of Linear Mappings and Linear Transformations}

\begin{definition}{Linear Mapping}{}
  Let $V$ and $W$ be linear spaces over $\mathbb{P}$.
  A mapping $\varphi: V \rightarrow W$ is called a \emph{linear mapping from $V$
  to $W$}
  if it satisfies:
  \begin{equation}
    \varphi(\alpha + \beta) = \varphi(\alpha) + \varphi(\beta), \quad
    \varphi(c \alpha) = c \varphi(\alpha),
  \end{equation}
  where $\alpha, \beta$ are arbitrary vectors in $V$,
  and $c$ is a contant in $\mathbb{P}$.
\end{definition}

\begin{note}
  The set of all linear mappings from $V$ to $W$ is denoted as $\operatorname{Hom}_P(V, W)$.
\end{note}

\begin{definition}{Linear Transformation}{}
  Let $V$ be a linear space over $\mathbb{P}$.
  $\mathcal{A}$ is a linear mapping from $V$ to itself,
  then $\mathcal{A}$ is called a \emph{linear transformation on $V$}.
\end{definition}

\subsection{Kernel and Image of Linear Mappings}

The kernel and the image are the most common invariant subspaces,
thus we need to investigate these two special sets.

\begin{definition}{Kernel and Image}{}
  Let $U$ and $V$ be linear spaces over $\mathbb{P}$,
  and $\varphi: U \rightarrow V$.
  Define the \emph{kernel} and \emph{image} of $\varphi$
  \begin{equation}
    \operatorname{Ker} (\varphi) = \{\alpha \in U: \varphi (\alpha) = 0\}; \quad
    \operatorname{Im} (\varphi) = \{\varphi (\alpha) : \alpha \in U\}.
  \end{equation}
\end{definition}

\begin{example}{Calculate Kernel and Image}{}
  Consider a linear mapping $\mathcal{A}: \mathbb{P}^4 \rightarrow
  \mathbb{P}^3$,
  find $\operatorname{Ker} \mathcal{A}$ and $\operatorname{Im} \mathcal{A}$, where
  \begin{equation}
    \mathcal{A}\begin{pmatrix}
    x_1 \\
    x_2 \\
    x_3 \\
    x_4
    \end{pmatrix}
    =
    \begin{pmatrix}
    x_1 - 3x_2 + x_3 - 2x_4 \\
    -x_1 - 11x_2 + 2x_3 - 5x_4 \\
    3x_1 + 5x_2 + x_4
    \end{pmatrix}
  \end{equation}
\end{example}

\begin{solution}
  $\operatorname{Ker} \mathcal{A}$ is the solution space of $AX = 0$,
  while $\operatorname{Im} \mathcal{A}$ is the column space of $A$.
  And the reason for why the image of $\mathcal{A}$ is the column space of $A$ is
  \begin{equation}
    A \left(
      \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        x_4
      \end{array}
    \right) = \mathbf{a}_1x_1 + \mathbf{a}_2x_2 + \mathbf{a}_3x_3 + \mathbf{a}_4x_4,
  \end{equation}
  where $\mathbf{a}_i$ is the $i$-th column vector of $A$.
\end{solution}

\begin{theorem}{Dimension Theorem for the Kernel and Image}{}
  For any $\varphi \in \operatorname{Hom}(U, V)$,
  its kernel and image satisfy
  \begin{equation}
    \dim \operatorname{Ker} (\varphi) + \dim \operatorname{Im} (\varphi)
    = \dim U.
  \end{equation}
\end{theorem}

\begin{proof}
  Let $\alpha_1,\cdots,\alpha_r$ be a basis of $\operatorname{Ker}(\varphi)$,
  and $\alpha_1,\cdots,\alpha_n$ be a basis of $U$.
  Then we have
  \begin{equation}
    \operatorname{Im}(\varphi) = \operatorname{span}(\varphi(\alpha_1),\cdots,\varphi(\alpha_n))
    = \operatorname{span}(\varphi(\alpha_{r+1}),\cdots,\varphi(\alpha_n)).
  \end{equation}
  Next, we prove that $\varphi(\alpha_{r+1}),\cdots,\varphi(\alpha_n)$ are
  linearly independent.
  Suppose $k_{r+1}\varphi(\alpha_{r+1}) + \cdots + k_n \varphi(\alpha_n) = 0$,
  according to the linearity of $\varphi$, we have
  \begin{equation}
    \varphi(k_{r+1}\alpha_{r+1} + \cdots + k_n\alpha_n) = 0,
  \end{equation}
  that is, $k_{r+1}\alpha_{r+1} + \cdots + k_n \alpha_n \in \operatorname{Ker}(\varphi)$.
  According to the linear independence of the basis,
  we have $k_{r+1} = \cdots = k_n = 0$.
\end{proof}

\begin{example}{Generalization of the Dimension Theorem}{}
  Suppose $\mathcal{A}$ is a linear transformation on a finite-dimensional
  linear space $V$,
  and $W$ is a subspace of $V$, prove that
  \begin{equation}
    \dim \mathcal{A} W + \dim (\operatorname{Ker} \mathcal{A} \cap W) = \dim W
  \end{equation}
\end{example}

\begin{proof}
  Let $\alpha_1,\cdots,\alpha_r$ be a basis of the space $\operatorname{Ker}
  \mathcal{A} \cap W$,
  and $\alpha_1,\cdots,\alpha_n$ be a basis of $W$.
  Then we have $\mathcal{A}W = \operatorname{span} (\mathcal{A}\alpha_{r+1},\cdots,\mathcal{A}\alpha_n)$.
  Next, we prove that $\mathcal{A}\alpha_{r+1},\cdots,\mathcal{A}\alpha_n$ are
  linearly independent, assume
  \begin{equation}
    k_{r+1}\alpha_{r+1} + \cdots + k_n \alpha_n = 0.
  \end{equation}
  By the linear property of $\mathcal{A}$,
  we have $k_{r+1}\alpha_{r+1} + \cdots + k_n\alpha_n \in
  \operatorname{Ker}\mathcal{A} \cap W$,
  thus $k_{r+1} = \cdots = k_n = 0$.
\end{proof}

\subsection{Kernel and Image of Idempotent Linear Transformations}

Generally speaking, although there is a dimensional relationship between the
kernel and image, they do not have a linearly independent relationship
(they may intersect, or their sum may not be the entire space).
However, idempotent transformations are an exception.

\begin{definition}{Idempotent Transformation}{}
  If a linear transformation $\mathcal{A}$ satisfies $\mathcal{A}^2 =
  \mathcal{A}$,
  then it is called a \emph{idempotent transformation}.
\end{definition}

\begin{theorem}{Decomposition of the Kernel and Image Spaces of Idempotent Transformations}{}
  Suppose $\mathcal{A}$ is an idempotent transformation on the linear space $V$,
  then $V$ can be decomposted into the direct sum of the kernel and image of
  $\mathcal{A}$,
  that is,
  \begin{equation}
    V = \operatorname{Ker} \mathcal{A} \oplus \operatorname{Im} \mathcal{A}.
  \end{equation}
\end{theorem}

\begin{proof}
  We first prove that $V$ is the sum of the kernel and image of $\mathcal{A}$.
  For any $\alpha \in V$, we have
  \begin{equation}
  \alpha = \alpha - \mathcal{A} \alpha + \mathcal{A} \alpha,
  \end{equation}
  where $\mathcal{A} \alpha \in \operatorname{Im}(\mathcal{A})$,
  and $\alpha - \mathcal{A}\alpha \in \operatorname{Ker}\mathcal{A}$ since
  \begin{equation}
    \mathcal{A}(\alpha - \mathcal{A} \alpha) = \mathcal{A} \alpha - \mathcal{A} \alpha = 0.
  \end{equation}

  Next, we prove the sum is direct sum.
  For any $\alpha \in \operatorname{Ker} \mathcal{A} \cap \operatorname{Im}
  \mathcal{A}$,
  we have $\mathcal{A} \alpha = 0$ and there exists $\beta \in V$ such that
  $\mathcal{A} \beta = \alpha$.
  Applying $\mathcal{A}$ to both sides yields
  \begin{equation}
    \mathcal{A}^2 \beta = \mathcal{A} \alpha \Rightarrow
    \mathcal{A}^2 \beta = \mathcal{A} \beta = \alpha = \mathcal{A} \alpha.
  \end{equation}
  Since $\mathcal{A} \alpha = 0$, we know that $\alpha = 0$.
\end{proof}

\begin{theorem}{Decomposition of the Kernel and Image Spaces of General Linear Transformations}{}
  Suppose $\mathcal{A}$ is linear transformation on $V$,
  and $\dim V = n$. Then we have
  \begin{equation}
    V = \operatorname{Ker} \mathcal{A}^n \oplus \operatorname{Im} \mathcal{A}^n.
  \end{equation}
\end{theorem}

\subsection{The Matrices of Linear Transformations}

\begin{definition}{The Matrix of a Linear Transformation}{}
  Let $\mathcal{A}$ be a linear transformation in $V$,
  and $V = \operatorname{span} (\epsilon_1,\cdots,\epsilon_n)$.
  If
  \begin{equation}
    (\mathcal{A}(\epsilon_1),\cdots,\mathcal{A}(\epsilon_n)) = (\epsilon_1,\cdots,\epsilon_n)A,
  \end{equation}
  then we call $A$ \emph{the matrix of $\mathcal{A}$ with respect to the basis $\epsilon_1,\cdots,\epsilon_n$}.
\end{definition}

\begin{note}
  How to find the matrix of a linear transformation with respect to a basis:
  \begin{equation}
    A = (\epsilon_1,\cdots,\epsilon_n)^{-1}(\mathcal{A}\epsilon_1,\cdots,\mathcal{A}\epsilon_n).
  \end{equation}
  We can calculate it via row elementary operations.
\end{note}

\begin{example}{Find the Matrix of Linear Transformations}{}
  Given that $A = (a_{ij}), i,j = 1,2,3$ is the matrix of $\mathcal{A}$
  with respect to the basis $\epsilon_1,\epsilon_2,\epsilon_3$. Calculate
  \begin{enumerate}
  \item The matrix of $A$ with respect to the basis $\epsilon_3, \epsilon_2, \epsilon_1$;
  \item The matrix of $A$ with respect to the basis $\epsilon_1 + \epsilon_2, \epsilon_2, \epsilon_3$;
  \end{enumerate}
\end{example}

\begin{solution}
  (1) Switch the first column and the third column of the matrix.
  (2) By the definition of linear transformation, we get
  \begin{equation}
    \begin{cases}
      \mathcal{A}\epsilon_1 = a_{11}\epsilon_1 + a_{21}\epsilon_2 + a_{31}\epsilon_3 \\
      \mathcal{A}\epsilon_2 = a_{12}\epsilon_1 + a_{22}\epsilon_2 + a_{32}\epsilon_3 \\
      \mathcal{A}\epsilon_3 = a_{13}\epsilon_1 + a_{23}\epsilon_2 + a_{33}\epsilon_3
    \end{cases}
  \end{equation}
  and
  \begin{equation}
    \begin{cases}
      \mathcal{A}(\epsilon_1 + \epsilon_2)= (a_{11} + a_{12})\epsilon_1 + (a_{21} + a_{22})\epsilon_2 + (a_{31} + a_{32})\epsilon_3 \\
      \mathcal{A}\epsilon_2 = a_{12}\epsilon_1 + a_{22}\epsilon_2 + a_{32}\epsilon_3 \\
      \mathcal{A}\epsilon_3 = a_{13}\epsilon_1 + a_{23}\epsilon_2 + a_{33}\epsilon_3
    \end{cases}
  \end{equation}
\end{solution}

\begin{proposition}{Coordinates After Linear Transformation}{}
  If the coordinate of $\alpha$ with respect to the basis
  $(\epsilon_1,\cdots,\epsilon_n)$ is $X$,
  then the coordinates $\mathcal{A}\alpha$ with respect to this basis is $AX$,
  that is,
  \begin{equation}
    \mathcal{A}\alpha = (\epsilon_1,\cdots,\epsilon_n)AX
  \end{equation}
\end{proposition}

\begin{proof}
  By the difinition of coordinates, we have $\alpha =
  (\epsilon_1,\cdots,\epsilon_n)X$,
  and
  \begin{equation}
    \mathcal{A}\alpha = (\mathcal{A}\epsilon_1,\cdots,\mathcal{A}\epsilon_n)X = (\epsilon_1,\cdots,\epsilon_n)AX.
  \end{equation}
  That is, the coordinate of $\mathcal{A}\alpha$ with respect to
  $\epsilon_1,\cdots,\epsilon_n$ is $AX$.
\end{proof}

\subsection{Similarity: Matrices of Linear Transformation under Different Bases}

\begin{theorem}{Matrices of a Linear Transformation with Respect to Different Bases}{}
  Let $(\epsilon_1,\cdots,\epsilon_n)$ and $(\eta_1,\cdots,\eta_n)$ be two bases
  of the linear space $V$,
  and the transition matrix from $\eta_i$ to $\epsilon_i$ is $T$.
  If the matrices of the linear transformation $\mathcal{A}$ with respect to the
  bases $\epsilon_i$ and $\eta_i$ are $A$ and $B$ respectively, then
  \begin{equation}
    A = T^{-1}BT.
  \end{equation}
\end{theorem}

\begin{proof}
  By the definition of the transition matrix, we have
  \begin{equation}
    (\epsilon_1,\cdots,\epsilon_n) = (\eta_1,\cdots,\eta_n)T.
  \end{equation}
  And by the definition of matrix of a linear transformation, we have
  \begin{equation}
    \mathcal{A}(\epsilon_1,\cdots,\epsilon_n) = (\epsilon_1,\cdots,\epsilon_n) A, \quad
    \mathcal{A}(\eta_1,\cdots,\eta_n) = (\eta_1,\cdots,\eta_n)B.
  \end{equation}
  Combine the above three equations we get
  \begin{equation}
    \mathcal{A}(\epsilon_1,\cdots,\epsilon_n) = \mathcal{A}(\eta_1,\cdots,\eta_n)T = (\eta_1,\cdots,\eta_n)BT
    = (\epsilon_1,\cdots,\epsilon_n)T^{-1}BT,
  \end{equation}
  thus $A = T^{-1}BT$.
\end{proof}

\begin{definition}{Similarity}{}
  Two square matrices $A$ and $B$ of the same size $n \times n$ are said to be
  \emph{similar} if there exists an invertible matrix $T$ such that
  \begin{equation}
    A = T^{-1}BT.
  \end{equation}
\end{definition}

\begin{proposition}{Properties of Similarity}{}
  If matrices $A$ and $B$ are similar, that is $A = T^{-1}BT$,
  then:
  \begin{itemize}
  \item $r(A) = r(B)$;
  \item $A^m$ is similar to $B^{m}$;
  \item If $f(x)$ is a polynomial, then $f(A)$ is similar to $B$;
  \end{itemize}
\end{proposition}

\begin{note}
  Note that if $A_1, B_1$ are similar, $A_2, B_2$ are similar,
  it is not always the case that $A_1 + A_2$ is similar to $B_1 + B_2$.
\end{note}

\section{Eigenvalues and Similarity Diagonalization}

\subsection{Eigenvalues and Eigenvectors}

\begin{definition}{Eigenvalue and Eigenvector}{}
  Let $\mathcal{A}$ be a linear transformation on the linear space $V$ over the number
  field $\mathbb{P}$.
  If for $\lambda \in \mathbb{P}$, there exists $\xi \neq 0$,
  such that
  \begin{equation}
    \mathcal{A} \xi = \lambda \xi.
  \end{equation}
  Then we call $\lambda$ an \emph{eigenvalue of $\mathcal{A}$},
  and $\xi$ an \emph{eigenvector of $\mathcal{A}$ belonging to $\lambda$}.
\end{definition}

\begin{definition}{Eigenspace}{}
  Given $\lambda \in \mathbb{P}$,
  its \emph{eigenspace} is defined as the set $V_{\lambda}$
  composed of all eigenvectors $\xi$ satisfying
  \begin{equation}
    \mathcal{A} \xi = \lambda \xi.
  \end{equation}
\end{definition}

\begin{proposition}{Eigenspace is a Subspace}{}
  The eigenspace $V_{\lambda}$ is a subspace of $V$.
\end{proposition}

\begin{proof}
  It is only needs to prove that $V_{\lambda}$ is closed under the addition and
  scalar multiplication in $V$.
  Consider $\xi, \eta \in V_{\lambda}$, they satisfying $\mathcal{A}\xi =
  \lambda \xi$ and $\mathcal{A}\eta = \lambda \eta$,
  then we get
  \begin{equation}
    \mathcal{A}(\xi + \eta) = \lambda (\xi + \eta),
  \end{equation}
  which means $\xi + \eta$ is in the space $V_{\lambda}$.
  Similar reasoning can be used to prove the closure of scalar multiplication.
\end{proof}

\begin{proposition}{The Necessary and Sufficient Condition for Non-zero Eigenspace}{}
  For $\lambda \in \mathbb{P}$, its eigenspace $V_{\lambda} \neq \{0\}$ if and
  only if
  \begin{equation}
    |\lambda I - A| = 0,
  \end{equation}
  here $A$ is the matrix of $\mathcal{A}$ under an arbitrary basis of $V$.
\end{proposition}

\begin{proof}
  Take an arbitrary basis of the space $V$,
  and assume the coordinates of $\alpha$ under this basis is $X$,
  the matrix of $\mathcal{A}$ under this basis is $A$.
  Then the condition $\lambda \alpha = \mathcal{A}\alpha$ is equivalent to
  \begin{equation}
    \lambda X = A X \Leftrightarrow (\lambda I - A)X = 0.
  \end{equation}
  The above system of linear equations have non-zero solutions if and only if $|\lambda I
  - A| = 0$.
\end{proof}

\begin{definition}{Characteristic Polynomial}{}
  For a matrix $A$ over $\mathbb{P}$,
  the \emph{characteristic polynomial of $A$} is defined as
  \begin{equation}
    f(\lambda) = |\lambda I - A|.
  \end{equation}
\end{definition}

\begin{proposition}{}{}
  Similar matrices have the same characteristic polynomial.
\end{proposition}

\begin{proposition}{Basic Properties of Eigenvalues}{}
  If $\lambda_1,\cdots,\lambda_n$ are the eigenvalues of matrix $A$,
  then we have
  \begin{equation}
    \sum \lambda_i = \operatorname{tr}(A),\quad
    \prod \lambda_i = |A|.
  \end{equation}
\end{proposition}

\subsection{Similarity Diagonalization}

\begin{definition}{Similarity Diagonalization}{}
  Let $\mathcal{A}$ be a linear transformation in $V$.
  If there exists a basis of $V$ such that the matrix of $\mathcal{A}$
  under this basis is a diagonal matrix,
  then $\mathcal{A}$ is said to be \emph{diagonalizable}.
\end{definition}

\begin{definition}{Algebraic Multiplicity and Geometric Multiplicity}{}
  Let $f(\lambda)$ be the characteristic polynomial of $\mathcal{A}$,
  and $\lambda_i$ be an eigenvalue of $\mathcal{A}$.
  Then we call the multiplicity of $\lambda_i$ in $f(\lambda)$ its
  \emph{algebraic multiplicity},
  and the dimension of its corresponding eigenspace its \emph{geometric multiplicity}.
\end{definition}

\begin{lemma}{Relationship between Algebraic and Geometric Multiplicity}{}
  For each eigenvalue $\lambda_i$,
  its algebraic multiplicity is greater than or equal to its geometric multiplicity.
\end{lemma}

\begin{proposition}{Linear Independence of Eigenvectors Corresponding to
    Different Eigenvalues}{}
  Eigenvectors belonging to different eigenvalues are linearly independent.
\end{proposition}

\begin{proof}
  Let $\lambda_1,\cdots,\lambda_k$ be $k$ different eigenvalues of
  $\mathcal{A}$,
  with corresponding eigenvectors $\xi_1,\cdots,\xi_k$.
  By mathematical induction, assume that $\xi_1,\cdots,\xi_{k-1}$ are linearly
  independent,
  and suppose
  \begin{equation}
    c_1\xi_1 + \cdots + c_k\xi_k = 0.
  \end{equation}
  Apply $\mathcal{A}$ to both sides, we get $c_1\lambda_1\xi_1 + \cdots +
  c_k\lambda_k\xi_k = 0$.
  Substract $\lambda_k$ times the above equation from this new equation yields
  \begin{equation}
    c_1(\lambda_1 - \lambda_k)\xi_1 + \cdots + c_{k-1}(\lambda_{k-1} - \lambda_k)\xi_{k-1} = 0.
  \end{equation}
  Since $\lambda_i$ are pairwise distinct,
  we have $c_1 = \cdots = c_{k-1} = 0$,
  and $c_k = 0$.
\end{proof}

\begin{proposition}{Sufficient Condition for Diagonalization}{}
  If $\mathcal{A}$ has $n$ different eigenvalues,
  then $\mathcal{A}$ is diagonalizable.
\end{proposition}

\begin{proposition}{Necessary and Sufficient Conditions for Diagonalization}{}
  Suppose $V$ is a $n$-dimension linear space,
  then the following are equivalent conditions for $\mathcal{A}$ to be diagonalizable:
  \begin{itemize}
  \item $\mathcal{A}$ has $n$ different eigenvectors.
  \item $V = \oplus_i V_i$, where $V_i$ is the eigenspace of $\lambda_i$.
  \item The geometric multiplicity of each eigenvalue equals its algebraic multiplicity.
  \item The minimal polynomial of $\mathcal{A}$ is a product of first-degree polynomial.
  \end{itemize}
\end{proposition}

\section{Minimal Polynomials}

\subsection{Annihilating Polynomials and Minimal Polynomials}

\begin{definition}{Annihilating Polynomial}{}
  Let $A$ be a $n\times n$ square matrix over the number field $\mathbb{P}$,
  and $g(x)$ be a polynomial over $\mathbb{P}$.
  If $g(A) = 0$, then $g(x)$ is called an \emph{annihilating polynomial of $A$}.
\end{definition}

\begin{definition}{Minimal Polynomial}{}
  The monic polynomial of the lowest degree among all annihilating polynomials
  of $A$ is called the \emph{minimal polynomial of $A$}.
\end{definition}

\begin{proposition}{Uniqueness of Minimal Polynomial}{}
  The minimal polynomial of $A$ is unique.
\end{proposition}

\begin{proof}
  Suppose there are two different minimal polynomials $m_1(\lambda),
  m_2(\lambda)$.
  Define $h(\lambda) = m_1(\lambda) - m_2(\lambda)$,
  then we have
  \begin{equation}
    h(A) = m_1(A) - m_2(A) = 0,
  \end{equation}
  if $m_1, m_2$ are different polynomials, that means $h(\lambda)$ is the
  minimal polynomial of $A$, which contradicts the given condition.
\end{proof}

\begin{proposition}{Divisibility of Minimal Polynomial}{}
  The minimal polynomial of $A$ divides any annihilating polynomial of $A$.
\end{proposition}

\begin{proof}
  For any annihilating polynomial $f(x)$, it can be expressed as
  \begin{equation}
    f(x) = q(x) m(x) + r(x).
  \end{equation}
  Substituting $x = A$ gives $r(A) = 0$.
  If $r(x)$ is not the zero polynomial, that means $r(x)$ is the minimal
  polynomial of $A$, which contradicts the condition.
\end{proof}

\begin{proposition}{Minimal Polynomial of Block-Diagonal Matrix}{}
  If matrix $A = \operatorname{diag}(A_1,\cdots,A_s)$,
  then the minimal polynomial of $A$ is the least common multiple
  of the minimal polynomials of $A_1,\cdots,A_s$, i.e.,
  \begin{equation}
    m_A(\lambda) = [m_1(\lambda),\cdots,m_s(\lambda)],
  \end{equation}
  where $m_i(\lambda)$ is the minimal polynomial of $A_i$.
\end{proposition}

\begin{proposition}{Minimal Polynomials of Similar Matrices}{}
  Similar matrices have the same minimal polynomial.
\end{proposition}

\begin{proof}
  Hint: Suppose $B = P^{-1}AP$,
  then $m(B) = m(P^{-1}AP) = P^{-1}m(A)P$.
\end{proof}

\subsection{Hamilton-Cayley Theorem}

\begin{theorem}{Hamilton-Cayley Theorem}{}
  Let $A$ be a $n$-order square matrix over $\mathbb{P}$,
  and $f(x) = |\lambda I - A|$ be the characteristic polynomial of $A$.
  Then $f(x)$ is an annihilating polynomial of $A$.
\end{theorem}

\begin{corollary}{}{}
  The minimal polynomial and the characteristic polynomial of $A$ have the same roots.
\end{corollary}





