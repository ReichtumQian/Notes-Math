
\section{Rank of Matrices}

\subsection{The Concept and Properties of Matrix Rank}

\begin{definition}{Rank of a Matrix}{}
  The \emph{rank} of a matrix is equal to:
  (1) The rank of row-vector group of the matrix;
  (2) The order of the largest non-zero submatrix of the matrix.
\end{definition}

\begin{proposition}{Elementary Transformation Standard Form}{}
  Let $A$ be an $s \times n$ matrix with rank $r$.
  Then there exist an $s$-order invertible matrix $P$
  and an $n$-order invertible matrix $Q$ such that
  \begin{equation}
    A = P \left[
      \begin{array}{cc}
        I_r & O\\
         O  & O
      \end{array}
    \right] Q.
  \end{equation}
\end{proposition}

\begin{corollary}{Standard Forms for Column-full-rank and Row-full-rank Matrices}{}
  For an $m \times n$ matrix $A$,
  $A$ is column-full-rank if and only if there exists an $m$-order invertible
  matrix $P$ such that
  \begin{equation}
    A = P \left[
      \begin{array}{c}
        I_n\\
        O
      \end{array}
    \right].
  \end{equation}
  Similarly, $A$ is row-full-rank if and only if there exists an $n$-order invertible
  matrix $Q$ such that
  \begin{equation}
    A = \left[
      \begin{array}{cc}
        I_m&O
      \end{array}
    \right]Q.
  \end{equation}
\end{corollary}

\begin{lemma}{Matrix Decomposition}{}
  An $s \times n$ matrix $E = \mathrm{diag}(I_r, O)$ can be decomposed as:
  \begin{equation}
      \begin{bmatrix}I_r&O\\O&O\end{bmatrix}=\begin{bmatrix}I_r\\O\end{bmatrix}\begin{bmatrix}I_r&O\end{bmatrix}.
  \end{equation}
  Furthermore, when it is a square matrix, it can be decomposed as:
  \begin{equation}
      \begin{bmatrix}I_r&O\\O&O\end{bmatrix}=\begin{bmatrix}I_r&O\\O&O\end{bmatrix}\begin{bmatrix}I_r&O\\O&O\end{bmatrix}.
  \end{equation}
\end{lemma}

\begin{example}{Matrix Decomposition}{}
  (1) Prove that any non-zero matrix can be decomposed into the product of
  a column-full-rank matrix and a row-full-rank matrix.

  (2) Prove that any square matrix can be decomposed into the product of
  an invertible matrix and an idempotent matrix.
\end{example}

\begin{proof}
  (1) Let $A$ be a non-zero matrix with a rank of $r$, then we have
  \begin{equation}
    A=P\begin{bmatrix}I_r&O\\O&O\end{bmatrix}Q=P\begin{bmatrix}I_r\\O\end{bmatrix}\begin{bmatrix}I_r&O\end{bmatrix}Q
    = A_1 A_2,
  \end{equation}
  where $I_r$ is the identity matrix of order $r$,
  and $A_1 = P [I_r, O]^T, A_2 = [I_r, O]Q$.

  (2) Let $A$ be a $n \times n$ matrix with a rank of $r$,
  then we can get
  \begin{equation}
    A=P\begin{pmatrix}I_r&O\\O&O\end{pmatrix}Q=PQQ^{-1}\begin{pmatrix}I_r&O\\O&O\end{pmatrix}Q,
  \end{equation}
  where $PQ$ is an invertible matrix, and the remaining is a idempotent matrix.
\end{proof}

\subsection{Rank Inequalities}

\begin{lemma}{Invertible Matrices Do Not Change Rank}{}
  Let $A$ be an arbitrary matrix,
  and let $P, Q$ be invertible matrices,
  then
  \begin{equation}
    r(A) = r(PAQ).
  \end{equation}
\end{lemma}

\begin{example}{}{}
  Let $A$ and $B$ be $m \times n$ and $n \times s$ matrices respectively,
  prove (1) if $r(A) = n$, then $r(AB) = r(B)$;
  (2) if $r(B) = n$, then $r(AB) = r(A)$.
\end{example}

\begin{proof}
  (1) Since $r(A) = n$, there exists an invertible matrix $P$ such that $PA =
  [A_1, O]^T$, where $A_1$ is an invertible matrix.
  Then
  \begin{equation}
    r(AB) = r \left(
      \begin{bmatrix}
        A_1\\
        O
      \end{bmatrix}B
    \right)
    = r\left(
      \begin{bmatrix}
        A_1 B\\
        O
      \end{bmatrix}
    \right) = r(A_1B) = r(B).
  \end{equation}
\end{proof}

\begin{lemma}{Rank of Block Matrices}{}
  Let $A$ be a matrix with a rank of $r_1$,
  and let $B$ be a matrix with a rank of $r_2$,
  $C$ be an arbitrary matrix,
  then we have
  \begin{equation}
   r\begin{pmatrix}A&O\\O&B\end{pmatrix}=r(A)+r(B), \quad
   r\begin{pmatrix}A&O\\C&B\end{pmatrix}\geq r(A)+r(B), \quad
  \end{equation}
\end{lemma}

\begin{proof}
  (1) Hint:
  \begin{equation}
    \begin{pmatrix}P_1&O\\O&P_2\end{pmatrix}
    \begin{pmatrix}A&O\\O&B\end{pmatrix}
    \begin{pmatrix}Q_1&O\\O&Q_2\end{pmatrix}
    =\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\O&O&I_{r_2}&O\\O&O&O&O\end{pmatrix}
  \end{equation}

  (2)Hint: 
  \begin{equation}
    \begin{pmatrix}P_1&O\\O&P_2\end{pmatrix}
    \begin{pmatrix}A&O\\C&B\end{pmatrix}
    \begin{pmatrix}Q_1&O\\O&Q_2\end{pmatrix}
    =\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\C_{11}&C_{12}&I_{r_2}&O\\C_{21}&C_{22}&O&O\end{pmatrix}\to\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\O&O&I_{r_2}&O\\O&C_{22}&O&O\end{pmatrix}
  \end{equation}
\end{proof}

\begin{theorem}{Inequalities of Matrix Rank}{}
  Let $A$ and $B$ be arbitrary matrices, we have
  \begin{enumerate}
  \item $r(AB) \leq \min\{r(A), r(B)\}$;
  \item $r(A + B) \leq r(A) + r(B)$;
  \item Sylvester's inequality: $A, B$ are $n$-order square matrices, then $r(A)
    + r(B) \leq r(AB) + n$;
  \item Frobenius inequality: $A, B, C$ are $n$-order square matrices,
    then $r(ABC) \geq r(AB) + r(BC) - r(B)$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  (1) Hint: Write $B$ in row-vector form, and by the multiplication between
  matrices we know that each row-vector of $AB$ can be linearly represented by
  the row-vector of $B$, thus $r(AB) \leq r(B)$.
  Similarly, $r(AB) \leq r(A)$.

  (2) Hint: From the column-vector perspective,
  the column-vectors of $A + B$ can be linearly represented by
  the column-vectors of $A$ and $B$, so the conclusion holds.

  (3)(4) Hint: Prove that $r(ABC) + r(B) \geq r(AB) + r(BC)$,
  \begin{equation}
    \begin{bmatrix}ABC&\\&B\end{bmatrix}\to\begin{bmatrix}ABC&O\\BC&B\end{bmatrix}\to\begin{bmatrix}O&-AB\\BC&B\end{bmatrix},
  \end{equation}
  and according the rank inequality of block matrices, the conclusion holds.
\end{proof}


\section{Inverse and Adjugate Matrices}

\subsection{Inverse of Matrices}

\begin{definition}{Inverse of a Matrix}{}
  If two matrices $A, B$ satisfy $AB = BA = I$,
  then $B$ is called the \emph{inverse} of $A$, denoted as $A^{-1}$.
\end{definition}

\begin{proposition}{Basic Properties of Matrix Inverses}{}
  Consider two matrices $A, B$ and a real number $k$, then
  \begin{enumerate}
  \item $(kA)^{-1} = k^{-1}A^{-1}$;
  \item $(AB)^{-1} = B^{-1} A^{-1}$;
  \item $(A^T)^{-1} = (A^{-1})^T$;
  \item $|A^{-1}| = |A|^{-1}$.
  \end{enumerate}
\end{proposition}

\begin{proof}
  (1) is obvious.
  (2) by $(AB)^{-1}AB = I$ then $(AB)^{-1} = B^{-1}A^{-1}$.
\end{proof}

\begin{proposition}{Necessary and Sufficient Condition for Invertibility}{}
  A square matrix $A$ is invertible if and only if:
  (1) $|A| \neq 0$;
  (2) or $A$ can be expressed as a product of elementary matrices.
\end{proposition}

\begin{proposition}{Finding the Inverse Using Elementary Row Operations}{}
  If a matrix $A$ is invertible, then we can apply a sequence of
  elementary row operations such that
  \begin{equation}
    (A, I) \rightarrow (I, A^{-1}).
  \end{equation}
  Similarly, for any matrix $B$ with compatible dimensions,
  we can apply elementary row operations such that $(A, B)
  \rightarrow (I, A^{-1}B)$.
\end{proposition}

\subsection{Adjugate Matrix}


\begin{definition}{Adjugate Matrix}{}
  Let $A = (a_{ij})$ be an $n \times n$ matrix, and let $A_{ij}$ denote
  the cofactor of the entry $a_{ij}$.
  The \emph{adjugate matrix} of $A$, denoted as $A^{\ast}$,
  is defined as the transpose of the cofactor matrix of $A$, i.e.,
  \begin{equation}
    A^*=\begin{bmatrix}A_{11}&A_{21}&\cdots&A_{n1}\\A_{12}&A_{22}&\cdots&A_{n2}\\\vdots&\vdots&\ddots&\vdots\\A_{1n}&A_{2n}&\cdots&A_{nn}\end{bmatrix}.
  \end{equation}
\end{definition}

\begin{proposition}{Finding the Inverse Using the Adjugate Matrix}{}
  Let $A$ be an $n \times n$ matrix, and let $A^{\ast}$ denote its adjugate matrix.
  Then
  \begin{equation}
    AA^{\ast} = |A| I.
  \end{equation}
\end{proposition}

\begin{proof}
  Define $B = AA^{\ast}$.
  The $(i,j)$-entry of $B$, denoted as $b_{ij}$, is given by 
  \begin{equation}
    b_{ij} = \sum\limits_{k = 1}^n a_{ik} A_{jk} =
    \begin{cases}
      |A|, & i = j;\\
      0, & i \neq j.
    \end{cases}
  \end{equation}
  This result follows directly from the expansion of the determinant along the
  $j$-th column.
  Therefore, $AA^{\ast} = |A|I$.
\end{proof}

\section{Several Special Matrices}




