
\section{Rank of Matrices}

\subsection{The Concept and Properties of Matrix Rank}

\begin{definition}{Rank of a Matrix}{}
  The \emph{rank} of a matrix is equal to:
  (1) The rank of row-vector group of the matrix;
  (2) The order of the largest non-zero submatrix of the matrix.
\end{definition}

\begin{proposition}{Elementary Transformation Standard Form}{}
  Let $A$ be an $s \times n$ matrix with rank $r$.
  Then there exist an $s$-order invertible matrix $P$
  and an $n$-order invertible matrix $Q$ such that
  \begin{equation}
    A = P \left[
      \begin{array}{cc}
        I_r & O\\
         O  & O
      \end{array}
    \right] Q.
  \end{equation}
\end{proposition}

\begin{proof}
  Hint: Use the elementary transformations to transform $A$ into an identity matrix.
\end{proof}

\begin{corollary}{Standard Forms for Column-full-rank and Row-full-rank Matrices}{}
  For an $m \times n$ matrix $A$,
  $A$ is column-full-rank if and only if there exists an $m$-order invertible
  matrix $P$ such that
  \begin{equation}
    A = P \left[
      \begin{array}{c}
        I_n\\
        O
      \end{array}
    \right].
  \end{equation}
  Similarly, $A$ is row-full-rank if and only if there exists an $n$-order invertible
  matrix $Q$ such that
  \begin{equation}
    A = \left[
      \begin{array}{cc}
        I_m&O
      \end{array}
    \right]Q.
  \end{equation}
\end{corollary}

\begin{lemma}{Matrix Decomposition}{}
  An $s \times n$ matrix $E = \mathrm{diag}(I_r, O)$ can be decomposed as:
  \begin{equation}
      \begin{bmatrix}I_r&O\\O&O\end{bmatrix}=\begin{bmatrix}I_r\\O\end{bmatrix}\begin{bmatrix}I_r&O\end{bmatrix}.
  \end{equation}
  Furthermore, when it is a square matrix, it can be decomposed as:
  \begin{equation}
      \begin{bmatrix}I_r&O\\O&O\end{bmatrix}=\begin{bmatrix}I_r&O\\O&O\end{bmatrix}\begin{bmatrix}I_r&O\\O&O\end{bmatrix}.
  \end{equation}
\end{lemma}

\begin{example}{Matrix Decomposition}{}
  (1) Prove that any non-zero matrix can be decomposed into the product of
  a column-full-rank matrix and a row-full-rank matrix.

  (2) Prove that any square matrix can be decomposed into the product of
  an invertible matrix and an idempotent matrix.
\end{example}

\begin{proof}
  (1) Let $A$ be a non-zero matrix with a rank of $r$, then we have
  \begin{equation}
    A=P\begin{bmatrix}I_r&O\\O&O\end{bmatrix}Q=P\begin{bmatrix}I_r\\O\end{bmatrix}\begin{bmatrix}I_r&O\end{bmatrix}Q
    = A_1 A_2,
  \end{equation}
  where $I_r$ is the identity matrix of order $r$,
  and $A_1 = P [I_r, O]^T, A_2 = [I_r, O]Q$.

  (2) Let $A$ be a $n \times n$ matrix with a rank of $r$,
  then we can get
  \begin{equation}
    A=P\begin{pmatrix}I_r&O\\O&O\end{pmatrix}Q=PQQ^{-1}\begin{pmatrix}I_r&O\\O&O\end{pmatrix}Q,
  \end{equation}
  where $PQ$ is an invertible matrix, and the remaining is a idempotent matrix.
\end{proof}

\subsection{Rank Inequalities}

\begin{lemma}{Invertible Matrices Do Not Change Rank}{}
  Let $A$ be an arbitrary matrix,
  and let $P, Q$ be invertible matrices,
  then
  \begin{equation}
    r(A) = r(PAQ).
  \end{equation}
\end{lemma}

\begin{example}{}{}
  Let $A$ and $B$ be $m \times n (m > n)$ and $n \times s$ matrices respectively,
  prove (1) if $r(A) = n$, then $r(AB) = r(B)$;
  (2) if $r(B) = n$, then $r(AB) = r(A)$.
\end{example}

\begin{proof}
  (1) Since $r(A) = n$, there exists an invertible matrix $P$ such that $PA =
  [A_1, O]^T$, where $A_1$ is an invertible matrix.
  Then
  \begin{equation}
    r(AB) = r \left(
      \begin{bmatrix}
        A_1\\
        O
      \end{bmatrix}B
    \right)
    = r\left(
      \begin{bmatrix}
        A_1 B\\
        O
      \end{bmatrix}
    \right) = r(A_1B) = r(B).
  \end{equation}

  (2) Similarly, there exists an invertible matrix $P$ such that $BP = [B_1, O]$,
  then
  \begin{equation}
    r(AB) = r(A [B_1, O]) = r([AB_1, O]) = r(AB_1) = r(A),
  \end{equation}
  thus the conclusion holds.
\end{proof}

\begin{lemma}{Rank of Block Matrices}{}
  Let $A$ be a matrix with a rank of $r_1$,
  and let $B$ be a matrix with a rank of $r_2$,
  $C$ be an arbitrary matrix,
  then we have
  \begin{equation}
   r\begin{pmatrix}A&O\\O&B\end{pmatrix}=r(A)+r(B), \quad
   r\begin{pmatrix}A&O\\C&B\end{pmatrix}\geq r(A)+r(B), \quad
  \end{equation}
\end{lemma}

\begin{proof}
  (1) Hint:
  \begin{equation}
    \begin{pmatrix}P_1&O\\O&P_2\end{pmatrix}
    \begin{pmatrix}A&O\\O&B\end{pmatrix}
    \begin{pmatrix}Q_1&O\\O&Q_2\end{pmatrix}
    =\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\O&O&I_{r_2}&O\\O&O&O&O\end{pmatrix}
  \end{equation}

  (2)Hint: 
  \begin{equation}
    \begin{pmatrix}P_1&O\\O&P_2\end{pmatrix}
    \begin{pmatrix}A&O\\C&B\end{pmatrix}
    \begin{pmatrix}Q_1&O\\O&Q_2\end{pmatrix}
    =\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\C_{11}&C_{12}&I_{r_2}&O\\C_{21}&C_{22}&O&O\end{pmatrix}\to\begin{pmatrix}I_{r_1}&O&O&O\\O&O&O&O\\O&O&I_{r_2}&O\\O&C_{22}&O&O\end{pmatrix}
  \end{equation}
\end{proof}

\begin{theorem}{Inequalities of Matrix Rank}{}
  Let $A$ and $B$ be arbitrary matrices, we have
  \begin{enumerate}
  \item $r(AB) \leq \min\{r(A), r(B)\}$;
  \item $r(A + B) \leq r(A) + r(B)$;
  \item Sylvester's inequality: $A, B$ are $n$-order square matrices, then $r(A)
    + r(B) \leq r(AB) + n$;
  \item Frobenius inequality: $A, B, C$ are $n$-order square matrices,
    then $r(ABC) \geq r(AB) + r(BC) - r(B)$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  (1) Hint: Write $B$ in row-vector form, and by the multiplication between
  matrices we know that each row-vector of $AB$ can be linearly represented by
  the row-vector of $B$, thus $r(AB) \leq r(B)$.
  Similarly, $r(AB) \leq r(A)$.

  (2) Hint: From the column-vector perspective,
  the column-vectors of $A + B$ can be linearly represented by
  the column-vectors of $A$ and $B$, so the conclusion holds.

  (3)(4) Hint: Prove that $r(ABC) + r(B) \geq r(AB) + r(BC)$,
  \begin{equation}
    \begin{bmatrix}ABC&\\&B\end{bmatrix}\to\begin{bmatrix}ABC&O\\BC&B\end{bmatrix}\to\begin{bmatrix}O&-AB\\BC&B\end{bmatrix},
  \end{equation}
  and according the rank inequality of block matrices, the conclusion holds.
\end{proof}

\subsection{Rank of Polynomial Matrices}

\begin{theorem}{Rank of Polynomial Matrices}{}
  Let $f(x), f_1(x), f_2(x)$ be three polynomials, satisfying
    $f(x) = f_1(x) f_2(x)$ and $(f_1(x), f_2(x)) = 1$.
    Then for any matrix $A$, $f(A) = O$ if and only if
    \begin{equation}
    r(f_1(A)) + r(f_2(A)) = n.
    \end{equation}
\end{theorem}

\begin{proof}
  By the equivalent conditions for relatively prime, we know there exists $u(x),
  v(x)$ such that $u(x)f_1(x) + v(x)f_2(x) = 1$,
  which means $u(A)f_1(A) + v(A)f_2(A) = I$.
  By elementary transformations
  \begin{align}
    \left[
      \begin{array}{cc}
        f_1(A)&\\
        &f_2(A)
      \end{array}
    \right] &\rightarrow \left[
      \begin{array}{cc}
        f_1(A)&f_1(A)u(A) + v(A)f_2(A)\\
        O&f_2(A)
      \end{array}
    \right] = \left[
      \begin{array}{cc}
        f_1(A)&I\\
        O&f_2(A)
      \end{array}
    \right]\\
    &\rightarrow \left[
      \begin{array}{cc}
        O&I\\
        -f_2(A)f_1(A)& f_2(A)
      \end{array}
    \right] \rightarrow \left[
      \begin{array}{cc}
        O&I\\
        -f(A)&O
      \end{array}
    \right].
  \end{align}
  This implies that $r(f_1(A)) + r(f_2(A)) = r(f(A)) + n$.
\end{proof}

\begin{example}{Applications of Polynomial Matrices}{}
  Prove the following propositions:
  \begin{enumerate}
  \item A matrix $A$ is idempotent if and only if $r(A - I) + r(A) = n$;
  \item A matrix $A$ satisfies $A^2 = I$ if and only if $r(A - I) + r(A + I) = n$.
  \end{enumerate}
\end{example}


\section{Inverse and Adjugate Matrices}

\subsection{Inverse of Matrices}

\begin{definition}{Inverse of a Matrix}{}
  If two matrices $A, B$ satisfy $AB = BA = I$,
  then $B$ is called the \emph{inverse} of $A$, denoted as $A^{-1}$.
\end{definition}

\begin{proposition}{Basic Properties of Matrix Inverses}{}
  Consider two matrices $A, B$ and a real number $k$, then
  \begin{enumerate}
  \item $(kA)^{-1} = k^{-1}A^{-1}$;
  \item $(AB)^{-1} = B^{-1} A^{-1}$;
  \item $(A^T)^{-1} = (A^{-1})^T$;
  \item $|A^{-1}| = |A|^{-1}$.
  \end{enumerate}
\end{proposition}

\begin{proof}
  (1) is obvious.
  (2) by $(AB)^{-1}AB = I$ then $(AB)^{-1} = B^{-1}A^{-1}$.
  (3) Suppose $A^TB = I$, then $B^TA = I$ implies $B = (A^{-1})^T$.
\end{proof}

\begin{proposition}{Necessary and Sufficient Condition for Invertibility}{}
  A square matrix $A$ is invertible if and only if:
  (1) $|A| \neq 0$;
  (2) or $A$ can be expressed as a product of elementary matrices.
\end{proposition}

\begin{proposition}{Finding the Inverse Using Elementary Row Operations}{}
  If a matrix $A$ is invertible, then we can apply a sequence of
  elementary row operations such that
  \begin{equation}
    (A, I) \rightarrow (I, A^{-1}).
  \end{equation}
  Similarly, for any matrix $B$ with compatible dimensions,
  we can apply elementary row operations such that $(A, B)
  \rightarrow (I, A^{-1}B)$.
\end{proposition}

\subsection{Diagonal Dominant Matrix}

\begin{proposition}{Invertibility of Dominant Matrices}{}
  Let $A$ be a $n$-order real matrix, if $|a_{ii}| > \sum_{j \neq i} |a_{ij}|$,
  then $|A| \neq 0$.
\end{proposition}

\begin{proof}
  Assume $|A| = 0$, then $Ax = 0$ has a non-trivial solution.
  Let $x = (x_1,\cdots,x_n)^T$ be a non-zero solution of $Ax = 0$,
  and $|x_k|$ be the largest among all the elements.
  Since $-a_{kk}x_k = \sum _{j \neq k}a_{kj}x_j$, then
  \begin{equation}
    |a_{kk}| \cdot |x_k| \leq \sum_{j \neq k} |a_{kj}| \cdot |x_j|
    \leq \sum _{j \neq k} |a_{kj}| \cdot |x_k|,
  \end{equation}
  which contradicts the condition.
\end{proof}

\begin{corollary}{Enlarge Diagonal Elements to Make a Matrix Invertible}{}
  For any non-invertible matrix $A$, there exists a real number $M$ such that
  \begin{equation}
    B = tI + A, \quad t > M,
  \end{equation}
  are invertible.
\end{corollary}

\subsection{Adjugate Matrix}


\begin{definition}{Adjugate Matrix}{}
  Let $A = (a_{ij})$ be an $n \times n$ matrix, and let $A_{ij}$ denote
  the cofactor of the entry $a_{ij}$.
  The \emph{adjugate matrix} of $A$, denoted as $A^{\ast}$,
  is defined as the transpose of the cofactor matrix of $A$, i.e.,
  \begin{equation}
    A^*=\begin{bmatrix}A_{11}&A_{21}&\cdots&A_{n1}\\A_{12}&A_{22}&\cdots&A_{n2}\\\vdots&\vdots&\ddots&\vdots\\A_{1n}&A_{2n}&\cdots&A_{nn}\end{bmatrix}.
  \end{equation}
\end{definition}

\begin{proposition}{Finding the Inverse Using the Adjugate Matrix}{}
  Let $A$ be an $n \times n$ matrix, and let $A^{\ast}$ denote its adjugate matrix.
  Then
  \begin{equation}
    AA^{\ast} = |A| I.
  \end{equation}
\end{proposition}

\begin{proof}
  Define $B = AA^{\ast}$.
  The $(i,j)$-entry of $B$, denoted as $b_{ij}$, is given by 
  \begin{equation}
    b_{ij} = \sum\limits_{k = 1}^n a_{ik} A_{jk} =
    \begin{cases}
      |A|, & i = j;\\
      0, & i \neq j.
    \end{cases}
  \end{equation}
  This result follows directly from the expansion of the determinant along the
  $j$-th column.
  Therefore, $AA^{\ast} = |A|I$.
\end{proof}

\begin{example}{Properties of Adjugate Matrix}{}
  Find
  (1) the determinant of $A^{\ast}$;
  (2) the rank of $A^{\ast}$;
  (3) the eigenvalue of $A^{\ast}$;
  (4) $(AB)^{\ast} = B^{\ast}A^{\ast}$.
\end{example}

\begin{solution}
  (1) $|A^{\ast}| = \left| |A| A^{-1} \right| = |A|^n \cdot |A^{-1}| = |A|^{n-1}$.

  (2) When $r(A) = n$, $|A^{\ast}| \neq 0$ then $r(A^{\ast}) = n$.
  When $r(A) = n-1$, $AA^{\ast} = |A|I = O$, using the rank inequality
  \begin{equation}
    r(AA^{\ast}) \geq r(A) + r(A^{\ast}) - n = r(A^{\ast}) - 1,
  \end{equation}
  this means $r(A^{\ast}) \leq 1$.
  But since $r(A) = n-1$, there exists a non-zero cofactor of $A$,
  so $r(A^{\ast}) = 1$.
  When $r(A) < n-1$, we know $r(A^{\ast}) = 0$ by definition.

  (3) Let $A = P^{-1}JP$ where $J$ is the Jordan canonical form of $A$.
  Then $J^{\ast} = |J|J^{-1}$, which means
  \begin{equation}
    \lambda_j^{\ast} = \prod \limits_{i \neq j} \lambda_i,
  \end{equation}
  where $\lambda_1,\cdots,\lambda_n$ are the eigenvalues of $A$.

  (4) If both $A$ and $B$ are invertible, then
  \begin{equation}
    (AB)^{\ast} = |AB|(AB)^{-1} = |B|B^{-1}\cdot |A|A^{-1} = B^{\ast}A^{\ast}.
  \end{equation}
  Otherwise, there exists a real number $M$, and for all $t > M$ such that
  $tI + A$ and $t I + B$ are invertible. Then
  \begin{equation}
    [(tI + A)(tI + B)]^{\ast} = (tI + B)^{\ast}(tI + A)^{\ast},
  \end{equation}
  each entry of the above two matrices are polynomials with respect to $t$.
  Since the equation holds for all $t > M$, then the left-hand-side and
  right-hand-side matrices are identical.
  So the equation also holds when $t = 0$, which deduces the conclusion.
\end{solution}

\section{Several Special Matrices}

\subsection{Real Symmetric Matrices}

\begin{definition}{Real Symmetric Matrices}{}
  Let a $A$ satisfies every entry is real number and $A = A^T$,
  then $A$ is said to be a \emph{real symmetric matrix}.
\end{definition}

\begin{proposition}{Eigenvalues and Eigenvectors of Real Symmetric Matrices}{}
  Let $A$ be a real symmetric matrix, then
  (1) all the eigenvalues of $A$ are real;
  (2) eigenvectors belonging to different eigenvalue are orthogonal.
\end{proposition}

\begin{proof}
  (1) Let $\lambda \in \mathbb{C}$ be an eigenvalue of $A$,
  and $\xi \in \mathbb{C}^n$ be the corresponding eigenvector,
  then we have
  \begin{equation}
    A\xi = \lambda \xi \Rightarrow \overline{\xi}^T A\xi = \lambda \cdot \overline{\xi}^T\xi.
  \end{equation}
  Similarly by taking conjugate transpose
  \begin{equation}
    \overline{\xi}^T A = \overline{\xi}^T \overline{\lambda} \Rightarrow
    \overline{\xi}^T A \xi = \overline{\lambda}  \cdot \overline{\xi}^T \xi.
  \end{equation}
  Compare the two equations, we know that $\lambda = \overline{\lambda}$.

  (2) Let $\lambda_1, \lambda_2$ be two different eigenvalues of $A$,
  and $\xi_1, \xi_2$ belong to them respectively.
  Then
  \begin{equation}
    \lambda_1 \xi_1^T \xi_2 = (A\xi_1)^T\xi_2 = \xi_1^T (A\xi_2) = \lambda_2 \xi_1^T\xi_2.
  \end{equation}
  Since $\lambda_1 \neq \lambda_2$, the equation implies $\xi_1^T\xi_2 = 0$.
\end{proof}

\begin{proposition}{Standard Form of Real Symmetric Matrices}{}
  Any real symmetric matrix is orthogonally similar to a diagonal matrix.
\end{proposition}

\begin{proof}
  We prove by induction.
  When $n = 1$, the conclusion holds.
  Assume the conclusion holds for $n - 1$,
  then for all eigenvalue $\lambda$, and an eigenvector $\alpha_1$ belonging to
  $\lambda$, expand $\alpha_1$ to an orthonormal basis
  $\alpha_1,\cdots,\alpha_n$.
  Let $T = (\alpha_1,\cdots,\alpha_n)$, then
  \begin{equation}
    AT = T \left(
      \begin{array}{cc}
        \lambda&\beta^T\\
        0&A_{n-1}
      \end{array}
    \right) \Rightarrow T^T AT = \left(
      \begin{array}{cc}
        \lambda&\beta^T \\
               0&A_{n-1}
      \end{array}
    \right).
  \end{equation}
  Since $T^TAT$ is symmetric, we know that $\beta^T = \mathbf{0}$.
  By assumption, there exists another orthogonal matrix $P_1$ such that
  $P_1^TA_{n-1}P$ is diagonal. Let $P = \operatorname{diag}\{1, P_1\}$,
  and $Q = TP$, we get $Q^TAQ$ is a diagonal matrix, and $Q$ is an orthogonal matrix.
\end{proof}

\subsection{Nilpotent Matrices}

\begin{definition}{Nilpotent Matrix}{}
  Let $A = (a_{ij})$ be an $n$-order square matrix, if
  there exists an integer $k$ such that
  \begin{equation}
    A^k = O,
  \end{equation}
  then it is said to be a \emph{nilpotent matrix}.
\end{definition}

\begin{proposition}{Eigenvalue of Nilpotent Matrix}{}
  A square matrix $A$ is nilpotent if and only if all the eigenvalues of $A$ are
  zero.
\end{proposition}

\begin{proof}
  Hint: Consider the Jordan canonical form of $A$.
\end{proof}

\begin{corollary}{Characteristic Polynomial of Nilpotent Matrix}{}
  The characteristic polynomial of a nilpotent matrix is $f(x) = x^n$.
\end{corollary}

\begin{proof}
  Hint: Consider the Jordan canonical form.
\end{proof}

\subsection{Idempotent Matrices}

\begin{definition}{Idempotent Matrix}{}
  Let $A$ be a $n$-order square matrix, if it satisfies
  \begin{equation}
    A^2 = A,
  \end{equation}
  then it is said to be an \emph{idempotent matrix}.
\end{definition}

\begin{proposition}{Eigenvalues of Idempotent Matrix}{}
  The eigenvalues of an idempotent matrix are either $0$ or $1$.
\end{proposition}

\begin{proof}
  Let $\lambda$ be an eigenvalue of $A$, then
  \begin{equation}
    \lambda \alpha = A \alpha = A^2\alpha = \lambda^2\alpha,
  \end{equation}
  which implies that $\lambda = \lambda^2$.
  Then $\lambda$ is either $0$ or $1$.
\end{proof}

\begin{example}{Idempotent Matrices Decomposition}{}
  Prove that any square matrix can be decomposed into the product
  of an invertible matrix and an idempotent matrix.
\end{example}

\begin{proof}
  Using the standard form of a matrix
  \begin{equation}
    A = P \left(
      \begin{array}{cc}
        I_r&O\\
        O&O
      \end{array}
    \right)Q = PQ Q^{-1} \left(
      \begin{array}{cc}
        I_r&O\\
        O&O
      \end{array}
    \right) Q = A_1A_2,
  \end{equation}
  where $A_1 = PQ$, and $A_2$ is an idempotent matrix.
\end{proof}





