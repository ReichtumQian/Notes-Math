
\section{Euclidean Space}

\subsection{Concept of Euclidean Space}

\begin{definition}{Euclidean Space}{}
  Let $V$ be a linear space over $\mathbb{R}$.
  If a bilinear function $\langle \alpha, \beta \rangle$ in $V$ satisfies
  \begin{enumerate}
  \item Linearity: $\langle k_1\alpha_1 + k_2\alpha_2, \beta \rangle = k_1
    \langle \alpha_1, \beta\rangle + k_2 \langle \alpha_2, \beta \rangle$;
  \item Symmetry: $\langle \alpha, \beta \rangle = \langle \beta, \alpha \rangle$;
  \item Positive-definiteness: $\langle \alpha, \alpha \rangle \geq 0$;
    and $\langle \alpha, \alpha \rangle = 0$ if and only
    if $\alpha = 0$,
  \end{enumerate}
  then it is said to be an \emph{inner product in $V$}.
\end{definition}

\begin{example}{}{}
  Let $C_{[a,b]}$ denote the set containing all the continuous functions on $[a,
  b]$, prove that it is an Euclidean space with the inner product 
  \begin{equation}
    \langle f(x), g(x) \rangle := \int_a^b f(x)g(x)\mathrm{d} x.
  \end{equation}
\end{example}

\begin{proof}
  Hint: Verify the three properties of inner product.
\end{proof}

\begin{definition}{Metric/Gram Matrix}
  Let $\epsilon_1,\cdots,\epsilon_n$ be a basis of an Euclidean space $V$,
  define a $n$-order square matrix
  \begin{equation}
    G = \begin{pmatrix}
      \langle\epsilon_1,\epsilon_1\rangle & \langle\epsilon_1,\epsilon_2\rangle & \cdots & \langle\epsilon_1,\epsilon_n\rangle \\
      \langle\epsilon_2,\epsilon_1\rangle & \langle\epsilon_2,\epsilon_2\rangle & \cdots & \langle\epsilon_2,\epsilon_n\rangle \\
      \vdots & \vdots & \ddots & \vdots \\
      \langle\epsilon_n,\epsilon_1\rangle & \langle\epsilon_n,\epsilon_2\rangle & \cdots & \langle\epsilon_n,\epsilon_n\rangle
    \end{pmatrix}
  \end{equation}
  as its \emph{metric matrix}.
\end{definition}

\begin{proposition}{Representation of Inner Product}{}
  If the coordinate vectors of vectors $\alpha, \beta$ with respect to the basis
  $(\epsilon_1,\cdots,\epsilon_n)$ are $X, Y$ respectively.
  Then 
  \begin{equation}
    \langle \alpha, \beta \rangle = X^TGY,
  \end{equation}
  where $G$ is the metric matrix of $(\epsilon_1,\cdots,\epsilon_n)$.
\end{proposition}

\begin{proof}
  See bilinear functions.
\end{proof}

\begin{example}{Find the Metric Matrix and Inner Product}{}
  Given a basis of Euclidean space $V$:
  \begin{equation}
    \epsilon_1=e_1+e_2,\epsilon_2=e_1+e_3,\epsilon_3=e_4-e_1,\epsilon_4=e_1-e_2-e_3+e_4,
  \end{equation}
  where $\{e_i\}$ are standard basis vectors.
  The coordinates of two vectors $\alpha, \beta$ under
  $(\epsilon_1,\cdots,\epsilon_4)$ are $(1,2,3,4)^T$ and $(2,0,1,0)^T$ respectively.
  Find (1) The metric matrix under $(\epsilon_1,\cdots,\epsilon_4)$
  (2) $\langle \alpha, \beta \rangle$.
\end{example}

\begin{solution}
  Answer:
  \begin{equation}
    G=\begin{bmatrix}2&1&-1&0\\1&2&-1&0\\-1&-1&2&0\\0&0&0&4\end{bmatrix}, \quad
    \langle \alpha,\beta \rangle=5
  \end{equation}
\end{solution}

\subsection{Orthogonality and Orthogonal Bases}

\begin{definition}{Orthogonality}{}
  Let $\alpha, \beta$ be two vectors in an Euclidean space $V$,
  if
  \begin{equation}
    \langle \alpha, \beta \rangle = 0,
  \end{equation}
  then we say that $\alpha, \beta$ are \emph{orthogonal}.
\end{definition}

\begin{proposition}{Orthogonality Implies Linearly Independent}{}
  Let $\alpha_1,\cdots,\alpha_n$ be a set of vectors in an Euclidean space $V$,
  if they are pairwise orthogonal, then they are linearly independent.
\end{proposition}

\begin{proof}
  Without loss of generality,
  assume that $k_1 \alpha_1 + \cdots + k_n\alpha_n = 0$.
  Then taking the inner product of both sides with $\alpha_i$ yields
  \begin{equation}
    k_i \langle \alpha_i, \alpha_i \rangle = 0.
  \end{equation}
  Therefore for any $i = 1,\cdots,n$, we have $k_i = 0$.
\end{proof}

\begin{definition}{Standard Orthogonal Basis}{}
  Let $\epsilon_1,\cdots,\epsilon_n$ be $n$ pairwise-orthogonal unit vectors
  in an $n$-th dimensional Euclidean space $V$.
  They are called the \emph{standard orthogonal basis of $V$}.
\end{definition}

\begin{proposition}{Transition Matrix between two Orthogonal Bases}{}
  Let $\epsilon_1,\cdots,\epsilon_n$ be an orthogonal basis of $V$,
  and let $\eta_1,\cdots,\eta_n$ be basis satisfying
  \begin{equation}
    (\eta_1,\cdots,\eta_n) = (\epsilon_1,\cdots,\epsilon_n)T,
  \end{equation}
  then $\eta_1,\cdots,\eta_n$ is an orthogonal basis if and only if $T$ is an
  orthogonal matrix.
\end{proposition}

\begin{theorem}{Schmidt Orthogonalization}{}
  Given a set of linearly independent vectors $\alpha_1,\cdots,\alpha_n$,
  then the following process yields a set of pairwise-orthogonal vectors (but
  not unit)
  \begin{align*}
    &\epsilon_1 = \alpha_1,\\
    &\epsilon_2 = \alpha_2 - \frac{\langle \alpha_2, \epsilon_1 \rangle}{\langle \epsilon_1, \epsilon_1\rangle} \epsilon_1,\\
    &~~~~~~~\vdots\\
    &\epsilon_s = \alpha_s - \sum\limits_{i = 1}^{s - 1} \frac{\langle \alpha_s, \epsilon_i \rangle}{\langle\epsilon_i, \epsilon_i \rangle}\epsilon_i.
  \end{align*}
\end{theorem}

\begin{example}{Schmidt Orthogonalization}{}
  Given $\epsilon_1 = (1, 2, -1)^T, \epsilon_2 = (-1,3,1)^T, \epsilon_3 =
  (4,-1,0)^T$,
  use Schmidt orthogonalization to transform them into a standard orthogonal basis.
\end{example}

\begin{solution}
  The final results are $\eta_{1}=\frac{1}{\sqrt{6}}(1,2,-1)^{T},\eta_{2}=\frac{1}{\sqrt{3}}(-1,1,1)^{T},\eta_{3}=\frac{1}{\sqrt{2}}(1,0,1)^{T}$.
\end{solution}

\begin{theorem}{QR Decomposition}{}
  Any real-invertible matrix $A$ can be uniquely decomposed into the product of an
  orthogonal matrix $Q$ and an upper-triangular matrix with positive main
  diagonal elements $R$:
  \begin{equation}
    A = QR.
  \end{equation}
\end{theorem}

\begin{proof}
  Denote the column vectors of $A$ as $\alpha_1,\cdots,\alpha_n$.
  Applying Schmidt orthogonalization to $\alpha_1,\cdots,\alpha_n$ yields
  $\beta_1,\cdots,\beta_n$, and normalizing then yields $\eta_1,\cdots,\eta_n$.
  Let $Q = (\eta_1,\cdots,\eta_n)$, then $A = QR$,
  where $R$ is an upper-triangular matrix with $r_{ii} = |\beta_i|$.
\end{proof}

\begin{example}{QR Decomposition}{}
  Decompose the following matrix $A$ into the product of an orthogonal matrix
  and an upper-triangular matrix
  \begin{equation}
    A =
    \begin{bmatrix}
      1 & 1 & 1\\
      1 & 0 & 1\\
      1 & 2 & 0
    \end{bmatrix}.
  \end{equation}
\end{example}

\subsection{Orthogonal Sum and Orthogonal Complement}

\begin{definition}{Orthogonal Sum}{}
  Let $V_1, \cdots, V_s$ be a sequence of subspaces of an Euclidean space $V$,
  if they are pairwise orthogonal, then we call
  \begin{equation}
    V_1 + \cdots + V_s
  \end{equation}
  an \emph{orthogonal sum}.
\end{definition}

\begin{proposition}{Orthogonal Sums are Direct Sums}{}
  If $V_1 + \cdots +V_s$ is an orthogonal sum,
  then it is a direct sum.
\end{proposition}

\begin{proof}
  Assume $\alpha_1 + \cdots + \alpha_s = 0$,
  by taking the inner product with $\alpha_i$ on both sides, we have $\alpha_i = 0$.
\end{proof}

\begin{definition}{Orthogonal Complement}{}
  Let $V_1, V_2$ be two subspaces of $V$.
  If $V_1$ and $V_2$ be orthogonal and
  \begin{equation}
    V = V_1 + V_2,
  \end{equation}
  then $V_1$ and $V_2$ are said to be the \emph{orthogonal complement} of each
  other,
  denoted as $V_1 = V_2^{\perp}$.
\end{definition}

\begin{proposition}{Uniqueness and Representation of Orthogonal Complement}{}
  For any subspace $W$ of an Euclidean space $V$,
  it has a unique orthogonal complement $W^{\perp}$.
  To be more specific,
  let $\epsilon_1,\cdots,\epsilon_s$ be an orthogonal basis of $W$,
  $\epsilon_1,\cdots,\epsilon_n$ be an orthogonal basis of $V$,
  then
  \begin{equation}
    W^{\perp} = \{\alpha \in V: \alpha \perp W\} = \operatorname{span}(\epsilon_{s+1},\cdots,\epsilon_n).
  \end{equation}
\end{proposition}

\begin{proof}
  Uniqueness: Assume that $W$ has orthogonal complement $W_1, W_2$,
  then for all $\alpha \in W_1$,
  there exist $\beta \in W$ and $\gamma \in W_2$ such that
  \begin{equation}
    \alpha = \beta + \gamma.
  \end{equation}
  However $\alpha \perp \beta$ and $\alpha \perp \gamma$,
  thus
  \begin{equation}
    0 = \langle \alpha, \beta \rangle = \langle \alpha, \beta \rangle
    + \langle \alpha, \gamma\rangle = \langle \alpha, \alpha \rangle,
  \end{equation}
  which means $\alpha = 0$ and $V_2 = V_3$.

  Representation: If $V = \operatorname{span}(\epsilon_1,\cdots,\epsilon_n)$ and
  $W = \operatorname{span}(\epsilon_1,\cdots,\epsilon_s)$,
  then $W^{\perp} = \operatorname{span}(\epsilon_{s+1},\cdots,\epsilon_n)$.
\end{proof}


\begin{example}{Properties of Orthogonal Complement}{}
  Prove that the following equations hold
  \begin{equation}
    (1) U = (U^{\perp})^{\perp}, \quad
    (2) (U_1 + U_2)^{\perp} = U_1^{\perp} \cap U_2^{\perp}, \quad
    (3) (U_1 \cap U_2)^{\perp} = U_1^{\perp} + U_2^{\perp}.
  \end{equation}
\end{example}

\begin{proof}
  Assume that $V = \operatorname{span}(\epsilon_1,\cdots,\epsilon_n)$,
  where $\epsilon_1,\cdots,\epsilon_n$ is a standard orthogonal basis.
  Denote $A = \{\epsilon_i: i = 1,\cdots,n\}$.

  (1) Let $U = \operatorname{span}(\epsilon_{i_1},\cdots,\epsilon_{i_s})$,
  and $B = \{\epsilon_i: i = i_1,\cdots,i_n\}$.
  \begin{equation}
    (U^{\perp})^{\perp} = (\operatorname{span}(A - B))^{\perp}
    = \operatorname{span}(A - (A - B))
    = \operatorname{span}(B) = U.
  \end{equation}

  (2) Let $U_1 = \operatorname{span}(\epsilon_{i_1},\cdots,\epsilon_{i_s})$
  and $U_2 = \operatorname{span}(\epsilon_{j_1},\cdots,\epsilon_{j_t})$.
  Denote $B = \{\epsilon_i: i = i_1,\cdots,i_s\}$ and $C = \{\epsilon_i: i = j_1,\cdots,j_t\}$.
  Then $U_1+U_2 = \operatorname{span}(B \cup C)$, and
  \begin{equation}
    (U_1 + U_2)^{\perp} = \operatorname{span}(A - (B \cup C))
    = \operatorname{span}((A - B) \cap (A - C))
    = \operatorname{span}(A - B) \cap \operatorname{span}(A - C)
    = U_1^{\perp} \cap U_2^{\perp}.
  \end{equation}
\end{proof}

\section{Unitary Space}









