
\section{Concepts of Numerical Series}

\begin{definition}{Partial-Sum and Convergence of Numerical Series}{}
  Given a numerical series $\sum\limits_{n = 1}^{\infty}u_n$,
  define $S_n = \sum\limits_{k = 1}^n u_k$ as the \emph{partial-sum function}.
  If the limit $\lim \limits _{n \rightarrow \infty} S_n$ exists,
  then the numerical series is said to \emph{converge};
  otherwise, the numerical series is said to \emph{diverge}.
\end{definition}

\begin{proposition}{Necessary Condition for Convergent Numerical Series}{}
  If a numerical series $\sum\limits_{n = 1}^{\infty}u_n$ converges,
  then
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} u_n = 0.
  \end{equation}
\end{proposition}

\begin{proof}
  Since the numerical series is convergent, the partial-sum sequence $S_n$ satisifies
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} S_n = L.
  \end{equation}
  Since $u_n = S_n - S_{n-1}$, taking limit at both sides yields
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} u_n = \lim \limits _{n \rightarrow \infty} S_n - \lim \limits _{n \rightarrow \infty} S_{n-1}
    = L - L = 0,
  \end{equation}
  which completes the proof.
\end{proof}

\begin{proposition}{Cauchy Criterion for Numerical Series}{}
  A numerical series $\sum\limits_{n = 1}^{\infty} u_n$ converges if and only if
  for any $\epsilon > 0$, there exists $N > 0$, such that for all $m, m+p > N$,
  \begin{equation}
    |u_{m+1} + \cdots + u_{m+p}| < \epsilon.
  \end{equation}
\end{proposition}

\begin{proof}
  Hint: Using the Cauchy criterion of $S_n$.
\end{proof}

\begin{example}{Application of Cauchy Criterion}{}
  Use Cauchy criterion to determine if the following numerical series are convergent
  \begin{equation}
  (1) \sum\limits_{n = 1}^{\infty}\frac{1}{n}, \quad
  (2) \sum\limits_{n = 1}^{\infty} \frac{1}{n^2}.
  \end{equation}
\end{example}

\begin{proof}
  (1) Take $\epsilon = \frac{1}{2}$, $m = N, p = N$.
  Use the contrapositive of Cauchy criterion.

  (2) $\frac{1}{n^2} < \frac{1}{n(n-1)} = \frac{1}{n-1} - \frac{1}{n}$,
  which yields
  \begin{equation}
    \sum\limits_{n = 1}^{\infty} \frac{1}{n^2}
    \leq 1 + 1 - \frac{1}{2} + \cdots - \frac{1}{n} = 2 - \frac{1}{n} \rightarrow 2.
  \end{equation}
  By monotonic convergence theorem, we know that the sequence is convergent.
\end{proof}

\section{Positive-Term Series}

\subsection{The Integral Test}

\begin{theorem}{The Integral Test for Positive-Term Series}{}
  Let $\sum\limits_{n = 1}^{\infty}u_n$ be a positive-term numerical series,
  $f(x)$ be a non-decreasing function on $[0, +\infty)$,
  then $\sum\limits_{n = 1}^{\infty} f(x)$ has the same convergence behavior with
  \begin{equation}
    \int_1^{+\infty} f(x) \mathrm{d} x.
  \end{equation}
\end{theorem}

\begin{example}{Convergence of Power Series}{}
  Prove that $\sum\limits_{n = 1}^{\infty} \frac{1}{n^p}$ converges when $p >
  1$,
  and diverges when $p \leq 1$.
\end{example}

\begin{proof}
  Hint: by integral test.
\end{proof}

\subsection{The Comparison Test}

\begin{theorem}{Non-limit form of Comparison Test}{}
  Let $\sum\limits_{n = 1}^{\infty} u_n$ and $\sum\limits_{n = 1}^{\infty}v_n$
  be two positive-term numerical series,
  if there exists $N > 0$, such that for all $n > N$,
  \begin{equation}
    u_n \leq v_n.
  \end{equation}
  then if $\sum\limits_{n = 1}^{\infty} v_n$ converges,
  it can be deduced that $\sum\limits_{n = 1}^{\infty}u_n$ converges.
  If $\sum\limits_{n = 1}^{\infty}u_n$ diverges,
  then $\sum\limits_{n = 1}^{\infty}v_n$ also diverges.
\end{theorem}


\begin{theorem}{limit form of Comparison Test}{}
  Let $\sum\limits_{n = 1}^{\infty} u_n$ and $\sum\limits_{n = 1}^{\infty}v_n$
  be two positive-term series.
  Suppose that
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \frac{u_n}{v_n} = \lambda.
  \end{equation}
  If $0 < \lambda < \infty$,
  then $\sum u_n, \sum v_n$ have the same convergence behavior.
  When $\lambda = 0$, if $\sum v_n$ converges,
  then $\sum u_n$ converges.
  When $\lambda = \infty$, if $\sum v_n$ diverges,
  then $\sum u_n$ diverges.
\end{theorem}

\begin{example}{Determine Convergence or Divergence}{}
  Using equivalent infinitesimals to determine convergence or divergence:
  \begin{equation}
    (1) \sum\limits_{n = 1}^{\infty} \frac{1}{\sqrt{n^2 + 1}}; \quad
    (2) \sum\limits_{n = 1}^{\infty} 2^n \sin \frac{\pi}{3^n}; \quad
    (3) \sum\limits_{n = 1}^{\infty} \sqrt[n]{2} - 1; \quad
    (4) \sum\limits_{n = 1}^{\infty} (a^{\frac{1}{n}} + a^{- \frac{1}{n}} - 2), a > 1
  \end{equation}
\end{example}

\begin{solution}
  (1) equivalent to $\sum \frac{1}{n}$, thus diverges.
  (2) equivalent to $\sum \frac{2^n}{3^n}\pi$, thus converges to $2\pi$.
  (3) equivalent to $\sum \frac{1}{n} \ln 2$, thus diverges (using Taylor expansion).
  (4) equivalent to $\sum \frac{1}{n^2} \ln^2 a$, thus converges (using Taylor expansion).
\end{solution}

\begin{example}{Series Involving Logarithms and Exponential Function}{}
  Determine convergence or divergence:
  \begin{equation}
    \sum\limits_{n = 1}^{\infty} \frac{1}{3^{\ln n}}.
  \end{equation}
\end{example}

\begin{solution}
  Using the identity $a^{\ln b} = b^{\ln a}$ (which can be proved by taking
  logarithms on both sides).
  Then it is equivalent to $\sum \frac{1}{n^{\ln 3}}$, and it is convergent.
\end{solution}

\subsection{The Ratio Test and The Root Test}

\begin{theorem}{Ratio Test for Positive-Term Series}{}
  Let $\sum\limits_{n = 1}^{\infty}u_n$ be a positive-term series.
  If
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \frac{u_{n+1}}{u_n} = q < 1,
  \end{equation}
  then $\sum u_n$ converges. If $q > 1$, then $\sum u_n$ diverges.
\end{theorem}

\begin{theorem}{Root Test for Positive-Term Series}{}
  Let $\sum\limits_{n = 1}^{\infty}u_n$ be a positive-term series.
  If
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \sqrt[n]{u_n} = q < 1,
  \end{equation}
  then $\sum u_n$ converges. If $q > 1$, then $\sum u_n$ diverges.
\end{theorem}

\begin{note}
  By Stirling's formula, i.e.,
  \begin{equation}
    \sqrt[n]{n!} \approx \frac{n}{e},
  \end{equation}
  we can cover most of the cases using the root test.
\end{note}

\begin{example}{Using the Root Test}{}
  Determine the convergence or divergence
  \begin{equation}
    (1) \sum\limits_{n = 1}^{\infty} \frac{n!}{n^n}; \quad
    (2) \sum\limits_{n = 1}^{\infty} \frac{(n!)^2}{(2n)!}; \quad
    (3) \sum\limits_{n = 1}^{\infty} \frac{1}{2^{n + (-1)^n}}; \quad
    (4) \sum\limits_{n = 1}^{\infty} \frac{(2n-1)!!}{n!}.
  \end{equation}
\end{example}

\begin{solution}
  (1) The limit is $\frac{1}{e}$, thus converges.
  (2) The limit is $\frac{1}{4}$, thus converges.
  (3) The limit is $\frac{1}{2}$, thus converges.
  (4) Note that $(2n-1)!! = \frac{(2n)!}{(2n)!!} = \frac{(2n)!}{2^nn!}$,
  it is equivalent to determine
  \begin{equation}
    \sum\limits_{n = 1}^{\infty} \frac{(2n)!}{2^n(n!)^2}
    \Leftarrow
    \sqrt[n]{\frac{(2n)!}{2^n(n!)^2}} \rightarrow 2 > 1,
  \end{equation}
  thus the series is divergent.
\end{solution}

\section{Arbitrary-Term Series}

\subsection{Absolute Convergence and Conditional Convergence}

\begin{definition}{Absolute Convergence}{}
  A series $\sum\limits_{n = 1}^{\infty}a_n$ is said to \emph{converge absolutely}
  if the series of absolute values
  \begin{equation}
    \sum\limits_{n = 1}^{\infty} |a_n|
  \end{equation}
  converges.
\end{definition}

\begin{definition}{Conditional Convergence}{}
  A series $\sum\limits_{n = 1}^{\infty}a_n$ is said to \emph{converge conditionally}
  if it converges, but not absolutely converges.
\end{definition}

\begin{proposition}{Absolute Convergence Implies Convergence}{}
  If a series $\sum\limits_{n = 1}^{\infty} a_n$ is absolutely convergent,
  then it is convergent.
\end{proposition}

\subsection{Alternating Series}

\begin{theorem}{Leibniz's Test}{}
  If the sequence $\{a_n\}$ is monotonically decreasing and converges to $0$,
  then the alternating series
  \begin{equation}
    \sum\limits_{n = 1}^{\infty}(-1)^{n-1}a_n
  \end{equation}
  converges.
\end{theorem}

\begin{example}{Determine Absolute Convergence and Conditional Convergence}{}
  Determine the absolute convergence and conditional convergence of the
  following series
  \begin{equation}
    (1) \sum\limits_{n = 1}^{\infty} \frac{(-1)^n}{n^{p+\frac{1}{n}}}; \quad
    (2) \sum\limits_{n = 1}^{\infty} \sin (\pi \sqrt{n^2 + p^2}).
  \end{equation}
\end{example}

\begin{solution}
  (1) When $p > 1$, it is absolutely convergent.
  When $1 \geq p > 0$, define $f(x) = x^{p + \frac{1}{x}}$,
  find its derivative
  \begin{equation}
    f^{\prime}(x) = x^{p-1}x^{\frac{1}{x}} \left( p + \frac{1 - ln x}{x} \right).
  \end{equation}
  When $x$ is sufficiently large, we have $f^{\prime}(x) > 0$,
  and thus when $n$ is large enough, it satisifies Leibniz's Test.
  When $p \leq 0$,
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \frac{(-1)^n}{n^{p + \frac{1}{n}}} \neq 0,
  \end{equation}
  so it diverges.

  (2) By the properties of trigonometric functions, we know
  the term equals to
  \begin{equation}
    \sum\limits_{n = 1}^{\infty} (-1)^n \sin (\pi \sqrt{n^2 + p^2} - n\pi)
    = \sum\limits_{n = 1}^{\infty}(-1)^n \sin \frac{p^2 \pi}{\sqrt{n^2 + p^2} + n},
  \end{equation}
  which is absolutely convergent when $p = 0$,
  and conditionally convergent when $p \neq 0$.
\end{solution}

\subsection{Dirichlet's and Abel's Test}

\begin{theorem}{Dirichlet's Test}{}
  Let $\{a_n\}, \{b_n\}$ be two sequences of numbers,
  satisfying
  (1) $A_k = \sum\limits_{i = 1}^k a_i$ is bounded;
  (2) $b_n$ monotonically decreasing to $0$.
  Then the numerical series
  \begin{equation}
    \sum\limits_{k = 1}^{\infty} a_kb_k
  \end{equation}
  is convergent.
\end{theorem}

\begin{example}{Convergence of Trigonometric Series}{}
  Consider the following propositions:
  \begin{enumerate}
  \item If $a_n$ monotonically decreasing to $0$, $x \neq 2k\pi$.
    Prove that $\sum\limits_{n = 1}^{\infty} a_n \cos nx$ and $\sum\limits_{n =
      1}^{\infty} a_n \sin nx$ are convergent;
  \item If $x \neq k\pi$, discuss the convergence of
    \begin{equation}
      \sum\limits_{n = 1}^{\infty} \frac{\cos nx}{n^p}, \quad
      \sum\limits_{n = 1}^{\infty} \frac{\sin nx}{n^p}.
    \end{equation}
  \end{enumerate}
\end{example}

\begin{solution}
  (1) $\frac{1}{n^p}$ monotonically decreases to $0$, and
  \begin{equation}
    \left|\sum\limits_{k = 1}^{\infty} \cos kx\right|
    = \left|\frac{\sin (n + \frac{1}{2})x - \sin \frac{x}{2}}{2 \sin \frac{x}{2}}\right|
    \leq \frac{1}{|\sin \frac{x}{2}|}, \quad
    \left|\sum\limits_{k = 1}^{\infty} \sin kx \right|
    = \left| \frac{\cos \frac{x}{2} - \cos (n + \frac{1}{2})x}{2 \sin \frac{x}{2}} \right|
    \leq \frac{1}{|\sin \frac{x}{2}|}.
  \end{equation}
  Then the Dirichlet test deduces the conclusion.

  (2) When $p > 1$, $\left| \frac{\cos nx}{n^p} \right| \leq \frac{1}{n^p}$,
  then the series is absolutely convergent.
  When $p \leq 0$, $\lim \limits _{n \rightarrow \infty} \frac{\cos nx}{n^p}
  \rightarrow \infty$, then the series is divergent.
  When $1 \geq p > 0$, the series is convergent due to (1).
  But 
  \begin{equation}
    \left| \frac{\cos nx}{n^p} \right|
    \geq \left| \frac{\cos^2 nx}{n^p} \right|
    \geq \left| \frac{1 - \sin^2 x}{n^p} \right|,
  \end{equation}
  where $\sum\limits_{n = 1}^{\infty} \frac{1}{n^p}$ diverges,
  thus the series is not absolutely convergent.
\end{solution}

\begin{theorem}{Abel's Test}{}
  Let $\{a_n\}, \{b_n\}$ be two sequences of numbers,
  satisfying
  (1) $\sum\limits_{n = 1}^{\infty}a_n$ is convergent;
  (2) $b_n$ is monotonic and bounded.
  Then the numerical series
  \begin{equation}
    \sum\limits_{k = 1}^{\infty} a_kb_k
  \end{equation}
  is convergent.
\end{theorem}

\section{Theory for Numerical Series}

\subsection{Mutual Control of Series}

\begin{proposition}{Mutual Control of Series}{}
  Let $a_n, b_n, c_n$ be arbitrary-term sequences,
  satisfying $a_n \leq b_n \leq c_n$.
  Suppose that $\sum a_n$ and $\sum c_n$ are convergent,
  prove that $\sum b_n$ is convergent.
\end{proposition}

\begin{proof}
  It is not hard to know that $0 \leq b_n - a_n \leq c_n - a_n$,
  where $b_n - a_n$ and $c_n - a_n$ are non-zero.
  Then by comparison test, we know $\sum b_n - a_n$ is convergent,
  so is $\sum b_n$.
\end{proof}

\begin{example}{Applications of Mutual Control}{}
  Suppose $a_n$ is a monotonically decreasing sequence.
  Prove that $\sum\limits_{n = 1}^{\infty} a_n$ and $\sum\limits_{m =
    1}^{\infty} 2^m a_{2^m}$ have the same convergence behavior.
\end{example}

\begin{proof}
  Since $a_n$ is monotonically decreasing, we have
  \begin{equation}
    2^{k-1}a_{2^k} \leq a_{2^{k-1} + 1} + a_{2^{k-1} + 2} + \cdots + a_{2^k}
    \leq 2^{k-1}a_{2^{k-1}}.
  \end{equation}
  Summing over $k$, we get
  \begin{equation}
    \frac{1}{2} \sum\limits_{k = 1}^n 2^ka_{2^k}
    \leq a_1 + a_2 + \cdots + a_{2^n}
    \leq \sum\limits_{k = 0}^{n-1}2^ka_{2^k}.
  \end{equation}
  This shows that the two partial-sums are equivalent,
  so they have the same convergence behavior.
\end{proof}



