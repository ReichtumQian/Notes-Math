
\section{Derivatives and Differentials}

\subsection{The Concept of Derivatives and Differential}

\begin{definition}{Differentiability}{}
  Let $f(x)$ be a function defined on a neighborhood of $x_0 \in \mathbb{R}$,
  if the limit
  \begin{equation}
     \lim \limits _{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}
  \end{equation}
  exists, then we say it is \emph{differentiable at $x_0$}.
\end{definition}

\begin{definition}{Derivative}{}
  Let $f(x)$ be a function that is differentiable at $x_0 \in \mathbb{R}$,
  then its \emph{derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0) := \lim \limits _{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
\end{definition}

\begin{definition}{Left-hand and Right-hand Derivatives}{}
  Let $f(x)$ be a function defined on a neighborhood of $x_0 \in \mathbb{R}$,
  then its \emph{left-hand derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0^-) := \lim \limits _{x \rightarrow x_0^-} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
  Similarly, the \emph{right-hand derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0^+) := \lim \limits _{x \rightarrow x_0^+} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
\end{definition}

\begin{note}
  A function $f(x)$ is differentiable at $x_0$ if and only if both its left-hand and right-hand
  derivatives at $x_0$ exist and are equal.
\end{note}

\begin{example}{Use the Definition to Find the Derivative}{}
  Let $g(0) = g^{\prime}(0) = 0$, and $f(x)$ defined as follows, find $f^{\prime}(0)$:
  \begin{equation}
    f(x)=\begin{cases}
      g(x)\sin\frac{1}{x},&x\neq 0;\\
      0,&x=0.
    \end{cases}
  \end{equation}
\end{example}

\begin{solution}
  By the definition of derivatives
  \begin{equation}
    f^{\prime}(0)=\lim_{\Delta x\to0}\frac{f(\Delta x)-f(0)}{\Delta x}=\lim_{\Delta x\to0}\frac{g(\Delta x)\sin\frac{1}{\Delta x}}{\Delta x}=\lim_{\Delta x\to0}\frac{g(\Delta x)-g(0)}{\Delta x-0}\sin\frac{1}{\Delta x}=0,
  \end{equation}
  thus we have $f^{\prime}(0) = 0$.
\end{solution}

\begin{example}{The Non-Differentiability of the Riemann Function}{}
  Prove that the Riemann function $R(x)$ is non-differentiable everywhere in
  $(0, 1)$:
  \begin{equation}
    R(x)=\begin{cases}\frac{1}{q},&x=\frac{p}{q}\text{ (reduced form)}\\0,&x\text{ is irrational or }0,1\end{cases}
\end{equation}
\end{example}

\begin{proof}
  For rational points,
  the Riemann function is not even continuous,
  so it must not be differentiable.
  For irrational points,
  let $x_0 = 0.a_1a_2 \cdots$ be an irrational point.
  If we approach $x_0$ along an irrational number sequence,
  then the limit
  \begin{equation}
    \frac{f(x_n) - f(x_0)}{x_n - x_0} = 0
  \end{equation}
  exists. When approaching along a rational number sequence,
  take $y_n = 0.a_1a_2 \cdots a_n$,
  then $R(y_n) = \frac{1}{q} \geq \frac{1}{10^n}$,
  but $|y_n - x_0| \leq \frac{1}{10^n}$,
  thus
  \begin{equation}
    \left| \frac{R(y_n) - R(x_0)}{y_n - x_0} \right| \geq 1,
  \end{equation}
  which means that $\lim \limits _{n \rightarrow \infty} \frac{R(y_n)}{y_n -
    x_0} \neq 0$.
  By Heine Theorem we know that the limit $\lim \limits _{x \rightarrow x_0}
  \frac{R(x) - R(x_0)}{x - x_0}$ does not exist,
  so $R(x)$ is also non-differentiable at irrational points.
\end{proof}

\begin{definition}{Differential}{}
  Let $f(x)$ be a function defined in a neighborhood of $x_0 \in \mathbb{R}$,
  and it is differentiable at $x_0$.
  And its \emph{differential} is
  \begin{equation}
    \mathrm{d} f := f^{\prime}(x_0) \mathrm{d} x,
  \end{equation}
  where $\mathrm{d} x = \Delta x = x - x_0$.
\end{definition}

\subsection{Derivatives of Implicit Functions and Inverse Functions}

\begin{proposition}{Derivative of Implicit Functions}{}
  Differentiate both sides of a equation with respect to the same independent variable,
  and the equation still holds.
\end{proposition}

\begin{proposition}{Derivative of Inverse Functions}{}
  Suppose the inverse function of the function $y = y(x)$ is $x = x(y)$,
  then we have
  \begin{equation}
    y^{\prime}(x) = \frac{1}{x^{\prime}(y)}.
  \end{equation}
\end{proposition}

\begin{example}{Find Derivatives of Inverse Functions}{}
  Find the derivatives of
  (1) $\arcsin x$ (2) $\arccos x$ (3) $\arctan x$.
\end{example}

\begin{solution}
  (1) Define $y(x) = \arcsin x$, and the inverse function of $y(x)$ is
  $x(y) = \sin y$. So $x^{\prime}(y) = \cos y$, and according to the derivative
  of inverse functions we have
  \begin{equation}
    y^{\prime}(x) = \frac{1}{x^{\prime}(y)} = \frac{1}{\cos y} = \frac{1}{\sqrt{1 - x^2}}.
  \end{equation}
  (2) (3) are similar to (1).
\end{solution}

\begin{proposition}{Derivative of Power-Exponential Functions}{}
  For functions of the form $y(x) = u(x)^{v(x)}$,
  taking the natural logarithm gives $\ln y = v(x) \ln u(x)$,
  and differentiating both sides yields
  \begin{equation}
    y^{\prime}=u(x)^{v(x)}\left[v^{\prime}(x)\ln u(x)+v(x)\frac{u^{\prime}(x)}{u(x)}\right]
  \end{equation}
\end{proposition}

\begin{example}{Derivative of Power-Exponential Functions}{}
  Find the derivatives of (1) $x^{\frac{1}{x}}$
  (2) $x^x$.
\end{example}

\begin{solution}
  (1) Define $f(x) = x^{\frac{1}{x}}$, and
  taking the natural logarithm on both sides yields
  \begin{equation}
    \ln f(x) = \frac{\ln x}{x}.
  \end{equation}
  Differentiating both sides, we get
  \begin{equation}
    f^{\prime}(x) = x^{\frac{1}{x}} \frac{1 - \ln x}{x^2}.
  \end{equation}
  (2) is similar to (1).
\end{solution}

\subsection{Higher-Order Derivatives}

\begin{proposition}{Leibniz Formula}{}
  If $f(x)$ and $g(x)$ are $n$-th order differentiable functions,
  then $f(x)g(x)$ is also $n$-th order differentiable, and
  \begin{equation}
    \left[ f(x)g(x) \right]^{(n)} = \sum\limits_{k = 0}^n \binom{n}{k} f^{(n-k)}(x) g^{(k)}(x).
  \end{equation}
\end{proposition}

\begin{example}{High-order Derivatives for Inverse Trigonometric Functions}{}
  Find the $n$-th order derivative of the following functions at $0$:
  \begin{equation}
    f(x) = \arctan x, \quad
    g(x) = \arcsin x.
  \end{equation}
\end{example}

\begin{solution}
  (1) We know $f^{\prime}(x) = \frac{1}{1+x^2}$, and by Taylor expansion
  \begin{equation}
    f^{\prime}(x) = \frac{1}{1+x^2} = 1 - x^2 + x^4 - \cdots + (-1)^mx^{2m} + \cdots.
  \end{equation}
  Thus $f(x) = x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots +
  \frac{(-1)^m}{2m+1}x^{2m+1} + \cdots$,
  which yields
  \begin{equation}
    f^{(2m)}(x) = 0, \quad f^{(2m+1)}(x) = (-1)^m (2m)!.
  \end{equation}

  (2) is similar.
\end{solution}

\subsection{Commonly Used Derivatives Formulas}

\begin{proposition}{Derivatives of Trigonometric Functions}{}
  \begin{equation}
    \begin{array}{ll}
      (\tan x)^{\prime} = \sec^2 x & (\cot x)^{\prime} = - \csc^2 x\\
      (\sec x)^{\prime} = \sec x \tan x&  (\csc x)^{\prime} = - \csc x \cot x\\
      (\sin x)^{(n)} = \sin \left( x + \frac{n\pi}{2} \right)&(\cos x)^{(n)} = \cos \left( x + \frac{n\pi}{2}\right)
    \end{array}
  \end{equation}
\end{proposition}

\begin{proof}
  (1) (2) Hint: $\tan x = \frac{\sin x}{\cos x}$, $(\tan x)^{\prime} =
  \frac{\cos^2 x + \sin^2 x}{\cos^2 x} = \sec^2 x$.
  (3) (4) Hint: $\sec x = \frac{1}{\cos x}$,
  $(\sec x)^{\prime} = \frac{\sin x}{\cos^2 x} = \sec x \tan x$.
\end{proof}

\begin{proposition}{Derivatives of Inverse-Trigonometric Functions}{}
  \begin{equation}
    \begin{array}{ll}
      (\arcsin x)^{\prime} = \frac{1}{\sqrt{1 - x^2}} & (\arccos x)^{\prime} = - \frac{1}{\sqrt{1 - x^2}} \\
      (\arctan x)^{\prime} = \frac{1}{1 + x^2} & (\operatorname{arccot} x)^{\prime} = - \frac{1}{1 + x^2} \\
    \end{array}
  \end{equation}
\end{proposition}

\section{Differential Mean-Value Theorems}

\begin{theorem}{Fermat's Theorem}{}
  If $x_0$ is an extreme point of $f(x)$ and $f^{\prime}(x_0)$ exists,
  then
  \begin{equation}
    f^{\prime}(x_0) = 0.
  \end{equation}
\end{theorem}

\begin{proof}
  Without loss of generality, let $x_0$ be a minimum-value point.
  First, consider the right-hand derivative $f_+^{\prime}(x_0) = f^{\prime}(x_0)
  \geq 0$.
  Similarly, consider the left-hand derivative $f_-^{\prime}(x_0) =
  f^{\prime}(x_0) \leq 0$.
  Since the derivative of $f(x)$ at $x_0$ exists,
  so the left-hand and right-hand derivatives are equal.
  So the derivative $f^{\prime}(x_0)$ is zero.
\end{proof}

\begin{theorem}{Rolle's Mean-Value Theorem}{}
  If $f(x)$ is continuous on $[a, b]$,
  differentiable on $(a, b)$,
  and $f(a) = f(b)$,
  then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    f^{\prime}(\xi) = 0.
  \end{equation}
\end{theorem}

\begin{proof}
  Let $M, m$ be the maximum value and minimum value of $f(x)$ on $[a, b]$ respectively.
  If $M = m$ then $f(x) \equiv M$.
  If $m < M$, then $f(x)$ has at least one extreme-value point in $(a, b)$.
  The conclusion follows from the Fermat's theorem.
\end{proof}

\begin{example}{Applications of Rolle's Mean-Value Theorem}{}
  Prove the following propositions:
  \begin{enumerate}
  \item If $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$,
    $f(a) < 0, f(b) < 0$, and there exists $c \in (a, b)$ such that $f(c) > 0$,
    prove that there exists $\xi \in (a, b)$ such that
    \begin{equation}
      f(\xi) + f^{\prime}(\xi) = 0.
    \end{equation}
  \item If $f(x)$ is continuous on $[0, 1]$, differentiable on $(a, b)$,
    satisfying $f(0) = f(1) = 0$, $f(\frac{1}{2}) = 1$.
    Prove that for all $\lambda > 0$, there exists $\xi \in (0, 1)$ such that
    \begin{equation}
      f^{\prime}(\xi) - \lambda [f(\xi) - \xi] = 1.
    \end{equation}
  \item If $f(x)$ is continuous on $(0, 1)$, prove that there exists $c \in (0,
    1)$, such that
    \begin{equation}
      \int_0^cf(x)\mathrm{d}x = (1-c)f(c).
    \end{equation}
  \end{enumerate}
\end{example}

\begin{proof}
  (1) Let $F(x) = e^xf(x)$, then $F(a) < 0, F(b) < 0, F(c) > 0$,
  then there exist $\xi \in (a, c), \eta \in (c, b)$ such that
  $F(\xi) = F(\eta) = 0$.
  Use the Rolle's mean-value theorem, there exists $\zeta$ such that
  \begin{equation}
    F^{\prime}(\zeta) = e^{\zeta}[ f(\zeta) + f^{\prime}(\zeta)] = 0
    \Rightarrow f(\zeta) + f^{\prime}(\zeta) = 0.
  \end{equation}

  (2) Let $F(x) = [f(x) - x]e^{-\lambda x}$, then
  \begin{equation}
     F(0) = 0, F(\frac{1}{2}) > 0, F(1) < 0,
  \end{equation}
  then there exists a zero point in $[\frac{1}{2}, 1]$.
  The conclusion follows from the Rolle's mean-value theorem.

  (3) It is equivalent to prove that $\int_0^c f(x)\mathrm{d} x - (1-c)f(c) =
  0$,
  then define
  \begin{equation}
    F(x) = (1-x)\int_0^xf(t)\mathrm{d} t \Rightarrow
    F(0) = F(1) = 0.
  \end{equation}
  The conclusion follows from the Rolle's mean-value theorem.

\end{proof}

\begin{theorem}{Lagrange's Mean-Value Theorem}{}
  If $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$,
  then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    f^{\prime}(\xi) = \frac{f(b) - f(a)}{b - a}.
  \end{equation}
\end{theorem}

\begin{proof}
  Construct an auxiliary function
  $F(x) = f(x) - \frac{f(b) - f(a)}{b - a}x$,
  then we have
  \begin{equation}
    F(b) - F(a) = \left[ f(b) - f(a) \right]
    - \frac{f(b) - f(a)}{b - a}(b - a) = 0,
  \end{equation}
  that is $F(b) = F(a)$.
  The conclusion follows from Rolle's Mean-Value theorem.
\end{proof}

\begin{example}{Generalization of Lagrange's Mean-Value Theorem}{}
  Prove the following propositions
  \begin{enumerate}
  \item If $f(x)$ is continuous on $[a, b]$, differentiable on $(a, b)$, prove
    that there exists $\xi \in (a, b)$ such that
    \begin{equation}
      f(\xi) + \xi f^{\prime}(\xi) = \frac{b f(b) - a f(a)}{b - a}.
    \end{equation}
  \item If $f(x)$ is continuous on $[a, b]$, differentiable on $(a, b)$, and
    $f(b) \neq f(a)$, prove that if $f(x)$ is a non-linear function, then there exist
    $\xi, \eta \in (a, b)$ such that
    \begin{equation}
      f^{\prime}(\xi) < \frac{f(b) - f(a)}{b - a} < f^{\prime}(\eta).
    \end{equation}
  \end{enumerate}
\end{example}

\begin{proof}
  (1) Define $F(x) = xf(x)$, then the conclusion follows from the Lagrange's
  Mean-Value theorem.

  (2) To prove the left-hand inequality, we assume it does not hold,
  i.e., for all $x \in (a, b)$, $f^{\prime}(x) \geq \frac{f(b) - f(a)}{b - a}$.
  Then we construct
  \begin{equation}
    F(x) = f(x) - \frac{f(b) - f(a)}{b - a} x  \Rightarrow
    F^{\prime}(x) \geq 0, F(a) = F(b) = 0,
  \end{equation}
  which implies $F^{\prime}(x) \equiv 0$,
  and contradicts the given condition.
\end{proof}

\begin{theorem}{Cauchy's Mean-Value Theorem}{}
  If $f$ and $g$ are continuous on $[a, b]$,
  differentiable on $(a, b)$,
  $f^{\prime}(x), g^{\prime}(x)$ are not both $0$ at the same time,
  and $g(a) \neq g(b)$, then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    \frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
  \end{equation}
\end{theorem}


\section{L'Hopital's Rule and Taylor's Formulas}

\subsection{L'Hopital's Rule}

\begin{theorem}{L'Hopital's Rule}{}
  If two functions $f(x), g(x)$ satisfy
  (1)$\lim \limits _{x \rightarrow x_0} f(x) = \lim \limits _{x \rightarrow x_0}
  g(x) = 0$ or
  $\lim \limits _{x \rightarrow x_0} g(x) = \infty$;
  (2) $f^{\prime}(x), g^{\prime}(x)$ are defined in the punctured neighborhood
  of $x_0$, then
  \begin{equation}
    \lim \limits _{x \rightarrow x_0} \frac{f(x)}{g(x)}
    = \lim \limits _{x \rightarrow x_0} \frac{f^{\prime}(x)}{g^{\prime}(x)}.
  \end{equation}
\end{theorem}


\subsection{Taylor's Formulas}

\begin{theorem}{Taylor's Formula with Peano Remainder}{}
  If $f$ has up to $n$-th order derivatives at $x_0$ (only single-point
  derivatives are required), then
  \begin{equation}
    f(x)=f(x_0)+f^{\prime}(x_0)(x-x_0)+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+o((x-x_0)^n)
  \end{equation}
\end{theorem}

\begin{theorem}{Taylor's Formula with Lagrange Remainder}{}
  If $f$ has up to $n + 1$-th order derivatives on $[a, b]$,
  and is $n$-th order continuously differentiable on $(a, b)$,
  then
  \begin{equation}
    f(x)=f(x_0)+f^{\prime}(x_0)(x-x_0)+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1},
  \end{equation}
  where $\xi \in (a, b)$.
\end{theorem}

\begin{example}{Applications of Taylor's Expansion}{}
  Prove the following propositions:
  \begin{enumerate}
  \item If $f(x)$ is twice continuously differentiable on $[a, b]$,
    prove there exists $\xi \in (a, b)$ such that
    \begin{equation}
      f(b) - 2 f(\frac{a+b}{2}) + f(a)
      = \frac{1}{4}(b-a)^2 f^{\prime\prime}(\xi).
    \end{equation}
  \item If $f(x)$ is twice continuously differentiable on $[a, b]$,
    prove that there exists $\xi \in (a, b)$ such that
    \begin{equation}
      \int_a^b f(x)\mathrm{d} x
      = (b-a) f(\frac{a+b}{2}) + \frac{1}{24} (b-a)^3 f^{\prime\prime\prime}(\xi).
    \end{equation}
  \end{enumerate}
\end{example}

\section{Convex and Concave Functions}

\begin{definition}{Convex Function}{}
  Let $f$ be a function on the interval $I$.
  If for any $x_1, x_2 \in I$ and any $\lambda \in (0, 1)$,
  $f$ satisfies
  \begin{equation}
    f \left( \lambda x_1 + (1-\lambda)x_2 \right) \leq \lambda f(x_1) + (1-\lambda)f(x_2).
  \end{equation}
  Then $f$ is called a \emph{convex function}.
\end{definition}

\begin{lemma}{Ratio-Dividing Point Formula}{}
  For a point $x \in [x_1, x_2]$, then $x$ can be expressed by
  its distances from $x_1$ and $x_2$:
  \begin{equation}
    x = \frac{x_2 - x}{x_2 - x_1}x_1 + \frac{x - x_1}{x_2 - x_1}x_2.
  \end{equation}
\end{lemma}

\begin{proposition}{Equivalent Conditions of Convex Functions}{}
  The fact that $f$ is a convex function on $I$ is equivalent to
  the following conditions
  \begin{itemize}
  \item First-order derivative form: For any $x_0 \in I$,
    there is $f(x) \geq f(x_0) + f^{\prime}(x_0) (x - x_0)$;
  \item Differentiable: $f^{\prime}(x)$ being monotonically increasing;
  \item Second-oder derivative form: $f^{\prime\prime}(x) \geq 0$.
  \end{itemize}
\end{proposition}

\begin{example}{Equivalent Conditions of Convex Functions}{}
  Suppose $f(x)$ is twice-continuously differentiable on $\mathbb{R}$.
  Prove that $f^{\prime\prime}(x) \geq 0$ if and only if for all $x,h \in \mathbb{R}$
  \begin{equation}
    f(x+h) + f(x-h) -2f(x) \geq 0.
  \end{equation}
\end{example}

\begin{proof}
  By $f^{\prime\prime}(x) \geq 0$ we know $f(x)$ is a convex function,
  which means
  \begin{equation}
    \frac{f(x+h) - f(x)}{h} \geq \frac{f(x) - f(x-h)}{h}.
  \end{equation}
  The above equation yields the conclusion.
\end{proof}

\begin{example}{Application of Convex Functions}{}
  Prove the following propositions:
  \begin{enumerate}
  \item Let $\varphi(x)$ be a convex function on $[0, 1]$, prove that
    \begin{equation}
      \varphi (\frac{1}{2}) \leq \int_0^1\varphi(x)\mathrm{d} x \leq \frac{\varphi(0) + \varphi(1)}{2}.
    \end{equation}
  \item Let $\varphi(x)$ be a continuous function on $[a, b]$, prove that
    \begin{equation}
      \varphi (\frac{a+b}{2}) \leq \frac{1}{b-a} \int_a^b \varphi(x)\mathrm{d} x \leq \frac{\varphi(a) + \varphi(b)}{2}.
    \end{equation}
  \end{enumerate}
\end{example}

\begin{proof}
  (1) Since $x = x \cdot 1 + (1 - x) \cdot 0$, then according to the properties
  of convex functions
  \begin{equation}
    \int_0^1 \varphi(x)\mathrm{d} x \leq
    \int_0^1 (1-x)\varphi(0) + x\varphi(1)\mathrm{d} x = 
    \frac{\varphi(0) + \varphi(1)}{2}.
  \end{equation}
  On the other hand, let $x = 1 - t$,
  we know that $\int_0^1 \varphi(x)\mathrm{d} x = \int_0^1
  \varphi(1-x)\mathrm{d} x$, and
  \begin{equation}
    \int_0^1\varphi(x)\mathrm{d}x=\int_0^1\frac{1}{2}\varphi(x)+\frac{1}{2}\varphi(1-x)\mathrm{d}x\geq\int_0^1\varphi\left(\frac{x}{2}+\frac{1-x}{2}\right)\mathrm{d}x=\varphi\left(\frac{1}{2}\right)
  \end{equation}

  (2) Change variable $x = \lambda a + (1-\lambda)b$ and convert it to (1).
\end{proof}


