
\section{Derivatives and Differentials}

\subsection{The Concept of Derivatives and Differential}

\begin{definition}{Differentiability}{}
  Let $f(x)$ be a function defined on a neighborhood of $x_0 \in \mathbb{R}$,
  if the limit
  \begin{equation}
     \lim \limits _{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}
  \end{equation}
  exists, then we say it is \emph{differentiable at $x_0$}.
\end{definition}

\begin{definition}{Derivative}{}
  Let $f(x)$ be a function that is differentiable at $x_0 \in \mathbb{R}$,
  then its \emph{derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0) := \lim \limits _{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
\end{definition}

\begin{definition}{Left-hand and Right-hand Derivatives}{}
  Let $f(x)$ be a function defined on a neighborhood of $x_0 \in \mathbb{R}$,
  then its \emph{left-hand derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0^-) := \lim \limits _{x \rightarrow x_0^-} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
  Similarly, the \emph{right-hand derivative at $x_0$} is defined by
  \begin{equation}
    f^{\prime}(x_0^+) := \lim \limits _{x \rightarrow x_0^+} \frac{f(x) - f(x_0)}{x - x_0}.
  \end{equation}
\end{definition}

\begin{note}
  A function $f(x)$ is differentiable at $x_0$ if and only if both its left-hand and right-hand
  derivatives at $x_0$ exist and are equal.
\end{note}

\begin{example}{Use the Definition to Find the Derivative}{}
  Let $g(0) = g^{\prime}(0) = 0$, and $f(x)$ defined as follows, find $f^{\prime}(0)$:
  \begin{equation}
    f(x)=\begin{cases}
      g(x)\sin\frac{1}{x},&x\neq 0;\\
      0,&x=0.
    \end{cases}
  \end{equation}
\end{example}

\begin{solution}
  By the definition of derivatives
  \begin{equation}
    f^{\prime}(0)=\lim_{\Delta x\to0}\frac{f(\Delta x)-f(0)}{\Delta x}=\lim_{\Delta x\to0}\frac{g(\Delta x)\sin\frac{1}{\Delta x}}{\Delta x}=\lim_{\Delta x\to0}\frac{g(\Delta x)-g(0)}{\Delta x-0}\sin\frac{1}{\Delta x}=0,
  \end{equation}
  thus we have $f^{\prime}(0) = 0$.
\end{solution}

\begin{example}{The Non-Differentiability of the Riemann Function}{}
  Prove that the Riemann function $R(x)$ is non-differentiable everywhere in
  $(0, 1)$:
  \begin{equation}
    R(x)=\begin{cases}\frac{1}{q},&x=\frac{p}{q}\text{ (reduced form)}\\0,&x\text{ is irrational or }0,1\end{cases}
\end{equation}
\end{example}

\begin{proof}
  For rational points,
  the Riemann function is not even continuous,
  so it must not be differentiable.
  For irrational points,
  let $x_0 = 0.a_1a_2 \cdots$ be an irrational point.
  If we approach $x_0$ along an irrational number sequence,
  then the limit
  \begin{equation}
    \frac{f(x_n) - f(x_0)}{x_n - x_0} = 0
  \end{equation}
  exists. When approaching along a rational number sequence,
  take $y_n = 0.a_1a_2 \cdots a_n$,
  then $R(y_n) = \frac{1}{q} \geq \frac{1}{10^n}$,
  but $|y_n - x_0| \leq \frac{1}{10^n}$,
  thus
  \begin{equation}
    \left| \frac{R(y_n) - R(x_0)}{y_n - x_0} \right| \geq 1,
  \end{equation}
  which means that $\lim \limits _{n \rightarrow \infty} \frac{R(y_n)}{y_n -
    x_0} \neq 0$.
  By Heine Theorem we know that the limit $\lim \limits _{x \rightarrow x_0}
  \frac{R(x) - R(x_0)}{x - x_0}$ does not exist,
  so $R(x)$ is also non-differentiable at irrational points.
\end{proof}

\begin{definition}{Differential}{}
  Let $f(x)$ be a function defined in a neighborhood of $x_0 \in \mathbb{R}$,
  and it is differentiable at $x_0$.
  And its \emph{differential} is
  \begin{equation}
    \mathrm{d} f = f^{\prime}(x_0) \mathrm{d} x,
  \end{equation}
  where $\mathrm{d} x = \Delta x = x - x_0$.
\end{definition}

\subsection{Derivatives of Implicit Functions and Inverse Functions}

\begin{proposition}{Derivative of Implicit Functions}{}
  Differentiate both sides of a equation with respect to the same independent variable,
  and the equation still holds.
\end{proposition}

\begin{proposition}{Derivative of Inverse Functions}{}
  Suppose the inverse function of the function $y = y(x)$ is $x = x(y)$,
  then we have
  \begin{equation}
    y^{\prime}(x) = \frac{1}{x^{\prime}(y)}.
  \end{equation}
\end{proposition}

\begin{example}{Finding Derivatives of Inverse Functions}{}
  Find the derivatives of
  (1) $\arcsin x$ (2) $\arccos x$ (3) $\arctan x$.
\end{example}

\begin{solution}
  (1) Define $y(x) = \arcsin x$, and the inverse function of $y(x)$ is
  $x(y) = \sin y$. So $x^{\prime}(y) = \cos y$, and according to the derivative
  of inverse functions we have
  \begin{equation}
    y^{\prime}(x) = \frac{1}{x^{\prime}(y)} = \frac{1}{\cos y} = \frac{1}{\sqrt{1 - x^2}}.
  \end{equation}
  (2) (3) are similar to (1).
\end{solution}

\begin{proposition}{Derivative of Power-Exponential Functions}{}
  For functions of the form $y(x) = u(x)^{v(x)}$,
  taking the natural logarithm gives $\ln y = v(x) \ln u(x)$,
  and differentiating both sides yields
  \begin{equation}
    y^{\prime}=u(x)^{v(x)}\left[v^{\prime}(x)\ln u(x)+v(x)\frac{u^{\prime}(x)}{u(x)}\right]
  \end{equation}
\end{proposition}

\begin{example}{Derivative of Power-Exponential Functions}{}
  Find the derivatives of (1) $x^{\frac{1}{x}}$
  (2) $x^x$.
\end{example}

\begin{solution}
  (1) Define $f(x) = x^{\frac{1}{x}}$, and
  taking the natural logarithm on both sides yields
  \begin{equation}
    \ln f(x) = \frac{\ln x}{x}.
  \end{equation}
  Differentiating both sides, we get
  \begin{equation}
    f^{\prime}(x) = x^{\frac{1}{x}} \frac{1 - \ln x}{x^2}.
  \end{equation}
  (2) is similar to (1).
\end{solution}

\subsection{Higher-Order Derivatives}

\begin{proposition}{Leibniz Formula}{}
  If $f(x)$ and $g(x)$ are $n$-th order differentiable functions,
  then $f(x)g(x)$ is also $n$-th order differentiable, and
  \begin{equation}
    \left[ f(x)g(x) \right]^{(n)} = \sum\limits_{k = 0}^n \binom{n}{k} f^{(n-k)}(x) g^{(k)}(x).
  \end{equation}
\end{proposition}

\subsection{Commonly Used Derivatives Formulas}

\begin{proposition}{Derivatives of Trigonometric Functions}{}
  \begin{equation}
    \begin{array}{ll}
      (\tan x)^{\prime} = \sec^2 x & (\cot x)^{\prime} = - \csc^2 x\\
      (\sec x)^{\prime} = \sec x \tan x&  (\csc x)^{\prime} = - \csc x \cot x\\
      (\sin x)^{(n)} = \sin \left( x + \frac{n\pi}{2} \right)&(\cos x)^{(n)} = \cos \left( x + \frac{n\pi}{2}\right)
    \end{array}
  \end{equation}
\end{proposition}

\begin{proof}
  (1) (2) Hint: $\tan x = \frac{\sin x}{\cos x}$, $(\tan x)^{\prime} =
  \frac{\cos^2 x + \sin^2 x}{\cos^2 x} = \sec^2 x$.
  (3) (4) Hint: $\sec x = \frac{1}{\cos x}$,
  $(\sec x)^{\prime} = \frac{\sin x}{\cos^2 x} = \sec x \tan x$.
\end{proof}

\begin{proposition}{Derivatives of Inverse-Trigonometric Functions}{}
  \begin{equation}
    \begin{array}{ll}
      (\arcsin x)^{\prime} = \frac{1}{\sqrt{1 - x^2}} & (\arccos x)^{\prime} = - \frac{1}{\sqrt{1 - x^2}} \\
      (\arctan x)^{\prime} = \frac{1}{1 + x^2} & (\operatorname{arccot} x)^{\prime} = - \frac{1}{1 + x^2} \\
    \end{array}
  \end{equation}
\end{proposition}

\begin{example}{Higher-Order Derivatives of Inverse-Trigonometric Functions}{}
  Consider $f(x) = \arctan x, g(x) = \arcsin x$,
  find $f^{(n)}(0)$ and $g^{(n)}(0)$.
\end{example}

\begin{solution}
  (1) We know that $f^{\prime}(x) = \frac{1}{1 + x^2}$,
  and according to Taylor expansion, we have
  \begin{equation}
    f^{\prime}(x) = \frac{1}{1 + x^2} = 1 - x^2 + x^4 - \cdots + (-1)^m x^{2m} + \cdots,
  \end{equation}
  thus $f(x) = x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots + \frac{(-1)^m}{2m +
    1}x^{2m+1} + \cdots$.
  According to the coefficients, we know that $f^{(2m)}(0) = 0, f^{(2m+1)}(0) = (-1)^m
  (2m)!$.
\end{solution}




\section{Differential Mean-Value Theorems}

\begin{theorem}{Fermat's Theorem}{}
  If $x_0$ is an extreme point of $f(x)$ and $f^{\prime}(x_0)$ exists,
  then
  \begin{equation}
    f^{\prime}(x_0) = 0.
  \end{equation}
\end{theorem}

\begin{proof}
  Without loss of generality, let $x_0$ be a minimum-value point.
  First, consider the right-hand derivative $f_+^{\prime}(x_0) = f^{\prime}(x_0)
  \geq 0$.
  Similarly, consider the left-hand derivative $f_-^{\prime}(x_0) =
  f^{\prime}(x_0) \leq 0$.
  Since the derivative of $f(x)$ at $x_0$ exists,
  so the left-hand and right-hand derivatives are equal.
  So the derivative $f^{\prime}(x_0)$ is zero.
\end{proof}

\begin{theorem}{Rolle's Mean-Value Theorem}{}
  If $f(x)$ is continuous on $[a, b]$,
  differentiable on $(a, b)$,
  and $f(a) = f(b)$,
  then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    f^{\prime}(\xi) = 0.
  \end{equation}
\end{theorem}


\begin{theorem}{Lagrange's Mean-Value Theorem}{}
  If $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$,
  then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    f^{\prime}(\xi) = \frac{f(b) - f(a)}{b - a}.
  \end{equation}
\end{theorem}

\begin{theorem}{Cauchy's Mean-Value Theorem}{}
  If $f$ and $g$ are continuous on $[a, b]$,
  differentiable on $(a, b)$,
  $f^{\prime}(x), g^{\prime}(x)$ are not both $0$ at the same time,
  and $g(a) \neq g(b)$, then there exists $\xi \in (a, b)$ such that
  \begin{equation}
    \frac{f^{\prime}(\xi)}{g^{\prime}(\xi)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
  \end{equation}
\end{theorem}

\section{L'Hopital's Rule and Taylor's Formulas}

\subsection{L'Hopital's Rule}

\begin{theorem}{L'Hopital's Rule}{}
  If two functions $f(x), g(x)$ satisfy
  (1)$\lim \limits _{x \rightarrow x_0} f(x) = \lim \limits _{x \rightarrow x_0}
  g(x) = 0$ or
  $\lim \limits _{x \rightarrow x_0} g(x) = \infty$;
  (2) $f^{\prime}(x), g^{\prime}(x)$ are defined in the punctured neighborhood
  of $x_0$, then
  \begin{equation}
    \lim \limits _{x \rightarrow x_0} \frac{f(x)}{g(x)}
    = \lim \limits _{x \rightarrow x_0} \frac{f^{\prime}(x)}{g^{\prime}(x)}.
  \end{equation}
\end{theorem}


\subsection{Taylor's Formulas}

\begin{theorem}{Taylor's Formula with Peano Remainder}{}
  If $f$ has up to $n$-th order derivatives at $x_0$ (only single-point
  derivatives are required), then
  \begin{equation}
    f(x)=f(x_0)+f^{\prime}(x_0)(x-x_0)+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+o((x-x_0)^n)
  \end{equation}
\end{theorem}

\begin{theorem}{Taylor's Formula with Lagrange Remainder}{}
  If $f$ has up to $n + 1$-th order derivatives on $[a, b]$,
  and is $n$-th order continuously differentiable on $(a, b)$,
  then
  \begin{equation}
    f(x)=f(x_0)+f^{\prime}(x_0)(x-x_0)+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+\frac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1},
  \end{equation}
  where $\xi \in (a, b)$.
\end{theorem}

\section{Convex and Concave Functions}

\subsection{The Convexity of Functions}

\begin{definition}{Convex Function}{}
  Let $f$ be a function on the interval $I$.
  If for any $x_1, x_2 \in I$ and any $\lambda \in (0, 1)$,
  $f$ satisfies
  \begin{equation}
    f \left( \lambda x_1 + (1-\lambda)x_2 \right) \leq \lambda f(x_1) + (1-\lambda)f(x_2).
  \end{equation}
  Then $f$ is called a \emph{convex function}.
\end{definition}

\begin{lemma}{Ratio-Dividing Point Formula}{}
  For a point $x \in [x_1, x_2]$, then $x$ can be expressed by
  its distances from $x_1$ and $x_2$:
  \begin{equation}
    x = \frac{x_2 - x}{x_2 - x_1}x_1 + \frac{x - x_1}{x_2 - x_1}x_2.
  \end{equation}
\end{lemma}

\begin{proposition}{Equivalent Conditions of Convex Functions}{}
  The fact that $f$ is a convex function on $I$ is equivalent to
  the following conditions:
  \begin{itemize}
  \item Not differentiable: $f(x) \geq f(x_0) + k(x - x_0)$;
  \item First-order derivative form: For any $x_0 \in I$,
    there is $f(x) \geq f(x_0) + f^{\prime}(x_0) (x - x_0)$;
  \item Differentiable: $f^{\prime}(x)$ being monotonically increasing;
  \item Second-oder derivative form: $f^{\prime\prime}(x) \geq 0$.
  \end{itemize}
\end{proposition}


