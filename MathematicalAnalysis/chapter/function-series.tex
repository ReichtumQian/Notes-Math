
\section{Uniform Convergence of Function Sequences}

\subsection{Concept of Uniform Convergence}

\begin{definition}{Uniform Convergence}{}
  Let $\{f_n\}$ be a sequence of functions on the interval $I$.
  If for any $\epsilon > 0$, there exists a positive integer $N$
  such that for all $x \in I, n > N$,
  \begin{equation}
    |f_n(x) - f(x)| < \epsilon,
  \end{equation}
  then we say that $\{f_n\}$ \emph{converges uniformly to $f$ on $I$}.
\end{definition}

\begin{theorem}{Cauchy's Criterion for Uniform Convergence}{}
  Let $\{f_n\}$ be a sequence of functions on the interval $I$.
  $\{f_n\}$ is uniformly convergent on $I$ if and only if
  for any $\epsilon > 0$, there exists a positive integer $N$ such that
  \begin{equation}
    \forall m,n > N, \sup \limits_{x \in I} |f_m(x) - f_n(x)| < \epsilon.
  \end{equation}
\end{theorem}

\begin{example}{Use Definition to Prove Uniform Convergence}{}
  Prove the following propositions:
  \begin{enumerate}
  \item The function sequence $\frac{x}{n}$ is uniformly convergent on $(0, a]$
    for all $a \in \mathbb{R}^+$, but it is not uniformly convergent on $(0, +\infty)$.
  \item The function sequence $x^n$ is pointwise convergent in $(0, 1)$,
    but it is not uniformly convergent.
  \item If $f(x)$ has continuous derivative on $\mathbb{R}$, $a_n$ is a
    positive-term sequence that monotonically increases to $+\infty$.
    Then
    \begin{equation}
      f_n(x) = a_n \left[ f(x + \frac{1}{a_n}) - f(x) \right]
    \end{equation}
    converge uniformly to $f^{\prime}(x)$ on any closed interval $[a, b]$.
  \end{enumerate}
\end{example}

\begin{proof}
  (1) For all $x \in (0, a]$, the function limit $f(x) = \lim \limits _{n
    \rightarrow \infty} f_n(x) = 0$. For all $\epsilon > 0$, choose $N >
  \frac{a}{\epsilon}$,
  then we have
  \begin{equation}
    |f_n(x) - 0| = \frac{x}{n} - 0 < \epsilon,
  \end{equation}
  which implies that $f(x)$ is uniformly convergent on $(0, a]$.
  Let $\epsilon = \frac{1}{2}$, for all $N$, let $n = N$ and $x = N$,
  we get
  \begin{equation}
    |f_n(x) - 0| = \frac{x}{n} = 1 > \epsilon,
  \end{equation}
  which implies that $f(x)$ is not uniformly convergent on $(0, +\infty)$.

  (2) Consider $x_n = 1 - \frac{1}{n}$, then
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} |x_n^n - 0|
    = \lim \limits _{n \rightarrow \infty} (1 - \frac{1}{n})^n = e^{-1} \neq 0.
  \end{equation}
  This means that $x^n$ is not uniformly convergent.
\end{proof}

\subsection{Criteria for Uniform Convergence}

\begin{theorem}{Weierstrass M-test}{}
  If there exists a positive-term sequence $\{a_n\}$
  with $\lim \limits _{n \rightarrow \infty} a_n = 0$ such that
  \begin{equation}
    |f(x) - f_n(x)| \leq a_n, \forall x \in I,
  \end{equation}
  then the sequence of functions $\{f_n(x)\}$ converges uniformly to $f(x)$ on $I$.
\end{theorem}

\begin{example}{Application of M-test}{}
  Determine if the following series are uniformly convergent on $(0, +\infty)$:
  \begin{equation}
    (1) f_n(x) = \sqrt{x^2 + \frac{1}{n^2}}, \quad
    (2) f_n(x) = \frac{x}{1 + n^2x^2}, \quad
  \end{equation}
\end{example}

\begin{solution}
  (1) The limit function is $f(x) = x$.
  And
  \begin{equation}
    |f_n(x) - f(x)| = \frac{\frac{1}{n^2}}{\sqrt{x^2 + \frac{1}{n^2}} + |x|} \leq \frac{1}{n} \rightarrow 0,
  \end{equation}
  and the conclusion follows from the M-test.

  (2) The limit function $f(x) = 0$, and
  \begin{equation}
    |f_n(x) - f(x)| = \frac{x}{1 + n^2 x^2} \leq \frac{x}{2nx} = \frac{1}{2n} \rightarrow 0,
  \end{equation}
  then it is uniformly convergent.
\end{solution}

\begin{theorem}{Supremum Criterion}{}
  Let $\{f_n\}$ be a sequence of functions on the interval $I$, then
  $\{f_n(x)\}$ converges uniformly to $f(x)$ if and only if
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \sup \limits_{x \in I} |f_n(x) - f(x)| = 0.
  \end{equation}
\end{theorem}

\begin{corollary}{}{}
  Let $\{f_n\}$ be a sequence of functions on the interval $I$, then
  $\{f_n(x)\}$ does not converge uniformly to $f(x)$ if and only if
  there exists a sequence $\{x_n\}$ such that
  \begin{equation}
    b_n = f_n(x_n) - f(x_n)
  \end{equation}
  does not converge to $0$.
\end{corollary}

\begin{example}{Determine Non-uniformly Convergent Sequences}{}
  Determine the uniform convergence of following functions on $(0, +\infty)$:
  \begin{equation}
    (1) f_n(x) = \frac{1}{1+nx}, \quad
    (2) f_n(x) = \frac{nx}{1+n^2x^2}, \quad
    (3) f_n(x) = \sin \frac{x}{n}.
  \end{equation}
\end{example}

\begin{solution}
  (1) The limit function $f(x) = 0$, choose $x_n = \frac{1}{n}$, and
  \begin{equation}
    f_n(x_n) - f(x_n) = \frac{1}{2} \neq 0,
  \end{equation}
  so it is not uniformly convergent.

  (2) The limit function $f(x) = 0$, choose $x_n = \frac{1}{n}$, then
  \begin{equation}
    f_n(x_n) - f(x_n) = \frac{1}{2} \neq 0,
  \end{equation}
  so it is not uniformly convergent.

  (3) The limit function is $f(x) = 0$, let $x_n = n$ then
  \begin{equation}
    |f_n(x) - f(x)| = \sin 1 \neq 0,
  \end{equation}
  thus the function sequence is not uniformly convergent.
\end{solution}


\subsection{Uniform Convergence in Arithmetic Operations and Composite}

\begin{definition}{Uniform Boundedness}{}
  Let $\{f_n\}$ be a sequence of functions on $I$.
  If there exists a positive real-number $M$ such that for all $n \in \mathbb{Z}^+$ and $x \in I$,
  \begin{equation}
    |f_n(x)| \leq M,
  \end{equation}
  then it is said to be \emph{uniformly bounded on $I$}.
\end{definition}

\begin{proposition}{Uniform Convergence in Arithmetic Operations}{}
  Let $f_n(x)$ and $g_n(x)$ be functions that uniformly converge
  to $f(x)$ and $g(x)$ on an interval $I$ respectively. Then
  \begin{enumerate}
  \item $kf_n(x), f_n(x) + g_n(x)$ uniformly converge to $kf(x)$ and $f(x) + g(x)$;
  \item If $f_n(x),g_n(x)$ are bounded, then $f_n(x)g_n(x)$ converges uniformly
    to $f(x)g(x)$.
  \end{enumerate}
\end{proposition}

\begin{proposition}{Uniform Convergence in Composite}{}
  If $f_n(x)$ converges uniformly to $f(x)$ on an interval $I$,
  and each $f_n(x)$ is bounded, $g(x)$ is continuous on $\mathbb{R}$.
  Then $g(f_n(x))$ converges uniformly to $g(f(x))$.
\end{proposition}

\begin{proof}
  
\end{proof}

\section{Properties of Uniformly Convergent Function Sequences}

\subsection{Continuity of the Limit Function}

\begin{theorem}{Interchange of Limit Orders}{}
  If the sequence of functions $\{f_n(x)\}$ converges uniformly to $f(x)$
  on $(a, x_0) \cup (x_0, b)$,
  and for each $n$, the limit $\lim \limits _{x \rightarrow x_0} f_n(x) = a_n$
  exists,
  then
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \lim \limits _{x \rightarrow x_0} f_n(x) = 
    \lim \limits _{x \rightarrow x_0} \lim \limits _{n \rightarrow \infty} f_n(x).
  \end{equation}
\end{theorem}

\begin{proof}
  
\end{proof}

\subsection{Integrability of the Limit Function}

\begin{theorem}{Integrability of the Limit Function}{}
  If $\{f_n(x)\}$ converges uniformly to $f(x)$ on $[a, b]$,
  and each $f_n(x)$ is integrable on $[a, b]$,
  then $f(x)$ is integrable on $[a, b]$, and
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \int_a^b f_n(x) \mathrm{d} x 
    = \int_a^b f(x)\mathrm{d} x = \int_a^b \lim \limits _{n \rightarrow \infty} f_n(x) \mathrm{d} x.
  \end{equation}
\end{theorem}

\begin{proof}
  We first prove that $f(x)$ is integrable.
  Since $f_n(x)$ converges uniformly to $f(x)$, i.e.,
  \begin{equation}
    \forall \epsilon > 0, \exists N > 0, \forall x \in [a, b], \forall n > N, \quad
    |f(x) - f_n(x)| < \frac{\epsilon}{b - a}.
  \end{equation}
  Given a partition $T$, for any $x_1, x_2$ from a subinterval, consider the
  oscillation
  \begin{align}
    |f(x_1) - f(x_2)| &\leq |f(x_1) - f_n(x_1)| + |f_n(x_1) - f_n(x_2)| + |f_n(x_2) - f(x_2)|\\
    &\leq \frac{\epsilon}{b - a} + \omega_i^{f_n} + \frac{\epsilon}{b-a} = \frac{2\epsilon}{b-a} + \omega_i^{f_n}.
  \end{align}
  Therefore
  \begin{equation}
    \sum \limits_T \omega_i^f \Delta x_i \leq \sum \limits_T \frac{2\epsilon}{b-a}\Delta x_i
    + \sum \limits_T \omega_i^{f_n}\Delta x_i < 3\epsilon,
  \end{equation}
  which implies that $f(x)$ is integrable. Then consider the integral:
  \begin{equation}
    \left| \int_a^b f_n(x) \mathrm{d} x - \int_a^b f(x)\mathrm{d} x \right|
    \leq \int_a^b \left| f_n(x) - f(x) \right|\mathrm{d} x
    < \int_a^b \frac{\epsilon}{b-a}\mathrm{d} x = \epsilon,
  \end{equation}
  so the conclusion holds.
\end{proof}

\subsection{Differentiability of the Limit Function}

\begin{theorem}{Differentiability of the Limit Function}{}
  If each $f_n(x)$ has continuous derivative,
  $\{f_n^{\prime}(x)\}$ converge uniformly on $[a, b]$,
  and there exists $x_0 \in [a, b]$ such that $f_n(x_0)$ converges.
  Then $\{f_n\}$ converges uniformly on $[a, b]$ and
  \begin{equation}
    (\lim \limits _{n \rightarrow \infty} f_n(x))^{\prime}
    = \lim \limits _{n \rightarrow \infty} f_n^{\prime}(x), \quad x \in [a, b].
  \end{equation}
\end{theorem}

\begin{proof}
  Suppose $f_n^{\prime}(x)$ converges uniformly to $g(x)$.
  According to Neitown-Leibniz formula,
  \begin{equation}
    f_n(x) = f_n(x_0) + \int_{x_0}^x f_n^{\prime}(t)\mathrm{d}t.
  \end{equation}
  Since $\int_{x_0}^xf_n^{\prime}(t)\mathrm{d} t$ converges uniformly
  to $\int_{x_0}^xg(t)\mathrm{d} t$,
  and uniform convergence is closed under addition, so $f_n(x)$ converges
  uniformly to
  \begin{equation}
    f(x) = \lim \limits _{n \rightarrow \infty} f_n(x_0) + \int_{x_0}^xg(t)\mathrm{d} t.
  \end{equation}
  Furthermore, the fundamental theorem of calculus yields that $f^{\prime}(x) = g(x)$,
  so $(\lim \limits _{n \rightarrow \infty} f_n(x))^{\prime} = \lim \limits _{n \rightarrow \infty} f_n^{\prime}(x)$.
\end{proof}

\section{Power Series}

\subsection{Convergence Interval and Radius of Convergence}

\begin{theorem}{Abel's Theorem}{}
  If the power series $\sum\limits_{n = 1}^{\infty} a_nx^n$
  converges at $x_0 \neq 0$,
  then for all $x$ with $|x| < |x_0|$,
  $\sum\limits_{n = 1}^{\infty} a_nx^n$ converges absolutely.
  If it diverges at $x_0$,
  then for all $x$ with $|x| > |x_0|$, it diverges.
\end{theorem}

\begin{definition}{Radius of Convergence}{}
 Let $R$ be a real number, and $\sum a_nx^n$ be a power series.
 If for all $|x| < R$ the power series is absolutely convergent,
 and for all $|x| > R$ it is divergent.
 Then $R$ is said to be the \emph{radius of convergence of the power series},
 and the interval $(-R, R)$ the \emph{interval of convergence}.
\end{definition}

\begin{proposition}{Cauchy-Hadamard Test}{}
  Let $\sum a_nx^n$ be a power series, then its radius of convergence satsifies
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \sqrt[n]{|a_n|} = \frac{1}{R}.
  \end{equation}
\end{proposition}

\begin{proposition}{Ratio Test}{}
  Let $\sum a_nx^n$ be a power series, then its radius of convergence satsifies
  \begin{equation}
    \lim \limits _{n \rightarrow \infty} \frac{|a_{n+1}|}{|a_n|} = \frac{1}{R}.
  \end{equation}
\end{proposition}

\begin{example}{Find Radius of Convergence}{}
  Find the power series expansion and covergent region of
  \begin{equation}
    (1) \sin x, \quad
    (2) \cos x.
  \end{equation}
\end{example}

\begin{solution}
  (1) $\sin x = \sum\limits_{n = 0}^{\infty} \frac{(-1)^n}{(2n+1)!}x^{2n+1}$,
  then by Cauchy-Hadamard test, we know the power series converges to $\sin x$
  on $(-\infty, +\infty)$.
\end{solution}

\subsection{Sum Function of Power Series}

\begin{proposition}{Commonly Used Power Series Expansion}{}
  The following power series expansions are commonly used
  \begin{equation}
    (1) \frac{1}{1-x} = \sum\limits_{n = 0}^{\infty} x^n, \quad
    (2) e^x = \sum\limits_{n = 0}^{\infty} \frac{x^n}{n!}, \quad
    (3) \ln (1 + x) = \sum\limits_{n = 1}^{\infty} (-1)^{n-1} \frac{x^n}{n}, \quad
    (4) \arctan x = \sum\limits_{n = 0}^{\infty} \frac{(-1)^n x^{2n+1}}{2n + 1}.
  \end{equation}
\end{proposition}

\begin{example}{Geometric Series}{}
  Find the sum function of
  \begin{equation}
    (1) \sum\limits_{n = 1}^{\infty} \frac{n^2 + 3n}{2^n}, \quad
    (2) \sum\limits_{n = 1}^{\infty} \frac{x^n}{n(n+1)}.
  \end{equation}
\end{example}

\begin{solution}
  (1) Here we show how to derive from the known formula
  (because we want to differeiate a function rather than integrate it).
  \begin{equation}
    f_1(x) = \sum\limits_{n = 0}^{\infty} x^n = \frac{1}{1-x}
    \Rightarrow
    f_1^{\prime}(x) = \sum\limits_{n = 1}^{\infty} nx^{n-1} = \frac{1}{(1-x)^2}.
  \end{equation}
  Since we want to find the formula for $\sum nx^n$, multiply $f_1^{\prime}(x)$
  by $x$ and define it as $f_2(x)$:
  \begin{equation}
    f_2(x) = \sum\limits_{n = 1}^{\infty} nx^n = \frac{x}{(1-x)^2}
    \Rightarrow
    f_2^{\prime}(x) = \sum\limits_{n = 1}^{\infty} n^2x^{n-1} = \frac{1+x}{(1-x)^3}.
  \end{equation}
  Similarly, we also want to find the formula for $\sum n^2x^n$, so multiply $f_2^{\prime}(x)$
  by $x$ and define it as $f_3(x)$:
  \begin{equation}
    f_3(x) = \sum\limits_{n = 1}^{\infty} n^2x^n = \frac{x(1+x)}{(1-x)^3}.
  \end{equation}
  Then the term we want to evaluate is $f_3(\frac{1}{2}) + 3f_2(\frac{1}{2}) = 12$.

  (2) Here we show how to derive from the given condition
  (because we want to differeiate a function rather than integrate it).
  Denote the target function as $f_1(x)$, and $f_2(x) = xf_1(x)$, then
  \begin{equation}
    f_2(x) = \sum\limits_{n = 1}^{\infty} \frac{x^{n+1}}{n(n+1)}
    \Rightarrow f_2^{\prime}(x) = \sum\limits_{n = 1}^{\infty} \frac{x^n}{n}
    \Rightarrow f_2^{\prime\prime}(x) = \sum\limits_{n = 1}^{\infty} x^{n-1} = \sum\limits_{n = 0}^{\infty}x^n = \frac{1}{1-x}.
  \end{equation}
  Then integrate at both sides and get $f_2^{\prime}(x) = -\ln(1-x)$, $f_2(x) =
  (1-x)\ln(1-x) + x$, and
  \begin{equation}
    f_1(x) = \frac{1 - x}{x} \ln(1-x) + 1.
  \end{equation}
\end{solution}


\subsection{Power Series Expansion of Functions}




