
\section{Derivatives and Differentials of Multivariable Functions}

\subsection{Partial Derivatives and Total Differential}

\begin{definition}{Partial Derivative}{}
  Let $f$ be a multivariable function, then the \emph{partial derivative of $f$
  at $x_0$ with respect to $x_i$} is
  \begin{equation}
    \frac{\partial f}{\partial x_i} := \lim \limits _{t \rightarrow 0} \frac{f(\mathbf{x}_0 + t\mathbf{e}_i) - f(\mathbf{x}_0)}{t}.
  \end{equation}
\end{definition}

\begin{example}{Find the Partial Derivative}{}
  Find the partial derivative of $u = xe^{yz} + e^{-z} + y$ at the origin.
\end{example}

\begin{solution}
  Substitute $y = z = 0$, we get $u(x, 0, 0) = x$, and $u_x(x,0,0) = 1$,
  $u_x(0,0,0) = 1$.
  Similarly we get $u_y(0,0,0) = 1$ and $u_z(0,0,0) = 1$.
\end{solution}

\begin{definition}{Total Differential}{}
  Let $f(x, y)$ be a multivariable function,
  and it is differentiable with respect to $x, y$ at $(x_0, y_0)$.
  If
  \begin{equation}
    f(x, y) - f(x_0, y_0) = f^{\prime}_x(x_0, y_0)(x - x_0)
    + f^{\prime}_y (x_0, y_0)(y - y_0) + o(\sqrt{(x - x_0)^2 + (y - y_0)^2}),
  \end{equation}
  then we say $f(x,y)$ is \emph{differentiable at $(x_0, y_0)$},
  and its \emph{total differential at $(x_0, y_0)$} be
  \begin{equation}
    \mathrm{d} f = f_x^{\prime}(x_0, y_0) \mathrm{d} x + f_y^{\prime}(x_0, y_0) \mathrm{d} y.
  \end{equation}
\end{definition}

\begin{note}
  If a function does not have partial derivatives at a point,
  then it is not differentiable.
\end{note}

\begin{example}{Find the Total Differential}{}
  Determine the differentiability of the following function at the origin.
  \begin{equation}
    f(x,y)=\begin{cases}\frac{xy}{\sqrt{x^2+y^2}},&(x,y)\neq(0,0)\\0,&(x,y)=(0,0)\end{cases} \quad
  \end{equation}
\end{example}

\begin{solution}
  We first calculate the partial derivatives: $f_x(0, 0) = 0$ and $f_y(0, 0) = 0$.
  Then use the definition of total differential, we check
  \begin{equation}
    \frac{f(x, y) - f(0, 0) - f_x(0, 0) x - f_y(0, 0)y}{\sqrt{x^2 + y^2}}
    = \frac{xy}{x^2 + y^2}.
  \end{equation}
  Take $y = kx$ we find that the above limit does not exist,
  so it is not differentiable at $(0, 0)$.
\end{solution}

\begin{example}{Determine the Differentiability}{}
  Determine if the following functions are differentiable at $(0, 0)$.
  \begin{equation}
    f(x, y) =
    \begin{cases}
      y \ln (x^2 + y^2) & (x, y) \neq (0, 0);\\
      0 & (x, y) = (0, 0).
    \end{cases}
    \quad
    g(x, y) =
    \begin{cases}
      \frac{|x|^{\alpha} y}{x^2 + y^2} & (x, y) \neq (0, 0);\\
      0 & (x, y) = (0, 0).
    \end{cases}
  \end{equation}
\end{example}

\begin{solution}
  (1) We first check the partial derivatives,
  $f_x(0, 0) = 0$, but
  \begin{equation}
    f_y(0, 0) = \lim \limits _{y \rightarrow 0} \frac{2y\ln y}{y} = \lim \limits _{y \rightarrow 0} 2 \ln y \rightarrow - \infty.
  \end{equation}
  That means one partial derivative of $f(x, y)$ does not exist, thus it is not
  differentiable at the origin.

  (2) By calculation we get $f_x(0, 0) = f_y(0, 0) = 0$, and
  \begin{equation}
    \frac{f(x, y)}{\sqrt{x^2 + y^2}} = \frac{|x|^{\alpha} y}{(x^2 + y^2)^{\frac{3}{2}}}.
  \end{equation}
  Use the polar coordinates we get it is $r^{\alpha - 2}|\cos \theta|^\alpha
  \sin \theta$. When $\alpha > 2$ it is differentiable, and $\alpha \leq 2$ it
  is not differentiable.
\end{solution}

\subsection{Gradient and Directional Derivatives}

\begin{definition}{Gradient}{}
  For a multivariable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$,
  its \emph{gradient} is defined as
  \begin{equation}
    \nabla f := \left[ \frac{\partial f}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n} \right]^T.
  \end{equation}
\end{definition}

\begin{example}{Find the Gradient}{}
  Given $u(x,y,z) = xy^2z^3$, find the direction in which the function value of
  $u(x, y, z)$ grows fastest at $(1,1,1)$.
\end{example}

\begin{solution}
  The direction that grows fastest is the gradient direction, that is $(1,2,3)$.
\end{solution}

\begin{definition}{Directional Derivatives}{}
  Given $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and an unit vector $\mathbf{u}
  \in \mathbb{R}^n$, then \emph{directional derivative of $f$ at $\mathbf{x}_0$ with respect to $\mathbf{u}$}
  is
  \begin{equation}
    \frac{\partial f}{\partial \mathbf{u}} := \lim \limits _{t \rightarrow 0} \frac{f(\mathbf{x}_0 + t\mathbf{u}) - f(\mathbf{x}_0)}{t}.
  \end{equation}
\end{definition}

\begin{example}{}{}
  Prove that $f(x,y) = \sqrt[3]{x^2 + y^2}$ has no directional derivatives in
  any direction at the origin.
\end{example}

\begin{proof}
  Consider the direction $\mathbf{u} = (\cos \alpha, \sin \alpha)$,
  then we can calculate the directional derivative
  \begin{equation}
    \frac{\partial f}{\partial \mathbf{u}}
    = \lim \limits _{r \rightarrow 0} \frac{f(r\cos \alpha, r \sin \alpha) - f(0,0)}{r}
    = \lim \limits _{r \rightarrow 0} \frac{1}{r^{\frac{1}{3}}},
  \end{equation}
  and the limit does not exist.
\end{proof}

\begin{proposition}{Represent Directional Derivatives in Gradient}{}
  If $f(x, y)$ is differentiable at $(x_0, y_0)$,
  then we have
  \begin{equation}
    f_{\mathbf{u}}(x_0, y_0) = f_x(x_0, y_0) \cos \alpha + f_y(x_0, y_0) \cos \beta = \nabla f \cdot \mathbf{u},
  \end{equation}
  where $\mathbf{u}$ is an unit vector, and $\alpha, \beta$ are the angle
  between the vector $\mathbf{u}$ and the $x$-axis and $y$-axis respectively.
\end{proposition}

\begin{example}{Use Gradient to Represent Directional Derivatives}{}
  Calculate the directional derivative of $u = xyz$
  at $A(5,1,2)$ in the direction towards $B(9,4,14)$.
\end{example}

\begin{solution}
  We first find the partial derivatives,
  $u_x = yz, u_y = xz, u_z = xy$, and 
  \begin{equation}
    u_x(5, 1, 2) = 2, \quad
    u_y(5, 1, 2) = 10, \quad
    u_z(5, 1, 2) = 5.
  \end{equation}
  We can also get the directional vector $\mathbf{v} = (\frac{4}{13},
  \frac{3}{13}, \frac{12}{13})$.
  Then the directional derivative $u_{\mathbf{v}} = \nabla u \cdot \mathbf{v} = \frac{98}{13}$.
\end{solution}

\subsection{Relationship between Differentiability and Continuity}

\begin{proposition}{Properties of Differentiability}{}
  Let $f$ be a multivariable function that is differentiable at $\mathbf{x}_0$,
  Prove that (1) $f$ is continuous at $\mathbf{x}_0$
  (2) $f$ has partial derivatives and directional derivatives in any direction
  at $\mathbf{x}_0$.
\end{proposition}

\begin{proof}
  (1) According to the definition of differentiability,
  let $\Delta x^2 + \Delta y^2$ approximates zero, then we get
  \begin{equation}
    \lim \limits _{(x,y) \rightarrow (x_0, y_0)} f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0, y_0) 
    = 0,
  \end{equation}
  which implies the continuity.

  (2) Let $\Delta x = 0$ or $\Delta y = 0$ implies the existence of the partial
  derivatives. The proof of directional derivatives are similar.
\end{proof}

\begin{proposition}{Continuous Derivatives Means Differentiability}{}
  If the partial derivatives $f_x, f_y$ are continous at $(x_0, y_0)$,
  prove that $f(x,y)$ is differentiable at $(x_0, y_0)$.
\end{proposition}

\begin{proof}
  Use the Lagrange mean-value theorem, we know that there exist $\theta_1,
  \theta_2 \in \mathbb{R}$ such that
  \begin{equation}
    f(x_0 + \delta x, y_0 + \Delta y) - f(x_0, y_0)
    = f_x(x_0 + \theta_1 \Delta x, y_0 + \Delta y) \Delta x
    + f_y(x_0, y_0 + \theta_2 \Delta y)\Delta y.
  \end{equation}
  Since the partial derivatives are continuous, then there exists $\alpha,
  \beta$ such that
  \begin{equation}
    f_x(x_0 + \theta_1 \Delta x, y_0 + \Delta y) = f_x(x_0, y_0) + \alpha,
    \quad f_y(x_0, y_0 + \Delta y) = f_y(x_0, y_0) + \beta,
  \end{equation}
  where $\alpha, \beta$ satisfy
  $\lim \limits _{(\Delta x, \Delta y) \rightarrow (0, 0)} \alpha = 0$ and
  $\lim \limits _{(\Delta x, \Delta y) \rightarrow (0, 0)} \beta = 0$.
  We get
  \begin{equation}
    f(x_0 + \delta x, y_0 + \Delta y) - f(x_0, y_0)
    = f_x(x_0, y_0) \Delta x + f_y(x_0, y_0)\Delta y + \alpha \Delta x + \beta \Delta y,
  \end{equation}
  which implies that it is differentiable.
\end{proof}

\begin{example}{A Classic Example}{}
  Prove the following $f(x, y)$ is not continuous and differentiable at the origin,
  but its partial derivatives and any directional derivatives exist and are zero.
  \begin{equation}
    f(x, y) =
    \begin{cases}
      1 & 0 < y < x^2, -\infty < x < \infty;\\
      0 & \text{otherwise}
    \end{cases}
  \end{equation}
\end{example}

\begin{proof}
  $f(x, y)$ is not continuous at the origin since we can approximate the origin
  with different directions.
  The function values can approach either $0$ or $1$.
  The reason why all directional derivatives at the origin are $0$ is that the
  tangent line of $y = x^2$ at the origin is horizontal.
  For all directions, it will pass through the region where $f(x, y) = 0$.
\end{proof}

\subsection{Composite Differentiation: Chain Rule}

\begin{theorem}{Chain Rule}{}
  Let $x(s,t), y(s,t)$ be differentiable at $(s_0, t_0)$,
  and let $z(x, y)$ be differentiable at $(x(s_0, t_0), y(s_0,t_0))$.
  Then $f(s,t) = z(x(s,t), y(s,t))$ is differentiable at $(s_0,t_0)$, and
  \begin{equation}
    \frac{\partial f}{\partial s} = \frac{\partial z}{\partial x} \cdot \frac{\partial x}{\partial s} + \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial s},
  \end{equation}
  \begin{equation}
    \frac{\partial f}{\partial t} = \frac{\partial z}{\partial x} \cdot \frac{\partial x}{\partial t} + \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial t}.
  \end{equation}
\end{theorem}

\begin{example}{Use Chain Rule to Find Derivatives}{}
  Let $z = f \left( x, \frac{x}{y} \right)$, find $\frac{\partial^2 z}{\partial
    x^2}$ and $\frac{\partial^2 z}{\partial x\partial y}$.
\end{example}



\section{Mean-Value Theorem and Taylor Expansion of Multivariable Functions}

\subsection{Mean-Value Theorem of Multivariable Functions}

\subsection{Taylor Expansion of Multivariable Functions}








